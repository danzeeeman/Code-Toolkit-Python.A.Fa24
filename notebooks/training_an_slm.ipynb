{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Everything You Need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert CSV file to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "def convert_csv_to_json(csv_file_path):\n",
        "    # Read CSV file\n",
        "    with open(csv_file_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "    # Convert CSV data to JSON\n",
        "    json_data = json.dumps(rows, indent=4)\n",
        "\n",
        "    # Save JSON data to a file (optional)\n",
        "    with open('07-12-08-10.json', 'w') as json_file:\n",
        "        json_file.write(json_data)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = './data_sources/Lottery_Powerball_Winning_Numbers__Beginning_2010-08-10-2023.csv'\n",
        "\n",
        "# Convert CSV to JSON\n",
        "json_data = convert_csv_to_json(csv_file_path)\n",
        "\n",
        "print(\"Conversion completed. JSON data:\")\n",
        "print(json_data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert JSON to Prompt List and llama input JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{datetime.datetime(2010, 2, 3, 0, 0): '17 22 36 37 52 24', datetime.datetime(2010, 2, 6, 0, 0): '14 22 52 54 59 04', datetime.datetime(2010, 2, 10, 0, 0): '05 08 29 37 38 34', datetime.datetime(2010, 2, 13, 0, 0): '10 14 30 40 51 01', datetime.datetime(2010, 2, 17, 0, 0): '07 08 19 26 36 15', datetime.datetime(2010, 2, 20, 0, 0): '13 27 37 41 54 32', datetime.datetime(2010, 2, 24, 0, 0): '04 17 35 50 57 12', datetime.datetime(2010, 2, 27, 0, 0): '18 47 51 53 58 30', datetime.datetime(2010, 3, 3, 0, 0): '07 09 14 45 49 23', datetime.datetime(2010, 3, 6, 0, 0): '10 29 33 41 59 15', datetime.datetime(2010, 3, 10, 0, 0): '17 21 37 41 50 01', datetime.datetime(2010, 3, 13, 0, 0): '06 16 20 31 36 08', datetime.datetime(2010, 3, 17, 0, 0): '24 26 45 48 55 08', datetime.datetime(2010, 3, 20, 0, 0): '09 36 39 44 45 09', datetime.datetime(2010, 3, 24, 0, 0): '14 20 24 39 49 07', datetime.datetime(2010, 3, 27, 0, 0): '07 21 32 44 52 10', datetime.datetime(2010, 3, 31, 0, 0): '05 13 17 45 54 12', datetime.datetime(2010, 4, 3, 0, 0): '10 15 31 52 59 04', datetime.datetime(2010, 4, 7, 0, 0): '04 36 40 44 52 33', datetime.datetime(2010, 4, 10, 0, 0): '21 22 49 52 58 34', datetime.datetime(2010, 4, 14, 0, 0): '06 14 32 38 52 20', datetime.datetime(2010, 4, 17, 0, 0): '05 21 22 41 49 15', datetime.datetime(2010, 4, 21, 0, 0): '11 34 41 49 55 20', datetime.datetime(2010, 4, 24, 0, 0): '01 12 53 56 57 05', datetime.datetime(2010, 4, 28, 0, 0): '12 22 25 28 44 24', datetime.datetime(2010, 5, 1, 0, 0): '16 23 25 49 58 20', datetime.datetime(2010, 5, 5, 0, 0): '13 34 40 47 57 11', datetime.datetime(2010, 5, 8, 0, 0): '05 22 34 41 57 31', datetime.datetime(2010, 5, 12, 0, 0): '37 51 52 53 58 38', datetime.datetime(2010, 5, 15, 0, 0): '15 21 23 28 36 20', datetime.datetime(2010, 5, 19, 0, 0): '02 07 29 55 58 27', datetime.datetime(2010, 5, 22, 0, 0): '19 20 40 47 57 29', datetime.datetime(2010, 5, 26, 0, 0): '01 06 10 13 20 32', datetime.datetime(2010, 5, 29, 0, 0): '01 03 24 28 41 10', datetime.datetime(2010, 6, 2, 0, 0): '04 09 14 39 43 38', datetime.datetime(2010, 6, 5, 0, 0): '18 34 40 48 59 25', datetime.datetime(2010, 6, 9, 0, 0): '14 22 27 32 49 05', datetime.datetime(2010, 6, 12, 0, 0): '09 12 13 35 38 30', datetime.datetime(2010, 6, 16, 0, 0): '08 11 18 29 36 06', datetime.datetime(2010, 6, 19, 0, 0): '09 30 31 50 54 39', datetime.datetime(2010, 6, 23, 0, 0): '11 30 45 47 48 10', datetime.datetime(2010, 6, 26, 0, 0): '13 30 32 38 57 25', datetime.datetime(2010, 6, 30, 0, 0): '06 38 43 47 48 27', datetime.datetime(2010, 7, 3, 0, 0): '03 10 14 52 53 03', datetime.datetime(2010, 7, 7, 0, 0): '10 41 44 48 56 04', datetime.datetime(2010, 7, 10, 0, 0): '20 21 27 28 56 04', datetime.datetime(2010, 7, 14, 0, 0): '20 21 23 38 42 06', datetime.datetime(2010, 7, 17, 0, 0): '22 27 35 37 45 03', datetime.datetime(2010, 7, 21, 0, 0): '16 22 30 51 58 25', datetime.datetime(2010, 7, 24, 0, 0): '20 30 38 46 59 27', datetime.datetime(2010, 7, 28, 0, 0): '01 11 20 25 27 02', datetime.datetime(2010, 7, 31, 0, 0): '01 16 17 41 57 15', datetime.datetime(2010, 8, 4, 0, 0): '19 28 30 37 53 36', datetime.datetime(2010, 8, 7, 0, 0): '04 22 26 31 52 30', datetime.datetime(2010, 8, 11, 0, 0): '07 10 22 23 52 29', datetime.datetime(2010, 8, 14, 0, 0): '09 33 36 50 58 31', datetime.datetime(2010, 8, 18, 0, 0): '04 32 33 47 55 39', datetime.datetime(2010, 8, 21, 0, 0): '07 10 12 22 27 26', datetime.datetime(2010, 8, 25, 0, 0): '16 17 29 31 36 23', datetime.datetime(2010, 8, 28, 0, 0): '04 22 27 32 56 13', datetime.datetime(2010, 9, 1, 0, 0): '17 20 21 40 51 19', datetime.datetime(2010, 9, 4, 0, 0): '11 14 22 33 42 38', datetime.datetime(2010, 9, 8, 0, 0): '10 35 39 51 57 20', datetime.datetime(2010, 9, 11, 0, 0): '07 17 20 36 59 33', datetime.datetime(2010, 9, 15, 0, 0): '07 20 21 34 43 34', datetime.datetime(2010, 9, 18, 0, 0): '01 18 37 39 44 13', datetime.datetime(2010, 9, 22, 0, 0): '10 24 36 52 55 15', datetime.datetime(2010, 9, 25, 0, 0): '08 16 27 35 42 30', datetime.datetime(2010, 9, 29, 0, 0): '13 44 51 52 55 30', datetime.datetime(2010, 10, 2, 0, 0): '12 20 30 36 47 25', datetime.datetime(2010, 10, 6, 0, 0): '14 26 37 41 46 24', datetime.datetime(2010, 10, 9, 0, 0): '02 06 32 42 49 35', datetime.datetime(2010, 10, 13, 0, 0): '12 22 32 34 46 02', datetime.datetime(2010, 10, 16, 0, 0): '11 12 15 16 28 11', datetime.datetime(2010, 10, 20, 0, 0): '07 17 20 39 59 17', datetime.datetime(2010, 10, 23, 0, 0): '02 07 16 20 46 34', datetime.datetime(2010, 10, 27, 0, 0): '20 24 25 53 59 15', datetime.datetime(2010, 10, 30, 0, 0): '01 07 27 36 49 39', datetime.datetime(2010, 11, 3, 0, 0): '34 38 39 45 50 33', datetime.datetime(2010, 11, 6, 0, 0): '07 12 23 34 38 33', datetime.datetime(2010, 11, 10, 0, 0): '05 08 11 40 44 10', datetime.datetime(2010, 11, 13, 0, 0): '17 30 48 51 54 29', datetime.datetime(2010, 11, 17, 0, 0): '14 16 53 54 59 05', datetime.datetime(2010, 11, 20, 0, 0): '10 12 38 53 57 01', datetime.datetime(2010, 11, 24, 0, 0): '08 20 21 32 37 04', datetime.datetime(2010, 11, 27, 0, 0): '10 30 37 47 54 39', datetime.datetime(2010, 12, 1, 0, 0): '05 10 11 12 20 02', datetime.datetime(2010, 12, 4, 0, 0): '13 24 27 31 42 22', datetime.datetime(2010, 12, 8, 0, 0): '08 11 25 41 58 16', datetime.datetime(2010, 12, 11, 0, 0): '01 08 10 19 20 23', datetime.datetime(2010, 12, 15, 0, 0): '10 11 18 32 45 18', datetime.datetime(2010, 12, 18, 0, 0): '04 11 19 33 43 14', datetime.datetime(2010, 12, 22, 0, 0): '11 33 44 46 47 12', datetime.datetime(2010, 12, 25, 0, 0): '01 17 38 50 52 24', datetime.datetime(2010, 12, 29, 0, 0): '03 16 18 20 37 30', datetime.datetime(2011, 1, 1, 0, 0): '18 22 37 47 54 36', datetime.datetime(2011, 1, 5, 0, 0): '22 26 32 38 40 07', datetime.datetime(2011, 1, 8, 0, 0): '06 07 26 33 52 24', datetime.datetime(2011, 1, 12, 0, 0): '19 21 23 40 48 27', datetime.datetime(2011, 1, 15, 0, 0): '09 13 22 23 37 31', datetime.datetime(2011, 1, 19, 0, 0): '22 36 51 56 59 32', datetime.datetime(2011, 1, 22, 0, 0): '30 31 34 45 51 23', datetime.datetime(2011, 1, 26, 0, 0): '04 05 36 47 58 06', datetime.datetime(2011, 1, 29, 0, 0): '24 28 45 49 52 02', datetime.datetime(2011, 2, 2, 0, 0): '03 14 33 53 57 36', datetime.datetime(2011, 2, 5, 0, 0): '15 37 41 56 59 05', datetime.datetime(2011, 2, 9, 0, 0): '07 11 39 42 51 30', datetime.datetime(2011, 2, 12, 0, 0): '11 32 36 48 52 19', datetime.datetime(2011, 2, 16, 0, 0): '09 13 21 23 48 24', datetime.datetime(2011, 2, 19, 0, 0): '03 12 34 37 42 36', datetime.datetime(2011, 2, 23, 0, 0): '29 32 36 39 49 29', datetime.datetime(2011, 2, 26, 0, 0): '04 13 17 21 45 10', datetime.datetime(2011, 3, 2, 0, 0): '07 31 50 51 58 06', datetime.datetime(2011, 3, 5, 0, 0): '02 23 31 42 48 21', datetime.datetime(2011, 3, 9, 0, 0): '12 20 28 40 48 08', datetime.datetime(2011, 3, 12, 0, 0): '01 04 12 41 47 03', datetime.datetime(2011, 3, 16, 0, 0): '28 39 40 48 53 09', datetime.datetime(2011, 3, 19, 0, 0): '03 11 20 27 46 08', datetime.datetime(2011, 3, 23, 0, 0): '05 15 26 28 32 09', datetime.datetime(2011, 3, 26, 0, 0): '04 10 11 19 33 27', datetime.datetime(2011, 3, 30, 0, 0): '19 20 42 56 58 37', datetime.datetime(2011, 4, 2, 0, 0): '06 22 34 43 45 23', datetime.datetime(2011, 4, 6, 0, 0): '10 18 41 55 56 15', datetime.datetime(2011, 4, 9, 0, 0): '05 14 32 53 56 11', datetime.datetime(2011, 4, 13, 0, 0): '04 23 39 49 50 39', datetime.datetime(2011, 4, 16, 0, 0): '21 33 44 45 55 07', datetime.datetime(2011, 4, 20, 0, 0): '09 24 34 36 43 27', datetime.datetime(2011, 4, 23, 0, 0): '03 11 47 48 58 19', datetime.datetime(2011, 4, 27, 0, 0): '04 24 40 44 55 05', datetime.datetime(2011, 4, 30, 0, 0): '06 13 15 32 41 03', datetime.datetime(2011, 5, 4, 0, 0): '03 15 27 29 41 24', datetime.datetime(2011, 5, 7, 0, 0): '02 11 27 47 55 15', datetime.datetime(2011, 5, 11, 0, 0): '09 17 32 43 45 31', datetime.datetime(2011, 5, 14, 0, 0): '08 17 18 40 44 16', datetime.datetime(2011, 5, 18, 0, 0): '07 12 13 42 49 16', datetime.datetime(2011, 5, 21, 0, 0): '02 08 40 49 50 36', datetime.datetime(2011, 5, 25, 0, 0): '04 23 31 42 50 23', datetime.datetime(2011, 5, 28, 0, 0): '12 20 43 51 55 11', datetime.datetime(2011, 6, 1, 0, 0): '08 18 38 46 56 31', datetime.datetime(2011, 6, 4, 0, 0): '17 19 39 41 58 21', datetime.datetime(2011, 6, 8, 0, 0): '14 37 44 45 53 29', datetime.datetime(2011, 6, 11, 0, 0): '16 18 27 36 50 08', datetime.datetime(2011, 6, 15, 0, 0): '19 20 38 41 43 29', datetime.datetime(2011, 6, 18, 0, 0): '12 21 22 38 41 18', datetime.datetime(2011, 6, 22, 0, 0): '12 15 19 46 59 12', datetime.datetime(2011, 6, 25, 0, 0): '18 36 39 41 57 12', datetime.datetime(2011, 6, 29, 0, 0): '24 30 45 57 59 26', datetime.datetime(2011, 7, 2, 0, 0): '01 11 18 29 51 32', datetime.datetime(2011, 7, 6, 0, 0): '11 15 24 50 55 08', datetime.datetime(2011, 7, 9, 0, 0): '01 09 11 23 31 06', datetime.datetime(2011, 7, 13, 0, 0): '08 18 19 32 54 08', datetime.datetime(2011, 7, 16, 0, 0): '24 28 48 50 54 25', datetime.datetime(2011, 7, 20, 0, 0): '01 04 38 40 42 17', datetime.datetime(2011, 7, 23, 0, 0): '01 07 27 38 48 30', datetime.datetime(2011, 7, 27, 0, 0): '38 40 41 51 59 33', datetime.datetime(2011, 7, 30, 0, 0): '20 40 41 47 55 19', datetime.datetime(2011, 8, 3, 0, 0): '13 19 21 28 49 11', datetime.datetime(2011, 8, 6, 0, 0): '25 30 54 57 59 06', datetime.datetime(2011, 8, 10, 0, 0): '11 18 36 41 46 38', datetime.datetime(2011, 8, 13, 0, 0): '09 12 35 50 58 04', datetime.datetime(2011, 8, 17, 0, 0): '18 28 31 48 52 37', datetime.datetime(2011, 8, 20, 0, 0): '02 17 23 28 47 36', datetime.datetime(2011, 8, 24, 0, 0): '09 13 47 49 53 39', datetime.datetime(2011, 8, 27, 0, 0): '02 12 25 54 58 14', datetime.datetime(2011, 8, 31, 0, 0): '13 19 35 47 57 29', datetime.datetime(2011, 9, 3, 0, 0): '15 25 52 53 54 02', datetime.datetime(2011, 9, 7, 0, 0): '03 05 18 27 54 13', datetime.datetime(2011, 9, 10, 0, 0): '04 19 22 32 53 24', datetime.datetime(2011, 9, 14, 0, 0): '16 41 42 50 59 05', datetime.datetime(2011, 9, 17, 0, 0): '06 20 22 32 43 11', datetime.datetime(2011, 9, 21, 0, 0): '12 47 48 52 55 13', datetime.datetime(2011, 9, 24, 0, 0): '03 04 12 27 44 26', datetime.datetime(2011, 9, 28, 0, 0): '30 41 50 51 53 08', datetime.datetime(2011, 10, 1, 0, 0): '01 12 23 27 43 31', datetime.datetime(2011, 10, 5, 0, 0): '07 20 43 46 54 17', datetime.datetime(2011, 10, 8, 0, 0): '03 27 35 37 45 31', datetime.datetime(2011, 10, 12, 0, 0): '10 12 23 43 47 18', datetime.datetime(2011, 10, 15, 0, 0): '05 10 24 38 43 01', datetime.datetime(2011, 10, 19, 0, 0): '16 26 35 52 58 02', datetime.datetime(2011, 10, 22, 0, 0): '03 08 23 30 58 13', datetime.datetime(2011, 10, 26, 0, 0): '01 18 21 39 55 06', datetime.datetime(2011, 10, 29, 0, 0): '11 16 40 51 56 38', datetime.datetime(2011, 11, 2, 0, 0): '12 14 34 39 46 36', datetime.datetime(2011, 11, 5, 0, 0): '02 33 39 40 43 26', datetime.datetime(2011, 11, 9, 0, 0): '05 35 57 58 59 12', datetime.datetime(2011, 11, 12, 0, 0): '04 35 36 51 56 08', datetime.datetime(2011, 11, 16, 0, 0): '13 22 25 39 51 28', datetime.datetime(2011, 11, 19, 0, 0): '09 16 17 28 30 11', datetime.datetime(2011, 11, 23, 0, 0): '04 30 35 57 59 25', datetime.datetime(2011, 11, 26, 0, 0): '20 37 39 45 55 28', datetime.datetime(2011, 11, 30, 0, 0): '02 06 34 35 47 22', datetime.datetime(2011, 12, 3, 0, 0): '05 18 33 43 45 08', datetime.datetime(2011, 12, 7, 0, 0): '03 14 20 39 40 37', datetime.datetime(2011, 12, 10, 0, 0): '04 19 33 41 59 09', datetime.datetime(2011, 12, 14, 0, 0): '02 24 46 52 56 19', datetime.datetime(2011, 12, 17, 0, 0): '13 28 49 51 59 33', datetime.datetime(2011, 12, 21, 0, 0): '10 13 15 31 54 18', datetime.datetime(2011, 12, 24, 0, 0): '14 16 30 51 52 19', datetime.datetime(2011, 12, 28, 0, 0): '16 21 27 41 45 14', datetime.datetime(2011, 12, 31, 0, 0): '05 23 25 28 40 34', datetime.datetime(2012, 1, 4, 0, 0): '21 35 46 47 50 02', datetime.datetime(2012, 1, 7, 0, 0): '03 21 24 38 39 24', datetime.datetime(2012, 1, 11, 0, 0): '05 19 29 45 47 25', datetime.datetime(2012, 1, 14, 0, 0): '10 30 36 38 41 01', datetime.datetime(2012, 1, 18, 0, 0): '06 29 34 44 50 28', datetime.datetime(2012, 1, 21, 0, 0): '12 24 43 44 45 07', datetime.datetime(2012, 1, 25, 0, 0): '04 19 28 29 47 05', datetime.datetime(2012, 1, 28, 0, 0): '05 33 41 54 59 13', datetime.datetime(2012, 2, 1, 0, 0): '08 13 17 34 59 35', datetime.datetime(2012, 2, 4, 0, 0): '15 23 43 45 56 07', datetime.datetime(2012, 2, 8, 0, 0): '17 28 38 39 51 33', datetime.datetime(2012, 2, 11, 0, 0): '01 10 37 52 57 11', datetime.datetime(2012, 2, 15, 0, 0): '11 12 32 52 56 11', datetime.datetime(2012, 2, 18, 0, 0): '23 28 50 56 59 05', datetime.datetime(2012, 2, 22, 0, 0): '07 16 17 39 51 32', datetime.datetime(2012, 2, 25, 0, 0): '06 11 42 53 54 07', datetime.datetime(2012, 2, 29, 0, 0): '01 04 11 23 26 14', datetime.datetime(2012, 3, 3, 0, 0): '29 30 45 47 49 35', datetime.datetime(2012, 3, 7, 0, 0): '12 35 45 46 47 12', datetime.datetime(2012, 3, 10, 0, 0): '05 14 17 20 41 05', datetime.datetime(2012, 3, 14, 0, 0): '01 08 41 46 59 24', datetime.datetime(2012, 3, 17, 0, 0): '11 14 49 55 58 30', datetime.datetime(2012, 3, 21, 0, 0): '32 43 53 55 56 06', datetime.datetime(2012, 3, 24, 0, 0): '01 15 35 37 47 08', datetime.datetime(2012, 3, 28, 0, 0): '11 16 29 50 58 33', datetime.datetime(2012, 3, 31, 0, 0): '05 14 36 54 58 27', datetime.datetime(2012, 4, 4, 0, 0): '01 24 33 45 49 06', datetime.datetime(2012, 4, 7, 0, 0): '05 13 17 20 30 18', datetime.datetime(2012, 4, 11, 0, 0): '16 23 42 44 47 02', datetime.datetime(2012, 4, 14, 0, 0): '14 15 16 19 24 02', datetime.datetime(2012, 4, 18, 0, 0): '20 22 39 46 49 29', datetime.datetime(2012, 4, 21, 0, 0): '06 08 20 42 51 16', datetime.datetime(2012, 4, 25, 0, 0): '04 25 29 34 43 29', datetime.datetime(2012, 4, 28, 0, 0): '31 39 40 57 58 33', datetime.datetime(2012, 5, 2, 0, 0): '07 08 33 38 50 29', datetime.datetime(2012, 5, 5, 0, 0): '09 12 20 44 59 23', datetime.datetime(2012, 5, 9, 0, 0): '01 07 11 55 56 01', datetime.datetime(2012, 5, 12, 0, 0): '10 24 35 53 58 22', datetime.datetime(2012, 5, 16, 0, 0): '03 07 21 28 43 02', datetime.datetime(2012, 5, 19, 0, 0): '08 13 35 46 51 30', datetime.datetime(2012, 5, 23, 0, 0): '04 07 26 53 59 32', datetime.datetime(2012, 5, 26, 0, 0): '13 14 41 49 59 14', datetime.datetime(2012, 5, 30, 0, 0): '09 10 24 52 56 14', datetime.datetime(2012, 6, 2, 0, 0): '09 10 17 29 45 33', datetime.datetime(2012, 6, 6, 0, 0): '19 30 33 48 59 27', datetime.datetime(2012, 6, 9, 0, 0): '18 22 45 56 57 27', datetime.datetime(2012, 6, 13, 0, 0): '07 10 14 33 57 18', datetime.datetime(2012, 6, 16, 0, 0): '08 14 15 16 27 26', datetime.datetime(2012, 6, 20, 0, 0): '11 17 29 56 57 14', datetime.datetime(2012, 6, 23, 0, 0): '01 03 41 44 53 30', datetime.datetime(2012, 6, 27, 0, 0): '06 34 40 46 58 06', datetime.datetime(2012, 6, 30, 0, 0): '07 15 20 41 44 22', datetime.datetime(2012, 7, 4, 0, 0): '14 19 35 39 56 33', datetime.datetime(2012, 7, 7, 0, 0): '03 05 29 39 59 29', datetime.datetime(2012, 7, 11, 0, 0): '05 22 36 49 55 23', datetime.datetime(2012, 7, 14, 0, 0): '04 16 32 37 46 13', datetime.datetime(2012, 7, 18, 0, 0): '02 05 20 23 57 03', datetime.datetime(2012, 7, 21, 0, 0): '09 31 38 54 56 20', datetime.datetime(2012, 7, 25, 0, 0): '03 14 35 38 46 16', datetime.datetime(2012, 7, 28, 0, 0): '05 06 13 36 50 13', datetime.datetime(2012, 8, 1, 0, 0): '03 16 48 56 58 04', datetime.datetime(2012, 8, 4, 0, 0): '19 30 48 53 55 18', datetime.datetime(2012, 8, 8, 0, 0): '03 07 11 15 28 12', datetime.datetime(2012, 8, 11, 0, 0): '04 13 39 46 51 01', datetime.datetime(2012, 8, 15, 0, 0): '06 27 46 51 56 21', datetime.datetime(2012, 8, 18, 0, 0): '14 26 41 55 59 01', datetime.datetime(2012, 8, 22, 0, 0): '22 29 31 47 55 19', datetime.datetime(2012, 8, 25, 0, 0): '01 06 07 20 49 23', datetime.datetime(2012, 8, 29, 0, 0): '25 28 49 54 56 28', datetime.datetime(2012, 9, 1, 0, 0): '08 11 21 44 49 22', datetime.datetime(2012, 9, 5, 0, 0): '04 19 26 42 51 29', datetime.datetime(2012, 9, 8, 0, 0): '06 20 34 44 48 29', datetime.datetime(2012, 9, 12, 0, 0): '24 33 36 48 56 06', datetime.datetime(2012, 9, 15, 0, 0): '03 20 26 43 48 01', datetime.datetime(2012, 9, 19, 0, 0): '01 05 08 39 50 23', datetime.datetime(2012, 9, 22, 0, 0): '02 16 18 40 42 33', datetime.datetime(2012, 9, 26, 0, 0): '13 26 39 41 42 10', datetime.datetime(2012, 9, 29, 0, 0): '14 18 28 29 57 08', datetime.datetime(2012, 10, 3, 0, 0): '17 23 36 55 59 10', datetime.datetime(2012, 10, 6, 0, 0): '15 26 34 36 59 35', datetime.datetime(2012, 10, 10, 0, 0): '18 26 29 35 43 28', datetime.datetime(2012, 10, 13, 0, 0): '02 05 25 26 49 18', datetime.datetime(2012, 10, 17, 0, 0): '01 07 10 23 42 35', datetime.datetime(2012, 10, 20, 0, 0): '04 21 28 31 44 10', datetime.datetime(2012, 10, 24, 0, 0): '03 18 21 23 50 04', datetime.datetime(2012, 10, 27, 0, 0): '22 32 34 36 56 33', datetime.datetime(2012, 10, 31, 0, 0): '01 27 31 45 48 05', datetime.datetime(2012, 11, 3, 0, 0): '04 07 09 30 54 25', datetime.datetime(2012, 11, 7, 0, 0): '32 34 45 52 58 20', datetime.datetime(2012, 11, 10, 0, 0): '32 42 50 54 55 32', datetime.datetime(2012, 11, 14, 0, 0): '08 10 30 44 58 13', datetime.datetime(2012, 11, 17, 0, 0): '03 15 27 58 59 20', datetime.datetime(2012, 11, 21, 0, 0): '08 18 24 30 39 26', datetime.datetime(2012, 11, 24, 0, 0): '22 32 37 44 50 34', datetime.datetime(2012, 11, 28, 0, 0): '05 16 22 23 29 06', datetime.datetime(2012, 12, 1, 0, 0): '03 10 19 36 46 03', datetime.datetime(2012, 12, 5, 0, 0): '13 17 19 27 38 12', datetime.datetime(2012, 12, 8, 0, 0): '07 23 26 40 53 21', datetime.datetime(2012, 12, 12, 0, 0): '08 10 25 36 44 28', datetime.datetime(2012, 12, 15, 0, 0): '15 23 40 44 55 14', datetime.datetime(2012, 12, 19, 0, 0): '05 08 20 23 30 03', datetime.datetime(2012, 12, 22, 0, 0): '01 18 35 39 44 11', datetime.datetime(2012, 12, 26, 0, 0): '11 13 23 43 54 04', datetime.datetime(2012, 12, 29, 0, 0): '36 46 50 52 55 14', datetime.datetime(2013, 1, 2, 0, 0): '18 20 28 35 53 20', datetime.datetime(2013, 1, 5, 0, 0): '26 30 49 51 54 25', datetime.datetime(2013, 1, 9, 0, 0): '11 13 20 27 59 26', datetime.datetime(2013, 1, 12, 0, 0): '10 14 21 23 47 07', datetime.datetime(2013, 1, 16, 0, 0): '09 21 28 32 51 35', datetime.datetime(2013, 1, 19, 0, 0): '08 28 29 34 38 35', datetime.datetime(2013, 1, 23, 0, 0): '11 12 24 43 45 09', datetime.datetime(2013, 1, 26, 0, 0): '03 22 26 41 49 18', datetime.datetime(2013, 1, 30, 0, 0): '14 16 32 47 52 16', datetime.datetime(2013, 2, 2, 0, 0): '11 16 33 40 41 34', datetime.datetime(2013, 2, 6, 0, 0): '05 27 36 38 41 12', datetime.datetime(2013, 2, 9, 0, 0): '05 06 16 36 58 03', datetime.datetime(2013, 2, 13, 0, 0): '12 23 25 27 43 29', datetime.datetime(2013, 2, 16, 0, 0): '15 16 46 50 58 29', datetime.datetime(2013, 2, 20, 0, 0): '03 17 19 25 32 17', datetime.datetime(2013, 2, 23, 0, 0): '02 05 31 39 41 29', datetime.datetime(2013, 2, 27, 0, 0): '03 14 20 34 48 21', datetime.datetime(2013, 3, 2, 0, 0): '03 08 13 41 56 16', datetime.datetime(2013, 3, 6, 0, 0): '06 10 23 41 45 01', datetime.datetime(2013, 3, 9, 0, 0): '10 37 40 46 52 12', datetime.datetime(2013, 3, 13, 0, 0): '05 09 28 32 38 29', datetime.datetime(2013, 3, 16, 0, 0): '03 07 21 44 53 16', datetime.datetime(2013, 3, 20, 0, 0): '13 14 17 43 54 15', datetime.datetime(2013, 3, 23, 0, 0): '17 29 31 52 53 31', datetime.datetime(2013, 3, 27, 0, 0): '07 37 43 48 52 16', datetime.datetime(2013, 3, 30, 0, 0): '11 23 26 46 55 27', datetime.datetime(2013, 4, 3, 0, 0): '01 06 08 12 35 03', datetime.datetime(2013, 4, 6, 0, 0): '04 07 08 29 39 24', datetime.datetime(2013, 4, 10, 0, 0): '01 36 40 52 53 20', datetime.datetime(2013, 4, 13, 0, 0): '10 12 31 56 57 33', datetime.datetime(2013, 4, 17, 0, 0): '13 18 36 48 58 28', datetime.datetime(2013, 4, 20, 0, 0): '06 08 30 39 48 20', datetime.datetime(2013, 4, 24, 0, 0): '09 19 31 56 59 02', datetime.datetime(2013, 4, 27, 0, 0): '03 23 48 54 55 05', datetime.datetime(2013, 5, 1, 0, 0): '22 26 31 54 55 18', datetime.datetime(2013, 5, 4, 0, 0): '07 12 26 36 40 17', datetime.datetime(2013, 5, 8, 0, 0): '21 22 26 30 57 27', datetime.datetime(2013, 5, 11, 0, 0): '06 13 19 23 43 16', datetime.datetime(2013, 5, 15, 0, 0): '02 11 26 34 41 32', datetime.datetime(2013, 5, 18, 0, 0): '10 13 14 22 52 11', datetime.datetime(2013, 5, 22, 0, 0): '09 31 35 41 57 26', datetime.datetime(2013, 5, 25, 0, 0): '02 06 19 21 27 25', datetime.datetime(2013, 5, 29, 0, 0): '09 14 17 49 57 02', datetime.datetime(2013, 6, 1, 0, 0): '22 28 33 53 59 14', datetime.datetime(2013, 6, 5, 0, 0): '04 26 33 36 55 32', datetime.datetime(2013, 6, 8, 0, 0): '02 11 22 26 32 19', datetime.datetime(2013, 6, 12, 0, 0): '16 22 23 42 55 32', datetime.datetime(2013, 6, 15, 0, 0): '28 36 40 48 55 01', datetime.datetime(2013, 6, 19, 0, 0): '07 46 47 52 57 17', datetime.datetime(2013, 6, 22, 0, 0): '13 19 23 33 57 28', datetime.datetime(2013, 6, 26, 0, 0): '01 18 33 39 46 33', datetime.datetime(2013, 6, 29, 0, 0): '08 28 30 53 56 16', datetime.datetime(2013, 7, 3, 0, 0): '03 06 29 40 51 04', datetime.datetime(2013, 7, 6, 0, 0): '02 13 35 36 52 11', datetime.datetime(2013, 7, 10, 0, 0): '30 31 45 55 59 27', datetime.datetime(2013, 7, 13, 0, 0): '02 08 22 35 37 06', datetime.datetime(2013, 7, 17, 0, 0): '01 22 34 38 42 17', datetime.datetime(2013, 7, 20, 0, 0): '14 25 27 38 58 06', datetime.datetime(2013, 7, 24, 0, 0): '09 29 40 44 54 07', datetime.datetime(2013, 7, 27, 0, 0): '09 23 40 53 58 06', datetime.datetime(2013, 7, 31, 0, 0): '08 24 39 49 59 05', datetime.datetime(2013, 8, 3, 0, 0): '21 24 36 42 45 15', datetime.datetime(2013, 8, 7, 0, 0): '05 25 30 58 59 32', datetime.datetime(2013, 8, 10, 0, 0): '04 12 14 37 58 13', datetime.datetime(2013, 8, 14, 0, 0): '04 11 17 43 51 20', datetime.datetime(2013, 8, 17, 0, 0): '18 21 46 54 56 23', datetime.datetime(2013, 8, 21, 0, 0): '30 40 42 46 48 23', datetime.datetime(2013, 8, 24, 0, 0): '12 17 25 45 59 19', datetime.datetime(2013, 8, 28, 0, 0): '06 07 09 19 32 13', datetime.datetime(2013, 8, 31, 0, 0): '02 07 25 40 56 20', datetime.datetime(2013, 9, 4, 0, 0): '02 09 26 45 47 11', datetime.datetime(2013, 9, 7, 0, 0): '02 19 22 26 45 24', datetime.datetime(2013, 9, 11, 0, 0): '11 19 33 42 52 33', datetime.datetime(2013, 9, 14, 0, 0): '01 17 25 37 44 20', datetime.datetime(2013, 9, 18, 0, 0): '07 10 22 32 35 19', datetime.datetime(2013, 9, 21, 0, 0): '12 17 45 54 58 13', datetime.datetime(2013, 9, 25, 0, 0): '02 07 17 49 53 23', datetime.datetime(2013, 9, 28, 0, 0): '14 47 52 53 54 05', datetime.datetime(2013, 10, 2, 0, 0): '04 06 25 42 51 17', datetime.datetime(2013, 10, 5, 0, 0): '11 12 17 39 40 05', datetime.datetime(2013, 10, 9, 0, 0): '03 09 19 33 38 18', datetime.datetime(2013, 10, 12, 0, 0): '08 10 26 57 58 04', datetime.datetime(2013, 10, 16, 0, 0): '03 26 28 34 42 28', datetime.datetime(2013, 10, 19, 0, 0): '09 33 54 56 57 05', datetime.datetime(2013, 10, 23, 0, 0): '03 23 31 34 47 13', datetime.datetime(2013, 10, 26, 0, 0): '04 06 34 49 56 29', datetime.datetime(2013, 10, 30, 0, 0): '02 36 40 49 54 10', datetime.datetime(2013, 11, 2, 0, 0): '13 23 24 27 40 17', datetime.datetime(2013, 11, 6, 0, 0): '01 05 10 15 49 22', datetime.datetime(2013, 11, 9, 0, 0): '03 09 37 49 56 32', datetime.datetime(2013, 11, 13, 0, 0): '05 31 50 55 56 09', datetime.datetime(2013, 11, 16, 0, 0): '10 29 37 44 59 10', datetime.datetime(2013, 11, 20, 0, 0): '04 18 23 32 45 07', datetime.datetime(2013, 11, 23, 0, 0): '05 12 43 52 55 10', datetime.datetime(2013, 11, 27, 0, 0): '18 25 50 55 57 17', datetime.datetime(2013, 11, 30, 0, 0): '05 26 44 45 57 29', datetime.datetime(2013, 12, 4, 0, 0): '06 09 11 31 44 25', datetime.datetime(2013, 12, 7, 0, 0): '13 20 32 45 48 17', datetime.datetime(2013, 12, 11, 0, 0): '01 10 13 18 19 27', datetime.datetime(2013, 12, 14, 0, 0): '14 25 32 33 41 34', datetime.datetime(2013, 12, 18, 0, 0): '07 24 37 39 40 01', datetime.datetime(2013, 12, 21, 0, 0): '25 36 40 45 51 08', datetime.datetime(2013, 12, 25, 0, 0): '23 28 38 39 56 32', datetime.datetime(2013, 12, 28, 0, 0): '08 35 44 51 56 18', datetime.datetime(2014, 1, 1, 0, 0): '15 24 40 48 52 23', datetime.datetime(2014, 1, 4, 0, 0): '19 20 37 41 58 14', datetime.datetime(2014, 1, 8, 0, 0): '10 28 39 47 58 22', datetime.datetime(2014, 1, 11, 0, 0): '10 15 33 48 54 34', datetime.datetime(2014, 1, 15, 0, 0): '07 08 09 24 29 25', datetime.datetime(2014, 1, 18, 0, 0): '13 14 19 31 38 25', datetime.datetime(2014, 1, 22, 0, 0): '01 02 07 09 55 29', datetime.datetime(2014, 1, 25, 0, 0): '08 12 18 55 57 02', datetime.datetime(2014, 1, 29, 0, 0): '11 23 28 32 47 20', datetime.datetime(2014, 2, 1, 0, 0): '05 12 15 27 38 07', datetime.datetime(2014, 2, 5, 0, 0): '08 17 32 57 59 24', datetime.datetime(2014, 2, 8, 0, 0): '24 25 34 37 54 29', datetime.datetime(2014, 2, 12, 0, 0): '36 44 49 52 57 01', datetime.datetime(2014, 2, 15, 0, 0): '02 09 14 21 23 03', datetime.datetime(2014, 2, 19, 0, 0): '01 17 35 49 54 34', datetime.datetime(2014, 2, 22, 0, 0): '02 03 13 14 54 04', datetime.datetime(2014, 2, 26, 0, 0): '11 12 17 38 42 02', datetime.datetime(2014, 3, 1, 0, 0): '03 08 25 30 47 13', datetime.datetime(2014, 3, 5, 0, 0): '03 07 09 26 54 19', datetime.datetime(2014, 3, 8, 0, 0): '10 14 24 32 41 30', datetime.datetime(2014, 3, 12, 0, 0): '14 15 28 37 54 10', datetime.datetime(2014, 3, 15, 0, 0): '02 05 34 51 58 09', datetime.datetime(2014, 3, 19, 0, 0): '02 19 23 34 43 14', datetime.datetime(2014, 3, 22, 0, 0): '13 28 31 55 58 15', datetime.datetime(2014, 3, 26, 0, 0): '28 33 41 44 59 21', datetime.datetime(2014, 3, 29, 0, 0): '02 03 12 27 38 17', datetime.datetime(2014, 4, 2, 0, 0): '08 13 19 22 53 24', datetime.datetime(2014, 4, 5, 0, 0): '11 21 26 33 34 29', datetime.datetime(2014, 4, 9, 0, 0): '09 14 44 48 49 29', datetime.datetime(2014, 4, 12, 0, 0): '14 26 45 54 55 20', datetime.datetime(2014, 4, 16, 0, 0): '34 39 42 44 59 08', datetime.datetime(2014, 4, 19, 0, 0): '05 06 29 35 51 21', datetime.datetime(2014, 4, 23, 0, 0): '19 25 29 36 48 12', datetime.datetime(2014, 4, 26, 0, 0): '03 07 22 30 33 20', datetime.datetime(2014, 4, 30, 0, 0): '02 09 11 19 50 32', datetime.datetime(2014, 5, 3, 0, 0): '05 15 16 46 49 26', datetime.datetime(2014, 5, 7, 0, 0): '17 29 31 48 49 34', datetime.datetime(2014, 5, 10, 0, 0): '04 31 41 47 55 01', datetime.datetime(2014, 5, 14, 0, 0): '07 33 39 52 55 33', datetime.datetime(2014, 5, 17, 0, 0): '23 32 39 47 49 22', datetime.datetime(2014, 5, 21, 0, 0): '04 20 34 39 58 31', datetime.datetime(2014, 5, 24, 0, 0): '15 16 28 49 55 18', datetime.datetime(2014, 5, 28, 0, 0): '02 24 28 32 59 25', datetime.datetime(2014, 5, 31, 0, 0): '15 27 31 34 48 01', datetime.datetime(2014, 6, 4, 0, 0): '01 07 10 22 49 24', datetime.datetime(2014, 6, 7, 0, 0): '28 30 35 58 59 15', datetime.datetime(2014, 6, 11, 0, 0): '14 18 25 33 49 23', datetime.datetime(2014, 6, 14, 0, 0): '09 33 42 45 54 30', datetime.datetime(2014, 6, 18, 0, 0): '06 09 29 52 59 07', datetime.datetime(2014, 6, 21, 0, 0): '05 06 37 41 54 26', datetime.datetime(2014, 6, 25, 0, 0): '10 20 25 50 53 35', datetime.datetime(2014, 7, 2, 0, 0): '08 18 45 53 58 35', datetime.datetime(2014, 7, 5, 0, 0): '24 34 36 57 58 11', datetime.datetime(2014, 7, 9, 0, 0): '09 25 42 55 57 14', datetime.datetime(2014, 7, 12, 0, 0): '02 03 07 23 51 26', datetime.datetime(2014, 7, 16, 0, 0): '05 15 18 26 32 35', datetime.datetime(2014, 7, 19, 0, 0): '10 17 25 45 53 09', datetime.datetime(2014, 7, 23, 0, 0): '04 10 12 22 31 03', datetime.datetime(2014, 7, 26, 0, 0): '24 28 30 38 39 16', datetime.datetime(2014, 7, 30, 0, 0): '13 30 42 49 53 29', datetime.datetime(2014, 8, 2, 0, 0): '12 26 44 46 47 29', datetime.datetime(2014, 8, 6, 0, 0): '01 08 24 28 49 24', datetime.datetime(2014, 8, 9, 0, 0): '03 12 31 34 51 24', datetime.datetime(2014, 8, 13, 0, 0): '08 37 39 40 52 24', datetime.datetime(2014, 8, 16, 0, 0): '07 08 17 48 59 09', datetime.datetime(2014, 8, 20, 0, 0): '04 08 21 38 40 03', datetime.datetime(2014, 8, 23, 0, 0): '28 32 35 36 52 31', datetime.datetime(2014, 8, 27, 0, 0): '17 24 26 45 46 19', datetime.datetime(2014, 8, 30, 0, 0): '05 28 31 52 59 27', datetime.datetime(2014, 9, 3, 0, 0): '02 16 43 45 51 35', datetime.datetime(2014, 9, 6, 0, 0): '09 29 31 43 50 18', datetime.datetime(2014, 9, 10, 0, 0): '02 14 39 40 43 13', datetime.datetime(2014, 9, 13, 0, 0): '01 06 16 37 53 27', datetime.datetime(2014, 9, 17, 0, 0): '18 25 36 48 50 23', datetime.datetime(2014, 9, 20, 0, 0): '22 23 30 37 39 16', datetime.datetime(2014, 9, 24, 0, 0): '07 14 21 24 41 26', datetime.datetime(2014, 9, 27, 0, 0): '02 11 35 52 54 13', datetime.datetime(2014, 10, 1, 0, 0): '01 04 18 20 45 07', datetime.datetime(2014, 10, 4, 0, 0): '13 18 24 25 33 31', datetime.datetime(2014, 10, 8, 0, 0): '05 16 31 46 50 18', datetime.datetime(2014, 10, 11, 0, 0): '10 19 37 38 39 28', datetime.datetime(2014, 10, 15, 0, 0): '05 07 19 27 28 20', datetime.datetime(2014, 10, 18, 0, 0): '20 26 27 36 54 19', datetime.datetime(2014, 10, 22, 0, 0): '29 30 40 42 50 16', datetime.datetime(2014, 10, 25, 0, 0): '06 10 51 54 57 12', datetime.datetime(2014, 10, 29, 0, 0): '25 28 48 57 59 16', datetime.datetime(2014, 11, 1, 0, 0): '01 03 13 25 38 17', datetime.datetime(2014, 11, 5, 0, 0): '02 11 19 21 42 34', datetime.datetime(2014, 11, 8, 0, 0): '09 19 33 38 54 15', datetime.datetime(2014, 11, 12, 0, 0): '37 39 51 52 55 11', datetime.datetime(2014, 11, 15, 0, 0): '13 16 33 35 51 28', datetime.datetime(2014, 11, 19, 0, 0): '06 36 38 48 51 17', datetime.datetime(2014, 11, 22, 0, 0): '23 49 53 54 57 35', datetime.datetime(2014, 11, 26, 0, 0): '16 17 22 46 54 35', datetime.datetime(2014, 11, 29, 0, 0): '13 24 30 42 48 27', datetime.datetime(2014, 12, 3, 0, 0): '25 30 32 46 54 26', datetime.datetime(2014, 12, 6, 0, 0): '12 15 22 43 49 14', datetime.datetime(2014, 12, 10, 0, 0): '34 44 48 54 55 10', datetime.datetime(2014, 12, 13, 0, 0): '05 13 28 43 55 33', datetime.datetime(2014, 12, 17, 0, 0): '22 31 38 47 48 15', datetime.datetime(2014, 12, 20, 0, 0): '14 15 19 31 56 05', datetime.datetime(2014, 12, 24, 0, 0): '11 12 46 47 50 22', datetime.datetime(2014, 12, 27, 0, 0): '07 10 11 14 36 15', datetime.datetime(2014, 12, 31, 0, 0): '17 27 37 40 53 35', datetime.datetime(2015, 1, 3, 0, 0): '04 18 43 46 55 25', datetime.datetime(2015, 1, 7, 0, 0): '14 15 47 49 59 10', datetime.datetime(2015, 1, 10, 0, 0): '02 09 19 28 29 19', datetime.datetime(2015, 1, 14, 0, 0): '02 04 10 41 53 22', datetime.datetime(2015, 1, 17, 0, 0): '15 16 23 27 36 09', datetime.datetime(2015, 1, 21, 0, 0): '11 12 15 28 57 23', datetime.datetime(2015, 1, 24, 0, 0): '16 19 20 29 33 10', datetime.datetime(2015, 1, 28, 0, 0): '12 24 35 36 49 01', datetime.datetime(2015, 1, 31, 0, 0): '05 11 16 26 50 34', datetime.datetime(2015, 2, 4, 0, 0): '24 36 51 52 56 22', datetime.datetime(2015, 2, 7, 0, 0): '05 10 21 34 58 33', datetime.datetime(2015, 2, 11, 0, 0): '11 13 25 39 54 19', datetime.datetime(2015, 2, 14, 0, 0): '01 24 44 45 51 28', datetime.datetime(2015, 2, 18, 0, 0): '01 09 29 32 49 22', datetime.datetime(2015, 2, 21, 0, 0): '10 14 18 34 51 26', datetime.datetime(2015, 2, 25, 0, 0): '17 19 21 32 39 08', datetime.datetime(2015, 2, 28, 0, 0): '11 17 25 28 46 12', datetime.datetime(2015, 3, 4, 0, 0): '08 12 15 35 50 32', datetime.datetime(2015, 3, 7, 0, 0): '34 36 38 42 50 33', datetime.datetime(2015, 3, 11, 0, 0): '11 24 31 40 44 27', datetime.datetime(2015, 3, 14, 0, 0): '08 14 39 46 47 18', datetime.datetime(2015, 3, 18, 0, 0): '14 25 30 33 47 08', datetime.datetime(2015, 3, 21, 0, 0): '11 16 30 38 42 07', datetime.datetime(2015, 3, 25, 0, 0): '07 19 23 50 54 14', datetime.datetime(2015, 3, 28, 0, 0): '02 04 06 12 38 17', datetime.datetime(2015, 4, 1, 0, 0): '02 30 33 39 44 01', datetime.datetime(2015, 4, 4, 0, 0): '33 39 40 41 54 28', datetime.datetime(2015, 4, 8, 0, 0): '01 19 45 46 58 29', datetime.datetime(2015, 4, 11, 0, 0): '01 12 32 42 58 12', datetime.datetime(2015, 4, 15, 0, 0): '01 16 21 29 40 30', datetime.datetime(2015, 4, 18, 0, 0): '13 22 23 29 31 17', datetime.datetime(2015, 4, 22, 0, 0): '10 14 25 39 53 18', datetime.datetime(2015, 4, 25, 0, 0): '21 33 35 38 45 12', datetime.datetime(2015, 4, 29, 0, 0): '01 26 34 38 51 06', datetime.datetime(2015, 5, 2, 0, 0): '02 06 11 30 31 33', datetime.datetime(2015, 5, 6, 0, 0): '23 24 27 39 41 30', datetime.datetime(2015, 5, 9, 0, 0): '04 15 17 35 58 17', datetime.datetime(2015, 5, 13, 0, 0): '01 25 29 31 47 07', datetime.datetime(2015, 5, 16, 0, 0): '24 29 38 48 52 32', datetime.datetime(2015, 5, 20, 0, 0): '01 12 28 35 44 25', datetime.datetime(2015, 5, 23, 0, 0): '09 15 17 31 43 16', datetime.datetime(2015, 5, 27, 0, 0): '08 15 34 53 59 23', datetime.datetime(2015, 5, 30, 0, 0): '08 09 25 56 57 22', datetime.datetime(2015, 6, 3, 0, 0): '06 08 13 37 40 11', datetime.datetime(2015, 6, 6, 0, 0): '08 13 18 27 43 15', datetime.datetime(2015, 6, 10, 0, 0): '31 32 48 49 53 25', datetime.datetime(2015, 6, 13, 0, 0): '29 41 48 52 54 29', datetime.datetime(2015, 6, 17, 0, 0): '20 21 22 41 54 07', datetime.datetime(2015, 6, 20, 0, 0): '09 10 16 20 57 15', datetime.datetime(2015, 6, 24, 0, 0): '03 05 10 22 32 07', datetime.datetime(2015, 6, 27, 0, 0): '18 28 35 46 49 27', datetime.datetime(2015, 7, 1, 0, 0): '07 24 26 31 41 25', datetime.datetime(2015, 7, 4, 0, 0): '03 06 14 18 24 21', datetime.datetime(2015, 7, 8, 0, 0): '04 15 25 27 30 18', datetime.datetime(2015, 7, 11, 0, 0): '11 39 46 52 54 03', datetime.datetime(2015, 7, 15, 0, 0): '13 16 34 45 50 11', datetime.datetime(2015, 7, 18, 0, 0): '06 37 39 45 55 33', datetime.datetime(2015, 7, 22, 0, 0): '12 31 43 44 57 11', datetime.datetime(2015, 7, 25, 0, 0): '27 29 34 41 44 02', datetime.datetime(2015, 7, 29, 0, 0): '04 22 27 28 52 35', datetime.datetime(2015, 8, 1, 0, 0): '07 13 24 49 57 15', datetime.datetime(2015, 8, 5, 0, 0): '09 11 14 16 42 19', datetime.datetime(2015, 8, 8, 0, 0): '09 34 48 52 54 15', datetime.datetime(2015, 8, 12, 0, 0): '08 13 29 38 52 28', datetime.datetime(2015, 8, 15, 0, 0): '03 13 17 42 52 24', datetime.datetime(2015, 8, 19, 0, 0): '06 08 43 48 50 07', datetime.datetime(2015, 8, 22, 0, 0): '04 12 14 21 55 07', datetime.datetime(2015, 8, 26, 0, 0): '02 22 32 45 56 12', datetime.datetime(2015, 8, 29, 0, 0): '18 21 25 28 29 16', datetime.datetime(2015, 9, 2, 0, 0): '17 22 30 46 56 16', datetime.datetime(2015, 9, 5, 0, 0): '10 16 18 29 45 19', datetime.datetime(2015, 9, 9, 0, 0): '44 45 47 50 51 08', datetime.datetime(2015, 9, 12, 0, 0): '02 03 13 16 35 27', datetime.datetime(2015, 9, 16, 0, 0): '05 07 24 31 39 07', datetime.datetime(2015, 9, 19, 0, 0): '12 17 26 43 48 24', datetime.datetime(2015, 9, 23, 0, 0): '08 29 41 51 58 05', datetime.datetime(2015, 9, 26, 0, 0): '23 31 42 50 57 05', datetime.datetime(2015, 9, 30, 0, 0): '21 39 40 55 59 17', datetime.datetime(2015, 10, 3, 0, 0): '06 26 33 44 46 04', datetime.datetime(2015, 10, 7, 0, 0): '18 30 40 48 52 09', datetime.datetime(2015, 10, 10, 0, 0): '12 27 29 43 68 01', datetime.datetime(2015, 10, 14, 0, 0): '15 20 29 31 40 01', datetime.datetime(2015, 10, 17, 0, 0): '48 49 57 62 69 19', datetime.datetime(2015, 10, 21, 0, 0): '30 32 42 56 57 11', datetime.datetime(2015, 10, 24, 0, 0): '20 31 56 60 64 02', datetime.datetime(2015, 10, 28, 0, 0): '04 54 56 62 63 10', datetime.datetime(2015, 10, 31, 0, 0): '09 20 25 47 68 07', datetime.datetime(2015, 11, 4, 0, 0): '02 12 17 20 65 17', datetime.datetime(2015, 11, 7, 0, 0): '07 16 25 50 53 15', datetime.datetime(2015, 11, 11, 0, 0): '04 26 32 55 64 18', datetime.datetime(2015, 11, 14, 0, 0): '14 22 37 45 66 05', datetime.datetime(2015, 11, 18, 0, 0): '17 40 41 46 69 06', datetime.datetime(2015, 11, 21, 0, 0): '37 47 50 52 57 21', datetime.datetime(2015, 11, 25, 0, 0): '16 29 53 58 69 21', datetime.datetime(2015, 11, 28, 0, 0): '02 06 47 66 67 02', datetime.datetime(2015, 12, 2, 0, 0): '14 18 19 32 64 09', datetime.datetime(2015, 12, 5, 0, 0): '13 27 33 47 68 13', datetime.datetime(2015, 12, 9, 0, 0): '07 10 16 46 56 01', datetime.datetime(2015, 12, 12, 0, 0): '02 14 19 30 62 22', datetime.datetime(2015, 12, 16, 0, 0): '09 10 32 42 55 06', datetime.datetime(2015, 12, 19, 0, 0): '28 30 41 59 68 10', datetime.datetime(2015, 12, 23, 0, 0): '16 38 55 63 67 25', datetime.datetime(2015, 12, 26, 0, 0): '27 40 44 59 65 20', datetime.datetime(2015, 12, 30, 0, 0): '12 36 38 54 61 22', datetime.datetime(2016, 1, 2, 0, 0): '05 06 15 29 42 10', datetime.datetime(2016, 1, 6, 0, 0): '02 11 47 62 63 17', datetime.datetime(2016, 1, 9, 0, 0): '16 19 32 34 57 13', datetime.datetime(2016, 1, 13, 0, 0): '04 08 19 27 34 10', datetime.datetime(2016, 1, 16, 0, 0): '03 51 52 61 64 06', datetime.datetime(2016, 1, 20, 0, 0): '05 39 44 47 69 24', datetime.datetime(2016, 1, 23, 0, 0): '22 32 34 40 69 19', datetime.datetime(2016, 1, 27, 0, 0): '03 12 40 52 67 21', datetime.datetime(2016, 1, 30, 0, 0): '05 12 16 31 43 18', datetime.datetime(2016, 2, 3, 0, 0): '26 28 31 60 67 23', datetime.datetime(2016, 2, 6, 0, 0): '04 13 31 36 52 08', datetime.datetime(2016, 2, 10, 0, 0): '02 03 40 50 62 05', datetime.datetime(2016, 2, 13, 0, 0): '07 15 18 19 36 20', datetime.datetime(2016, 2, 17, 0, 0): '07 17 27 29 40 25', datetime.datetime(2016, 2, 20, 0, 0): '11 12 15 16 54 25', datetime.datetime(2016, 2, 24, 0, 0): '21 31 64 65 67 05', datetime.datetime(2016, 2, 27, 0, 0): '10 11 21 22 53 18', datetime.datetime(2016, 3, 2, 0, 0): '12 13 44 52 62 06', datetime.datetime(2016, 3, 5, 0, 0): '03 27 34 59 69 19', datetime.datetime(2016, 3, 9, 0, 0): '14 23 32 34 68 03', datetime.datetime(2016, 3, 12, 0, 0): '11 28 50 57 62 23', datetime.datetime(2016, 3, 16, 0, 0): '10 12 13 46 50 21', datetime.datetime(2016, 3, 19, 0, 0): '11 23 43 54 60 03', datetime.datetime(2016, 3, 23, 0, 0): '05 08 15 22 49 25', datetime.datetime(2016, 3, 26, 0, 0): '11 23 42 52 68 06', datetime.datetime(2016, 3, 30, 0, 0): '24 44 53 55 63 19', datetime.datetime(2016, 4, 2, 0, 0): '09 28 30 40 61 03', datetime.datetime(2016, 4, 6, 0, 0): '04 28 49 60 65 25', datetime.datetime(2016, 4, 9, 0, 0): '14 22 23 41 61 09', datetime.datetime(2016, 4, 13, 0, 0): '30 33 35 38 64 22', datetime.datetime(2016, 4, 16, 0, 0): '03 18 25 32 51 03', datetime.datetime(2016, 4, 20, 0, 0): '12 25 30 52 62 08', datetime.datetime(2016, 4, 23, 0, 0): '19 35 46 59 62 13', datetime.datetime(2016, 4, 27, 0, 0): '02 25 33 39 64 17', datetime.datetime(2016, 4, 30, 0, 0): '03 12 16 32 34 14', datetime.datetime(2016, 5, 4, 0, 0): '30 47 57 66 69 03', datetime.datetime(2016, 5, 7, 0, 0): '05 25 26 44 66 09', datetime.datetime(2016, 5, 11, 0, 0): '20 32 52 66 69 23', datetime.datetime(2016, 5, 14, 0, 0): '13 27 47 64 65 09', datetime.datetime(2016, 5, 18, 0, 0): '23 25 39 54 67 11', datetime.datetime(2016, 5, 21, 0, 0): '05 07 09 23 32 26', datetime.datetime(2016, 5, 25, 0, 0): '11 24 41 59 64 15', datetime.datetime(2016, 5, 28, 0, 0): '06 33 34 58 59 12', datetime.datetime(2016, 6, 1, 0, 0): '23 30 33 40 69 12', datetime.datetime(2016, 6, 4, 0, 0): '16 20 22 43 64 17', datetime.datetime(2016, 6, 8, 0, 0): '12 25 37 60 69 20', datetime.datetime(2016, 6, 11, 0, 0): '20 27 36 41 58 07', datetime.datetime(2016, 6, 15, 0, 0): '04 22 24 31 33 10', datetime.datetime(2016, 6, 18, 0, 0): '02 23 41 53 63 11', datetime.datetime(2016, 6, 22, 0, 0): '14 40 42 43 52 17', datetime.datetime(2016, 6, 25, 0, 0): '03 27 36 56 69 25', datetime.datetime(2016, 6, 29, 0, 0): '23 29 37 60 64 06', datetime.datetime(2016, 7, 2, 0, 0): '10 34 39 59 63 04', datetime.datetime(2016, 7, 6, 0, 0): '02 24 31 57 66 18', datetime.datetime(2016, 7, 9, 0, 0): '10 28 32 61 64 12', datetime.datetime(2016, 7, 13, 0, 0): '03 15 29 54 57 10', datetime.datetime(2016, 7, 16, 0, 0): '11 17 40 50 62 26', datetime.datetime(2016, 7, 20, 0, 0): '06 25 35 58 66 05', datetime.datetime(2016, 7, 23, 0, 0): '05 07 23 35 39 11', datetime.datetime(2016, 7, 27, 0, 0): '10 47 50 65 68 24', datetime.datetime(2016, 7, 30, 0, 0): '11 17 21 23 32 05', datetime.datetime(2016, 8, 3, 0, 0): '09 11 27 66 67 02', datetime.datetime(2016, 8, 6, 0, 0): '20 33 36 47 52 12', datetime.datetime(2016, 8, 10, 0, 0): '23 56 61 64 67 12', datetime.datetime(2016, 8, 13, 0, 0): '38 44 60 64 69 06', datetime.datetime(2016, 8, 17, 0, 0): '33 44 49 50 52 08', datetime.datetime(2016, 8, 20, 0, 0): '03 06 21 60 68 24', datetime.datetime(2016, 8, 24, 0, 0): '09 11 25 64 65 16', datetime.datetime(2016, 8, 27, 0, 0): '04 32 48 49 63 20', datetime.datetime(2016, 8, 31, 0, 0): '05 10 24 56 61 12', datetime.datetime(2016, 9, 3, 0, 0): '07 39 50 59 67 25', datetime.datetime(2016, 9, 7, 0, 0): '22 23 29 33 55 21', datetime.datetime(2016, 9, 10, 0, 0): '03 17 49 55 68 08', datetime.datetime(2016, 9, 14, 0, 0): '10 11 23 28 31 14', datetime.datetime(2016, 9, 17, 0, 0): '09 19 51 55 62 14', datetime.datetime(2016, 9, 21, 0, 0): '01 28 63 67 69 17', datetime.datetime(2016, 9, 24, 0, 0): '07 15 20 29 41 22', datetime.datetime(2016, 9, 28, 0, 0): '30 38 52 53 62 01', datetime.datetime(2016, 10, 1, 0, 0): '02 12 50 61 64 01', datetime.datetime(2016, 10, 5, 0, 0): '08 18 27 29 60 15', datetime.datetime(2016, 10, 8, 0, 0): '03 54 61 64 68 09', datetime.datetime(2016, 10, 12, 0, 0): '16 30 34 37 44 16', datetime.datetime(2016, 10, 15, 0, 0): '23 49 57 64 67 20', datetime.datetime(2016, 10, 19, 0, 0): '10 16 38 43 63 23', datetime.datetime(2016, 10, 22, 0, 0): '01 28 33 55 56 22', datetime.datetime(2016, 10, 26, 0, 0): '02 03 16 48 56 24', datetime.datetime(2016, 10, 29, 0, 0): '19 20 21 42 48 23', datetime.datetime(2016, 11, 2, 0, 0): '13 18 37 54 61 05', datetime.datetime(2016, 11, 5, 0, 0): '21 31 50 51 69 08', datetime.datetime(2016, 11, 9, 0, 0): '01 25 28 31 54 02', datetime.datetime(2016, 11, 12, 0, 0): '08 17 20 27 52 24', datetime.datetime(2016, 11, 16, 0, 0): '28 41 61 63 65 07', datetime.datetime(2016, 11, 19, 0, 0): '16 24 28 43 61 21', datetime.datetime(2016, 11, 23, 0, 0): '07 32 41 47 61 03', datetime.datetime(2016, 11, 26, 0, 0): '17 19 21 37 44 16', datetime.datetime(2016, 11, 30, 0, 0): '03 14 18 25 45 07', datetime.datetime(2016, 12, 3, 0, 0): '08 10 26 27 33 22', datetime.datetime(2016, 12, 7, 0, 0): '41 48 49 53 64 20', datetime.datetime(2016, 12, 10, 0, 0): '12 21 32 44 66 15', datetime.datetime(2016, 12, 14, 0, 0): '18 26 37 39 66 15', datetime.datetime(2016, 12, 17, 0, 0): '01 08 16 40 48 10', datetime.datetime(2016, 12, 21, 0, 0): '25 33 40 54 68 03', datetime.datetime(2016, 12, 24, 0, 0): '28 38 42 51 52 21', datetime.datetime(2016, 12, 28, 0, 0): '16 23 30 44 58 04', datetime.datetime(2016, 12, 31, 0, 0): '01 03 28 57 67 09', datetime.datetime(2017, 1, 4, 0, 0): '16 17 29 41 42 04', datetime.datetime(2017, 1, 7, 0, 0): '03 12 24 37 63 10', datetime.datetime(2017, 1, 11, 0, 0): '01 03 13 16 43 24', datetime.datetime(2017, 1, 14, 0, 0): '23 55 59 64 69 13', datetime.datetime(2017, 1, 18, 0, 0): '09 40 41 53 58 12', datetime.datetime(2017, 1, 21, 0, 0): '23 25 45 52 67 02', datetime.datetime(2017, 1, 25, 0, 0): '18 28 62 66 68 22', datetime.datetime(2017, 1, 28, 0, 0): '12 20 39 49 69 17', datetime.datetime(2017, 2, 1, 0, 0): '09 43 57 60 64 10', datetime.datetime(2017, 2, 4, 0, 0): '06 13 16 17 52 25', datetime.datetime(2017, 2, 8, 0, 0): '14 20 42 49 66 05', datetime.datetime(2017, 2, 11, 0, 0): '05 09 17 37 64 02', datetime.datetime(2017, 2, 15, 0, 0): '05 28 33 38 42 19', datetime.datetime(2017, 2, 18, 0, 0): '03 07 09 31 33 20', datetime.datetime(2017, 2, 22, 0, 0): '10 13 28 52 61 02', datetime.datetime(2017, 2, 25, 0, 0): '06 32 47 62 65 19', datetime.datetime(2017, 3, 1, 0, 0): '10 16 40 52 55 17', datetime.datetime(2017, 3, 4, 0, 0): '02 18 19 22 63 19', datetime.datetime(2017, 3, 8, 0, 0): '23 33 42 46 59 04', datetime.datetime(2017, 3, 11, 0, 0): '01 26 41 50 57 11', datetime.datetime(2017, 3, 15, 0, 0): '16 30 41 48 53 16', datetime.datetime(2017, 3, 18, 0, 0): '13 25 44 54 67 05', datetime.datetime(2017, 3, 22, 0, 0): '02 09 27 29 42 09', datetime.datetime(2017, 3, 25, 0, 0): '18 31 32 45 48 16', datetime.datetime(2017, 3, 29, 0, 0): '08 15 31 36 62 11', datetime.datetime(2017, 4, 1, 0, 0): '09 32 36 44 65 01', datetime.datetime(2017, 4, 5, 0, 0): '08 20 46 53 54 13', datetime.datetime(2017, 4, 8, 0, 0): '23 36 51 53 60 15', datetime.datetime(2017, 4, 12, 0, 0): '08 14 61 63 68 24', datetime.datetime(2017, 4, 15, 0, 0): '05 22 26 45 61 13', datetime.datetime(2017, 4, 19, 0, 0): '01 19 37 40 52 15', datetime.datetime(2017, 4, 22, 0, 0): '21 39 41 48 63 06', datetime.datetime(2017, 4, 26, 0, 0): '01 15 18 26 51 26', datetime.datetime(2017, 4, 29, 0, 0): '22 23 24 45 62 05', datetime.datetime(2017, 5, 3, 0, 0): '17 18 49 59 66 09', datetime.datetime(2017, 5, 6, 0, 0): '11 21 31 41 59 21', datetime.datetime(2017, 5, 10, 0, 0): '29 31 46 56 62 08', datetime.datetime(2017, 5, 13, 0, 0): '17 20 32 63 68 19', datetime.datetime(2017, 5, 17, 0, 0): '04 11 39 45 48 09', datetime.datetime(2017, 5, 20, 0, 0): '05 22 45 47 54 03', datetime.datetime(2017, 5, 24, 0, 0): '28 32 33 38 62 15', datetime.datetime(2017, 5, 27, 0, 0): '05 10 28 55 67 09', datetime.datetime(2017, 5, 31, 0, 0): '04 33 39 46 60 06', datetime.datetime(2017, 6, 3, 0, 0): '03 09 21 41 54 25', datetime.datetime(2017, 6, 7, 0, 0): '05 21 57 66 69 13', datetime.datetime(2017, 6, 14, 0, 0): '05 22 43 57 63 24', datetime.datetime(2017, 6, 17, 0, 0): '10 13 32 53 62 21', datetime.datetime(2017, 6, 21, 0, 0): '14 46 61 65 68 13', datetime.datetime(2017, 6, 24, 0, 0): '10 22 32 36 58 10', datetime.datetime(2017, 6, 28, 0, 0): '29 37 46 53 68 08', datetime.datetime(2017, 7, 1, 0, 0): '19 42 45 48 53 16', datetime.datetime(2017, 7, 5, 0, 0): '04 09 16 54 68 21', datetime.datetime(2017, 7, 8, 0, 0): '08 10 29 40 59 26', datetime.datetime(2017, 7, 12, 0, 0): '01 02 18 23 61 09', datetime.datetime(2017, 7, 15, 0, 0): '09 40 63 64 66 17', datetime.datetime(2017, 7, 19, 0, 0): '50 51 59 61 63 04', datetime.datetime(2017, 7, 22, 0, 0): '05 32 44 53 60 09', datetime.datetime(2017, 7, 26, 0, 0): '07 19 21 42 69 12', datetime.datetime(2017, 7, 29, 0, 0): '01 28 40 45 48 12', datetime.datetime(2017, 8, 2, 0, 0): '01 16 54 63 69 18', datetime.datetime(2017, 8, 5, 0, 0): '11 21 28 33 45 11', datetime.datetime(2017, 8, 9, 0, 0): '12 30 36 47 62 09', datetime.datetime(2017, 8, 12, 0, 0): '20 24 26 35 49 19', datetime.datetime(2017, 8, 16, 0, 0): '09 15 43 60 64 04', datetime.datetime(2017, 8, 19, 0, 0): '17 19 39 43 68 13', datetime.datetime(2017, 8, 23, 0, 0): '06 07 16 23 26 04', datetime.datetime(2017, 8, 26, 0, 0): '07 15 32 38 66 15', datetime.datetime(2017, 8, 30, 0, 0): '19 28 43 67 69 07', datetime.datetime(2017, 9, 2, 0, 0): '06 21 41 52 62 26', datetime.datetime(2017, 9, 6, 0, 0): '08 14 32 58 67 17', datetime.datetime(2017, 9, 9, 0, 0): '06 20 29 57 59 22', datetime.datetime(2017, 9, 13, 0, 0): '17 24 35 57 63 19', datetime.datetime(2017, 9, 16, 0, 0): '17 18 24 25 31 24', datetime.datetime(2017, 9, 20, 0, 0): '39 48 53 67 68 26', datetime.datetime(2017, 9, 23, 0, 0): '24 45 55 56 57 19', datetime.datetime(2017, 9, 27, 0, 0): '08 10 21 23 25 22', datetime.datetime(2017, 9, 30, 0, 0): '08 12 25 41 64 15', datetime.datetime(2017, 10, 4, 0, 0): '22 23 62 63 66 24', datetime.datetime(2017, 10, 7, 0, 0): '10 49 61 63 65 07', datetime.datetime(2017, 10, 11, 0, 0): '01 03 13 19 69 23', datetime.datetime(2017, 10, 14, 0, 0): '32 37 56 66 69 11', datetime.datetime(2017, 10, 18, 0, 0): '30 49 54 66 69 08', datetime.datetime(2017, 10, 21, 0, 0): '14 41 42 45 69 04', datetime.datetime(2017, 10, 25, 0, 0): '18 22 29 54 57 08', datetime.datetime(2017, 10, 28, 0, 0): '27 35 38 57 66 10', datetime.datetime(2017, 11, 1, 0, 0): '03 06 19 26 44 01', datetime.datetime(2017, 11, 4, 0, 0): '12 14 26 48 51 13', datetime.datetime(2017, 11, 8, 0, 0): '12 14 20 21 34 22', datetime.datetime(2017, 11, 11, 0, 0): '04 06 16 30 56 18', datetime.datetime(2017, 11, 15, 0, 0): '23 32 44 48 50 25', datetime.datetime(2017, 11, 18, 0, 0): '17 28 31 32 39 26', datetime.datetime(2017, 11, 22, 0, 0): '35 37 46 51 61 13', datetime.datetime(2017, 11, 25, 0, 0): '08 13 27 53 54 04', datetime.datetime(2017, 11, 29, 0, 0): '24 26 28 59 63 16', datetime.datetime(2017, 12, 2, 0, 0): '28 30 32 36 58 06', datetime.datetime(2017, 12, 6, 0, 0): '19 20 50 55 62 09', datetime.datetime(2017, 12, 9, 0, 0): '25 36 37 55 60 06', datetime.datetime(2017, 12, 13, 0, 0): '02 24 28 51 58 07', datetime.datetime(2017, 12, 16, 0, 0): '09 35 37 50 63 11', datetime.datetime(2017, 12, 20, 0, 0): '01 20 61 64 69 20', datetime.datetime(2017, 12, 23, 0, 0): '01 03 13 15 44 25', datetime.datetime(2017, 12, 27, 0, 0): '03 09 16 56 60 03', datetime.datetime(2017, 12, 30, 0, 0): '28 36 41 51 58 24', datetime.datetime(2018, 1, 3, 0, 0): '02 18 37 39 42 12', datetime.datetime(2018, 1, 6, 0, 0): '12 61 30 29 33 26', datetime.datetime(2018, 1, 10, 0, 0): '07 24 33 49 50 04', datetime.datetime(2018, 1, 13, 0, 0): '14 25 35 58 69 24', datetime.datetime(2018, 1, 17, 0, 0): '03 33 37 51 57 21', datetime.datetime(2018, 1, 20, 0, 0): '26 28 47 49 58 03', datetime.datetime(2018, 1, 24, 0, 0): '05 09 11 33 64 21', datetime.datetime(2018, 1, 27, 0, 0): '17 21 26 47 54 07', datetime.datetime(2018, 1, 31, 0, 0): '04 07 14 46 59 22', datetime.datetime(2018, 2, 3, 0, 0): '15 23 27 48 53 06', datetime.datetime(2018, 2, 7, 0, 0): '23 34 35 40 47 10', datetime.datetime(2018, 2, 10, 0, 0): '01 13 27 41 59 20', datetime.datetime(2018, 2, 14, 0, 0): '37 39 44 46 69 26', datetime.datetime(2018, 2, 17, 0, 0): '13 26 39 44 62 02', datetime.datetime(2018, 2, 21, 0, 0): '07 15 31 34 36 08', datetime.datetime(2018, 2, 24, 0, 0): '24 25 38 62 63 06', datetime.datetime(2018, 2, 28, 0, 0): '12 30 59 65 69 16', datetime.datetime(2018, 3, 3, 0, 0): '13 17 25 36 40 05', datetime.datetime(2018, 3, 7, 0, 0): '06 13 19 36 51 18', datetime.datetime(2018, 3, 10, 0, 0): '43 44 54 61 69 22', datetime.datetime(2018, 3, 14, 0, 0): '06 12 24 41 68 09', datetime.datetime(2018, 3, 17, 0, 0): '22 57 59 60 66 07', datetime.datetime(2018, 3, 21, 0, 0): '03 04 18 29 61 25', datetime.datetime(2018, 3, 24, 0, 0): '10 33 45 53 56 24', datetime.datetime(2018, 3, 28, 0, 0): '06 08 26 52 53 21', datetime.datetime(2018, 3, 31, 0, 0): '08 24 52 55 61 21', datetime.datetime(2018, 4, 4, 0, 0): '08 24 42 54 64 24', datetime.datetime(2018, 4, 7, 0, 0): '02 17 20 38 39 20', datetime.datetime(2018, 4, 11, 0, 0): '16 18 27 55 67 18', datetime.datetime(2018, 4, 14, 0, 0): '17 19 26 61 62 15', datetime.datetime(2018, 4, 18, 0, 0): '09 10 12 17 23 09', datetime.datetime(2018, 4, 21, 0, 0): '40 50 54 62 69 19', datetime.datetime(2018, 4, 25, 0, 0): '17 18 39 56 64 12', datetime.datetime(2018, 4, 28, 0, 0): '20 22 28 45 50 08', datetime.datetime(2018, 5, 2, 0, 0): '05 14 31 40 50 06', datetime.datetime(2018, 5, 5, 0, 0): '14 29 36 57 61 17', datetime.datetime(2018, 5, 9, 0, 0): '11 16 38 50 69 19', datetime.datetime(2018, 5, 12, 0, 0): '22 42 45 55 56 14', datetime.datetime(2018, 5, 16, 0, 0): '17 19 21 22 51 19', datetime.datetime(2018, 5, 19, 0, 0): '03 06 09 17 56 25', datetime.datetime(2018, 5, 23, 0, 0): '20 54 56 61 64 07', datetime.datetime(2018, 5, 26, 0, 0): '01 21 31 45 49 21', datetime.datetime(2018, 5, 30, 0, 0): '17 23 26 46 68 20', datetime.datetime(2018, 6, 2, 0, 0): '23 25 37 44 64 07', datetime.datetime(2018, 6, 6, 0, 0): '23 28 41 53 56 14', datetime.datetime(2018, 6, 9, 0, 0): '06 10 15 25 36 14', datetime.datetime(2018, 6, 13, 0, 0): '13 20 38 45 55 01', datetime.datetime(2018, 6, 16, 0, 0): '09 45 57 58 65 09', datetime.datetime(2018, 6, 20, 0, 0): '04 14 23 27 56 13', datetime.datetime(2018, 6, 23, 0, 0): '16 29 43 45 56 25', datetime.datetime(2018, 6, 27, 0, 0): '07 28 37 62 63 15', datetime.datetime(2018, 6, 30, 0, 0): '03 09 20 42 61 24', datetime.datetime(2018, 7, 4, 0, 0): '04 07 15 41 44 10', datetime.datetime(2018, 7, 7, 0, 0): '01 10 43 45 64 22', datetime.datetime(2018, 7, 11, 0, 0): '19 21 27 46 47 07', datetime.datetime(2018, 7, 14, 0, 0): '22 41 42 49 67 11', datetime.datetime(2018, 7, 18, 0, 0): '01 10 27 28 36 12', datetime.datetime(2018, 7, 21, 0, 0): '09 23 56 58 68 01', datetime.datetime(2018, 7, 25, 0, 0): '02 18 41 44 64 26', datetime.datetime(2018, 7, 28, 0, 0): '22 27 46 56 65 13', datetime.datetime(2018, 8, 1, 0, 0): '05 22 32 38 58 26', datetime.datetime(2018, 8, 4, 0, 0): '03 11 38 44 58 02', datetime.datetime(2018, 8, 8, 0, 0): '10 21 30 43 63 17', datetime.datetime(2018, 8, 11, 0, 0): '05 43 56 62 68 24', datetime.datetime(2018, 8, 15, 0, 0): '12 15 28 47 48 16', datetime.datetime(2018, 8, 18, 0, 0): '24 34 52 61 67 16', datetime.datetime(2018, 8, 22, 0, 0): '01 07 45 47 69 13', datetime.datetime(2018, 8, 25, 0, 0): '20 25 54 57 63 08', datetime.datetime(2018, 8, 29, 0, 0): '25 41 53 57 67 12', datetime.datetime(2018, 9, 1, 0, 0): '11 54 55 61 66 09', datetime.datetime(2018, 9, 5, 0, 0): '06 15 50 59 60 13', datetime.datetime(2018, 9, 8, 0, 0): '03 13 20 32 33 21', datetime.datetime(2018, 9, 12, 0, 0): '06 28 48 63 64 24', datetime.datetime(2018, 9, 15, 0, 0): '02 18 19 24 34 03', datetime.datetime(2018, 9, 19, 0, 0): '04 39 48 50 51 11', datetime.datetime(2018, 9, 22, 0, 0): '24 61 63 64 69 18', datetime.datetime(2018, 9, 26, 0, 0): '01 02 07 30 50 08', datetime.datetime(2018, 9, 29, 0, 0): '09 17 34 59 64 22', datetime.datetime(2018, 10, 3, 0, 0): '41 53 59 63 66 03', datetime.datetime(2018, 10, 6, 0, 0): '01 22 27 53 67 15', datetime.datetime(2018, 10, 10, 0, 0): '08 23 27 42 60 07', datetime.datetime(2018, 10, 13, 0, 0): '11 14 32 43 65 15', datetime.datetime(2018, 10, 17, 0, 0): '03 57 64 68 69 15', datetime.datetime(2018, 10, 20, 0, 0): '16 54 57 62 69 23', datetime.datetime(2018, 10, 24, 0, 0): '03 21 45 53 56 22', datetime.datetime(2018, 10, 27, 0, 0): '08 12 13 19 27 04', datetime.datetime(2018, 10, 31, 0, 0): '07 25 39 40 47 20', datetime.datetime(2018, 11, 3, 0, 0): '15 21 24 32 65 11', datetime.datetime(2018, 11, 7, 0, 0): '26 28 34 42 50 25', datetime.datetime(2018, 11, 10, 0, 0): '05 29 34 53 57 24', datetime.datetime(2018, 11, 14, 0, 0): '07 42 49 62 69 23', datetime.datetime(2018, 11, 17, 0, 0): '06 08 20 52 68 05', datetime.datetime(2018, 11, 21, 0, 0): '07 14 23 38 55 18', datetime.datetime(2018, 11, 24, 0, 0): '11 33 51 56 58 18', datetime.datetime(2018, 11, 28, 0, 0): '04 19 59 68 69 21', datetime.datetime(2018, 12, 1, 0, 0): '10 11 47 55 58 26', datetime.datetime(2018, 12, 5, 0, 0): '09 11 36 37 38 11', datetime.datetime(2018, 12, 8, 0, 0): '14 32 34 46 61 10', datetime.datetime(2018, 12, 12, 0, 0): '04 09 21 29 64 26', datetime.datetime(2018, 12, 15, 0, 0): '08 38 43 52 55 17', datetime.datetime(2018, 12, 19, 0, 0): '15 29 31 37 43 16', datetime.datetime(2018, 12, 22, 0, 0): '21 28 30 40 59 26', datetime.datetime(2018, 12, 26, 0, 0): '05 25 38 52 67 24', datetime.datetime(2018, 12, 29, 0, 0): '12 42 51 53 62 25', datetime.datetime(2019, 1, 2, 0, 0): '08 12 42 46 56 12', datetime.datetime(2019, 1, 5, 0, 0): '03 07 15 27 69 19', datetime.datetime(2019, 1, 9, 0, 0): '06 19 37 49 59 22', datetime.datetime(2019, 1, 12, 0, 0): '07 36 48 57 58 24', datetime.datetime(2019, 1, 16, 0, 0): '14 29 31 56 61 01', datetime.datetime(2019, 1, 19, 0, 0): '05 08 41 65 66 20', datetime.datetime(2019, 1, 23, 0, 0): '23 25 47 48 50 24', datetime.datetime(2019, 1, 26, 0, 0): '08 12 20 21 32 10', datetime.datetime(2019, 1, 30, 0, 0): '02 12 16 29 54 06', datetime.datetime(2019, 2, 2, 0, 0): '10 17 18 43 65 13', datetime.datetime(2019, 2, 6, 0, 0): '05 13 28 38 63 21', datetime.datetime(2019, 2, 9, 0, 0): '01 02 03 07 39 25', datetime.datetime(2019, 2, 13, 0, 0): '02 08 14 24 69 26', datetime.datetime(2019, 2, 16, 0, 0): '29 30 41 48 64 01', datetime.datetime(2019, 2, 20, 0, 0): '27 49 50 51 52 02', datetime.datetime(2019, 2, 23, 0, 0): '04 06 14 20 32 13', datetime.datetime(2019, 2, 27, 0, 0): '21 31 42 49 59 23', datetime.datetime(2019, 3, 2, 0, 0): '01 19 25 27 68 21', datetime.datetime(2019, 3, 6, 0, 0): '06 10 21 35 46 23', datetime.datetime(2019, 3, 9, 0, 0): '05 06 45 55 59 14', datetime.datetime(2019, 3, 13, 0, 0): '18 36 45 47 69 14', datetime.datetime(2019, 3, 16, 0, 0): '30 34 39 53 67 11', datetime.datetime(2019, 3, 20, 0, 0): '10 14 50 53 63 21', datetime.datetime(2019, 3, 23, 0, 0): '24 25 52 60 66 05', datetime.datetime(2019, 3, 27, 0, 0): '16 20 37 44 62 12', datetime.datetime(2019, 3, 30, 0, 0): '21 52 54 64 68 04', datetime.datetime(2019, 4, 3, 0, 0): '16 19 25 32 49 18', datetime.datetime(2019, 4, 6, 0, 0): '15 33 43 59 60 08', datetime.datetime(2019, 4, 10, 0, 0): '12 21 23 39 67 06', datetime.datetime(2019, 4, 13, 0, 0): '04 17 26 32 49 10', datetime.datetime(2019, 4, 17, 0, 0): '01 15 17 46 66 15', datetime.datetime(2019, 4, 20, 0, 0): '03 27 30 63 65 01', datetime.datetime(2019, 4, 24, 0, 0): '06 32 35 36 65 04', datetime.datetime(2019, 4, 27, 0, 0): '02 29 41 45 62 06', datetime.datetime(2019, 5, 1, 0, 0): '05 23 28 56 66 17', datetime.datetime(2019, 5, 4, 0, 0): '06 16 23 30 61 02', datetime.datetime(2019, 5, 8, 0, 0): '01 45 53 64 66 03', datetime.datetime(2019, 5, 11, 0, 0): '06 08 09 37 40 26', datetime.datetime(2019, 5, 15, 0, 0): '07 17 33 61 68 04', datetime.datetime(2019, 5, 18, 0, 0): '02 10 25 66 67 26', datetime.datetime(2019, 5, 22, 0, 0): '07 10 20 44 57 03', datetime.datetime(2019, 5, 25, 0, 0): '01 02 39 43 66 02', datetime.datetime(2019, 5, 29, 0, 0): '03 32 34 42 61 07', datetime.datetime(2019, 6, 1, 0, 0): '06 15 34 45 52 08', datetime.datetime(2019, 6, 5, 0, 0): '17 23 28 34 38 08', datetime.datetime(2019, 6, 8, 0, 0): '09 13 42 48 60 18', datetime.datetime(2019, 6, 12, 0, 0): '05 35 38 42 57 13', datetime.datetime(2019, 6, 15, 0, 0): '08 11 14 16 49 14', datetime.datetime(2019, 6, 19, 0, 0): '04 18 21 26 38 01', datetime.datetime(2019, 6, 22, 0, 0): '03 06 11 14 66 21', datetime.datetime(2019, 6, 26, 0, 0): '01 05 16 22 54 24', datetime.datetime(2019, 6, 29, 0, 0): '13 17 24 59 62 08', datetime.datetime(2019, 7, 3, 0, 0): '40 43 45 50 61 25', datetime.datetime(2019, 7, 6, 0, 0): '04 08 23 46 65 01', datetime.datetime(2019, 7, 10, 0, 0): '07 09 26 44 68 03', datetime.datetime(2019, 7, 13, 0, 0): '13 23 32 35 68 21', datetime.datetime(2019, 7, 17, 0, 0): '19 43 47 60 68 10', datetime.datetime(2019, 7, 20, 0, 0): '05 26 36 64 69 19', datetime.datetime(2019, 7, 24, 0, 0): '22 29 35 53 56 13', datetime.datetime(2019, 7, 27, 0, 0): '01 19 31 48 61 06', datetime.datetime(2019, 7, 31, 0, 0): '14 37 47 55 67 06', datetime.datetime(2019, 8, 3, 0, 0): '03 06 45 66 68 13', datetime.datetime(2019, 8, 7, 0, 0): '08 32 47 53 59 03', datetime.datetime(2019, 8, 10, 0, 0): '35 41 44 58 59 03', datetime.datetime(2019, 8, 14, 0, 0): '10 13 30 51 69 10', datetime.datetime(2019, 8, 17, 0, 0): '18 21 24 30 60 20', datetime.datetime(2019, 8, 21, 0, 0): '12 21 22 29 32 21', datetime.datetime(2019, 8, 24, 0, 0): '05 12 20 21 47 01', datetime.datetime(2019, 8, 28, 0, 0): '09 32 37 41 56 14', datetime.datetime(2019, 8, 31, 0, 0): '14 41 50 56 57 18', datetime.datetime(2019, 9, 4, 0, 0): '04 08 30 52 59 02', datetime.datetime(2019, 9, 7, 0, 0): '11 20 41 42 56 06', datetime.datetime(2019, 9, 11, 0, 0): '06 17 24 53 57 03', datetime.datetime(2019, 9, 14, 0, 0): '11 27 31 36 67 11', datetime.datetime(2019, 9, 18, 0, 0): '14 19 39 47 51 15', datetime.datetime(2019, 9, 21, 0, 0): '01 09 22 36 68 22', datetime.datetime(2019, 9, 25, 0, 0): '37 43 44 45 53 25', datetime.datetime(2019, 9, 28, 0, 0): '15 23 34 51 55 04', datetime.datetime(2019, 10, 2, 0, 0): '04 08 10 43 53 07', datetime.datetime(2019, 10, 5, 0, 0): '06 14 36 51 54 04', datetime.datetime(2019, 10, 9, 0, 0): '05 18 33 43 65 02', datetime.datetime(2019, 10, 12, 0, 0): '12 29 34 53 65 23', datetime.datetime(2019, 10, 16, 0, 0): '01 05 25 63 67 03', datetime.datetime(2019, 10, 19, 0, 0): '14 27 29 59 65 12', datetime.datetime(2019, 10, 23, 0, 0): '05 12 50 61 69 23', datetime.datetime(2019, 10, 26, 0, 0): '03 20 48 54 59 04', datetime.datetime(2019, 10, 30, 0, 0): '19 22 52 56 67 21', datetime.datetime(2019, 11, 2, 0, 0): '03 23 32 37 58 22', datetime.datetime(2019, 11, 6, 0, 0): '15 28 46 62 64 17', datetime.datetime(2019, 11, 9, 0, 0): '14 17 35 38 60 25', datetime.datetime(2019, 11, 13, 0, 0): '23 26 27 28 66 11', datetime.datetime(2019, 11, 16, 0, 0): '14 22 26 55 63 26', datetime.datetime(2019, 11, 20, 0, 0): '07 15 39 40 57 12', datetime.datetime(2019, 11, 23, 0, 0): '28 35 38 61 66 23', datetime.datetime(2019, 11, 27, 0, 0): '15 26 37 53 55 21', datetime.datetime(2019, 11, 30, 0, 0): '15 35 42 63 68 18', datetime.datetime(2019, 12, 4, 0, 0): '08 27 44 51 61 14', datetime.datetime(2019, 12, 7, 0, 0): '18 42 53 62 66 25', datetime.datetime(2019, 12, 11, 0, 0): '24 29 42 44 63 10', datetime.datetime(2019, 12, 14, 0, 0): '03 06 12 32 64 19', datetime.datetime(2019, 12, 18, 0, 0): '14 18 26 39 68 09', datetime.datetime(2019, 12, 21, 0, 0): '19 31 35 50 67 14', datetime.datetime(2019, 12, 25, 0, 0): '02 04 16 30 46 20', datetime.datetime(2019, 12, 28, 0, 0): '20 23 39 59 60 18', datetime.datetime(2020, 1, 1, 0, 0): '49 53 57 59 62 26', datetime.datetime(2020, 1, 4, 0, 0): '01 11 21 25 54 07', datetime.datetime(2020, 1, 8, 0, 0): '02 04 07 43 56 22', datetime.datetime(2020, 1, 11, 0, 0): '03 21 23 31 59 03', datetime.datetime(2020, 1, 15, 0, 0): '39 41 53 55 68 19', datetime.datetime(2020, 1, 18, 0, 0): '20 24 38 56 68 18', datetime.datetime(2020, 1, 22, 0, 0): '11 33 44 59 67 08', datetime.datetime(2020, 1, 25, 0, 0): '02 09 17 36 67 18', datetime.datetime(2020, 1, 29, 0, 0): '09 12 15 31 60 02', datetime.datetime(2020, 2, 1, 0, 0): '12 33 54 57 60 13', datetime.datetime(2020, 2, 5, 0, 0): '23 30 35 41 57 02', datetime.datetime(2020, 2, 8, 0, 0): '35 49 50 59 66 06', datetime.datetime(2020, 2, 12, 0, 0): '14 47 54 55 68 25', datetime.datetime(2020, 2, 15, 0, 0): '16 32 35 36 46 03', datetime.datetime(2020, 2, 19, 0, 0): '10 12 15 19 56 19', datetime.datetime(2020, 2, 22, 0, 0): '25 37 39 61 62 11', datetime.datetime(2020, 2, 26, 0, 0): '08 27 29 36 47 24', datetime.datetime(2020, 2, 29, 0, 0): '24 44 46 50 51 13', datetime.datetime(2020, 3, 4, 0, 0): '18 43 58 60 68 14', datetime.datetime(2020, 3, 7, 0, 0): '07 15 21 33 62 23', datetime.datetime(2020, 3, 11, 0, 0): '04 29 49 50 67 02', datetime.datetime(2020, 3, 14, 0, 0): '09 23 26 30 32 08', datetime.datetime(2020, 3, 18, 0, 0): '15 27 44 59 63 08', datetime.datetime(2020, 3, 21, 0, 0): '02 23 40 59 69 13', datetime.datetime(2020, 3, 25, 0, 0): '05 09 27 39 42 16', datetime.datetime(2020, 3, 28, 0, 0): '07 40 48 55 66 11', datetime.datetime(2020, 4, 1, 0, 0): '33 35 45 48 60 16', datetime.datetime(2020, 4, 4, 0, 0): '08 31 39 40 43 04', datetime.datetime(2020, 4, 8, 0, 0): '02 37 39 48 54 05', datetime.datetime(2020, 4, 11, 0, 0): '22 29 30 42 47 17', datetime.datetime(2020, 4, 15, 0, 0): '10 12 33 36 41 02', datetime.datetime(2020, 4, 18, 0, 0): '04 44 46 56 63 19', datetime.datetime(2020, 4, 22, 0, 0): '01 33 35 40 69 24', datetime.datetime(2020, 4, 25, 0, 0): '01 03 21 47 57 18', datetime.datetime(2020, 4, 29, 0, 0): '02 20 49 61 67 20', datetime.datetime(2020, 5, 2, 0, 0): '13 16 33 58 68 24', datetime.datetime(2020, 5, 6, 0, 0): '07 08 35 50 65 20', datetime.datetime(2020, 5, 9, 0, 0): '12 18 42 48 65 19', datetime.datetime(2020, 5, 13, 0, 0): '39 53 54 56 57 20', datetime.datetime(2020, 5, 16, 0, 0): '08 12 26 39 42 11', datetime.datetime(2020, 5, 20, 0, 0): '18 34 40 42 50 09', datetime.datetime(2020, 5, 23, 0, 0): '02 08 18 21 23 16', datetime.datetime(2020, 5, 27, 0, 0): '38 58 59 64 68 21', datetime.datetime(2020, 5, 30, 0, 0): '13 32 41 58 60 14', datetime.datetime(2020, 6, 3, 0, 0): '01 03 26 41 64 17', datetime.datetime(2020, 6, 6, 0, 0): '01 17 38 68 69 18', datetime.datetime(2020, 6, 10, 0, 0): '10 33 41 52 54 18', datetime.datetime(2020, 6, 13, 0, 0): '02 12 32 50 65 05', datetime.datetime(2020, 6, 17, 0, 0): '07 10 63 64 68 10', datetime.datetime(2020, 6, 20, 0, 0): '10 31 41 63 67 05', datetime.datetime(2020, 6, 24, 0, 0): '15 22 27 33 46 23', datetime.datetime(2020, 6, 27, 0, 0): '09 36 49 56 62 08', datetime.datetime(2020, 7, 1, 0, 0): '15 28 52 53 63 18', datetime.datetime(2020, 7, 4, 0, 0): '16 21 27 60 61 06', datetime.datetime(2020, 7, 8, 0, 0): '03 10 34 36 62 05', datetime.datetime(2020, 7, 11, 0, 0): '14 19 61 62 64 04', datetime.datetime(2020, 7, 15, 0, 0): '27 47 61 62 69 04', datetime.datetime(2020, 7, 18, 0, 0): '13 16 32 58 59 09', datetime.datetime(2020, 7, 22, 0, 0): '16 25 36 44 55 14', datetime.datetime(2020, 7, 25, 0, 0): '05 21 36 61 62 18', datetime.datetime(2020, 7, 29, 0, 0): '07 29 35 40 45 26', datetime.datetime(2020, 8, 1, 0, 0): '06 25 36 43 48 24', datetime.datetime(2020, 8, 5, 0, 0): '07 14 17 57 65 24', datetime.datetime(2020, 8, 8, 0, 0): '02 03 14 40 51 24', datetime.datetime(2020, 8, 12, 0, 0): '02 06 18 36 37 21', datetime.datetime(2020, 8, 15, 0, 0): '05 12 34 45 56 03', datetime.datetime(2020, 8, 19, 0, 0): '13 23 47 55 58 23', datetime.datetime(2020, 8, 22, 0, 0): '19 30 36 42 66 14', datetime.datetime(2020, 8, 26, 0, 0): '08 12 19 47 58 02', datetime.datetime(2020, 8, 29, 0, 0): '05 21 22 29 43 10', datetime.datetime(2020, 9, 2, 0, 0): '01 04 11 20 69 18', datetime.datetime(2020, 9, 5, 0, 0): '15 21 22 27 47 07', datetime.datetime(2020, 9, 9, 0, 0): '27 52 55 60 64 21', datetime.datetime(2020, 9, 12, 0, 0): '16 17 20 53 67 04', datetime.datetime(2020, 9, 16, 0, 0): '10 17 31 51 53 01', datetime.datetime(2020, 9, 19, 0, 0): '11 14 23 47 57 14', datetime.datetime(2020, 9, 23, 0, 0): '08 17 49 52 59 01', datetime.datetime(2020, 9, 26, 0, 0): '11 21 27 36 62 24', datetime.datetime(2020, 9, 30, 0, 0): '14 18 36 49 67 18', datetime.datetime(2020, 10, 3, 0, 0): '18 31 36 43 47 20', datetime.datetime(2020, 10, 7, 0, 0): '06 24 30 53 56 19', datetime.datetime(2020, 10, 10, 0, 0): '05 18 23 40 50 18', datetime.datetime(2020, 10, 14, 0, 0): '21 37 52 53 58 05', datetime.datetime(2020, 10, 17, 0, 0): '06 10 31 37 44 23', datetime.datetime(2020, 10, 21, 0, 0): '01 03 13 44 56 26', datetime.datetime(2020, 10, 24, 0, 0): '18 20 27 45 65 06', datetime.datetime(2020, 10, 28, 0, 0): '11 28 37 40 53 13', datetime.datetime(2020, 10, 31, 0, 0): '02 06 40 42 55 24', datetime.datetime(2020, 11, 4, 0, 0): '23 32 33 45 49 14', datetime.datetime(2020, 11, 7, 0, 0): '14 16 37 48 58 18', datetime.datetime(2020, 11, 11, 0, 0): '13 15 17 45 63 13', datetime.datetime(2020, 11, 14, 0, 0): '07 15 18 32 45 20', datetime.datetime(2020, 11, 18, 0, 0): '04 05 17 43 52 05', datetime.datetime(2020, 11, 21, 0, 0): '51 54 57 60 69 11', datetime.datetime(2020, 11, 25, 0, 0): '02 57 58 60 65 26', datetime.datetime(2020, 11, 28, 0, 0): '08 12 18 44 51 18', datetime.datetime(2020, 12, 2, 0, 0): '28 31 40 41 46 04', datetime.datetime(2020, 12, 5, 0, 0): '03 04 06 48 53 10', datetime.datetime(2020, 12, 9, 0, 0): '11 14 31 47 48 04', datetime.datetime(2020, 12, 12, 0, 0): '17 54 56 63 69 20', datetime.datetime(2020, 12, 16, 0, 0): '04 23 37 61 67 07', datetime.datetime(2020, 12, 19, 0, 0): '27 32 34 43 52 13', datetime.datetime(2020, 12, 23, 0, 0): '06 13 38 39 53 06', datetime.datetime(2020, 12, 26, 0, 0): '10 24 27 35 53 18', datetime.datetime(2020, 12, 30, 0, 0): '03 43 45 61 65 14', datetime.datetime(2021, 1, 2, 0, 0): '03 04 11 41 67 05', datetime.datetime(2021, 1, 6, 0, 0): '01 20 22 60 66 03', datetime.datetime(2021, 1, 9, 0, 0): '14 26 38 45 46 13', datetime.datetime(2021, 1, 13, 0, 0): '04 19 23 25 49 14', datetime.datetime(2021, 1, 16, 0, 0): '14 20 39 65 67 02', datetime.datetime(2021, 1, 20, 0, 0): '40 53 60 68 69 22', datetime.datetime(2021, 1, 23, 0, 0): '05 08 17 27 28 14', datetime.datetime(2021, 1, 27, 0, 0): '17 33 35 42 52 09', datetime.datetime(2021, 1, 30, 0, 0): '01 02 07 52 61 04', datetime.datetime(2021, 2, 3, 0, 0): '05 37 40 64 66 05', datetime.datetime(2021, 2, 6, 0, 0): '01 16 48 49 65 08', datetime.datetime(2021, 2, 10, 0, 0): '15 39 58 63 67 07', datetime.datetime(2021, 2, 13, 0, 0): '20 28 33 63 68 20', datetime.datetime(2021, 2, 17, 0, 0): '01 15 21 32 46 01', datetime.datetime(2021, 2, 20, 0, 0): '04 08 22 32 58 04', datetime.datetime(2021, 2, 24, 0, 0): '04 33 43 53 65 21', datetime.datetime(2021, 2, 27, 0, 0): '02 28 31 44 52 18', datetime.datetime(2021, 3, 3, 0, 0): '21 40 44 50 55 16', datetime.datetime(2021, 3, 6, 0, 0): '11 31 50 52 58 18', datetime.datetime(2021, 3, 10, 0, 0): '17 18 37 44 53 18', datetime.datetime(2021, 3, 13, 0, 0): '05 11 51 56 61 02', datetime.datetime(2021, 3, 17, 0, 0): '34 38 42 61 62 19', datetime.datetime(2021, 3, 20, 0, 0): '01 06 22 42 61 04', datetime.datetime(2021, 3, 24, 0, 0): '04 09 17 27 38 18', datetime.datetime(2021, 3, 27, 0, 0): '06 14 38 39 65 06', datetime.datetime(2021, 3, 31, 0, 0): '03 10 44 55 68 24', datetime.datetime(2021, 4, 3, 0, 0): '01 12 17 39 53 05', datetime.datetime(2021, 4, 7, 0, 0): '27 35 39 51 66 16', datetime.datetime(2021, 4, 10, 0, 0): '14 16 23 50 53 03', datetime.datetime(2021, 4, 14, 0, 0): '13 30 33 45 61 14', datetime.datetime(2021, 4, 17, 0, 0): '10 21 26 41 49 25', datetime.datetime(2021, 4, 21, 0, 0): '21 25 32 63 67 06', datetime.datetime(2021, 4, 24, 0, 0): '22 36 48 59 61 22', datetime.datetime(2021, 5, 1, 0, 0): '35 36 47 61 63 03', datetime.datetime(2021, 5, 5, 0, 0): '16 23 28 40 63 01', datetime.datetime(2021, 5, 8, 0, 0): '12 17 20 21 26 08', datetime.datetime(2021, 5, 12, 0, 0): '01 19 20 38 54 17', datetime.datetime(2021, 5, 15, 0, 0): '04 10 37 39 69 24', datetime.datetime(2021, 5, 19, 0, 0): '11 13 55 56 69 04', datetime.datetime(2021, 5, 22, 0, 0): '03 19 27 37 40 08', datetime.datetime(2021, 5, 26, 0, 0): '02 08 21 34 62 16', datetime.datetime(2021, 5, 29, 0, 0): '11 13 22 27 46 20', datetime.datetime(2021, 6, 2, 0, 0): '06 07 11 66 67 19', datetime.datetime(2021, 6, 5, 0, 0): '44 52 54 64 69 26', datetime.datetime(2021, 6, 12, 0, 0): '08 25 34 38 41 10', datetime.datetime(2021, 6, 16, 0, 0): '19 29 34 44 50 25', datetime.datetime(2021, 6, 19, 0, 0): '04 22 35 38 39 20', datetime.datetime(2021, 6, 23, 0, 0): '13 20 40 51 63 01', datetime.datetime(2021, 6, 26, 0, 0): '08 31 39 43 60 17', datetime.datetime(2021, 6, 30, 0, 0): '24 29 50 65 66 14', datetime.datetime(2021, 7, 3, 0, 0): '26 40 41 55 65 24', datetime.datetime(2021, 7, 7, 0, 0): '08 21 30 49 57 08', datetime.datetime(2021, 7, 10, 0, 0): '01 05 29 54 62 03', datetime.datetime(2021, 7, 14, 0, 0): '33 46 52 59 62 10', datetime.datetime(2021, 7, 17, 0, 0): '15 22 38 54 66 03', datetime.datetime(2021, 7, 21, 0, 0): '27 28 44 67 68 11', datetime.datetime(2021, 7, 24, 0, 0): '01 04 11 59 67 10', datetime.datetime(2021, 7, 28, 0, 0): '25 30 53 59 60 05', datetime.datetime(2021, 7, 31, 0, 0): '01 21 22 34 47 04', datetime.datetime(2021, 8, 4, 0, 0): '05 21 32 36 58 14', datetime.datetime(2021, 8, 7, 0, 0): '07 24 36 54 60 23', datetime.datetime(2021, 8, 11, 0, 0): '12 18 20 29 30 16', datetime.datetime(2021, 8, 14, 0, 0): '06 21 49 65 67 18', datetime.datetime(2021, 8, 18, 0, 0): '35 36 51 55 61 26', datetime.datetime(2021, 8, 21, 0, 0): '16 28 36 39 59 04', datetime.datetime(2021, 8, 23, 0, 0): '17 36 47 60 61 15', datetime.datetime(2021, 8, 25, 0, 0): '27 39 54 56 59 24', datetime.datetime(2021, 8, 28, 0, 0): '12 22 26 46 59 26', datetime.datetime(2021, 8, 30, 0, 0): '03 15 45 51 61 08', datetime.datetime(2021, 9, 1, 0, 0): '10 20 29 48 51 17', datetime.datetime(2021, 9, 4, 0, 0): '32 35 40 52 54 01', datetime.datetime(2021, 9, 6, 0, 0): '11 20 22 33 54 24', datetime.datetime(2021, 9, 8, 0, 0): '09 22 41 47 61 21', datetime.datetime(2021, 9, 11, 0, 0): '20 31 38 40 49 21', datetime.datetime(2021, 9, 13, 0, 0): '37 40 50 61 63 21', datetime.datetime(2021, 9, 15, 0, 0): '01 04 18 46 62 25', datetime.datetime(2021, 9, 18, 0, 0): '05 36 39 45 57 11', datetime.datetime(2021, 9, 20, 0, 0): '37 51 54 58 60 19', datetime.datetime(2021, 9, 22, 0, 0): '20 40 47 55 63 05', datetime.datetime(2021, 9, 25, 0, 0): '22 23 37 62 63 19', datetime.datetime(2021, 9, 27, 0, 0): '21 22 39 44 60 12', datetime.datetime(2021, 9, 29, 0, 0): '02 07 11 17 32 11', datetime.datetime(2021, 10, 2, 0, 0): '28 38 42 47 52 01', datetime.datetime(2021, 10, 4, 0, 0): '12 22 54 66 69 15', datetime.datetime(2021, 10, 6, 0, 0): '01 17 52 58 64 01', datetime.datetime(2021, 10, 9, 0, 0): '12 17 30 45 62 05', datetime.datetime(2021, 10, 11, 0, 0): '11 20 33 39 65 24', datetime.datetime(2021, 10, 13, 0, 0): '23 29 47 59 60 15', datetime.datetime(2021, 10, 16, 0, 0): '30 31 41 42 48 03', datetime.datetime(2021, 10, 18, 0, 0): '30 32 48 53 63 12', datetime.datetime(2021, 10, 20, 0, 0): '07 29 36 41 43 05', datetime.datetime(2021, 10, 23, 0, 0): '10 30 51 57 63 20', datetime.datetime(2021, 10, 25, 0, 0): '10 27 29 44 58 24', datetime.datetime(2021, 10, 27, 0, 0): '03 06 26 35 51 17', datetime.datetime(2021, 10, 30, 0, 0): '05 23 28 43 56 19', datetime.datetime(2021, 11, 1, 0, 0): '09 25 34 44 45 08', datetime.datetime(2021, 11, 3, 0, 0): '01 02 24 50 57 26', datetime.datetime(2021, 11, 6, 0, 0): '08 30 48 57 64 09', datetime.datetime(2021, 11, 8, 0, 0): '21 46 47 57 62 08', datetime.datetime(2021, 11, 10, 0, 0): '19 25 43 46 48 14', datetime.datetime(2021, 11, 13, 0, 0): '08 15 26 35 45 09', datetime.datetime(2021, 11, 15, 0, 0): '05 31 34 51 53 23', datetime.datetime(2021, 11, 17, 0, 0): '03 16 48 52 60 01', datetime.datetime(2021, 11, 20, 0, 0): '40 43 48 59 69 19', datetime.datetime(2021, 11, 22, 0, 0): '07 20 29 38 67 22', datetime.datetime(2021, 11, 24, 0, 0): '10 16 32 63 65 17', datetime.datetime(2021, 11, 27, 0, 0): '08 32 55 64 66 10', datetime.datetime(2021, 11, 29, 0, 0): '18 26 28 38 47 17', datetime.datetime(2021, 12, 1, 0, 0): '12 15 38 57 63 24', datetime.datetime(2021, 12, 4, 0, 0): '10 40 45 56 67 02', datetime.datetime(2021, 12, 6, 0, 0): '03 21 38 50 59 06', datetime.datetime(2021, 12, 8, 0, 0): '03 07 33 50 69 24', datetime.datetime(2021, 12, 11, 0, 0): '03 25 44 53 64 10', datetime.datetime(2021, 12, 13, 0, 0): '10 30 37 53 59 04', datetime.datetime(2021, 12, 15, 0, 0): '19 20 40 42 59 15', datetime.datetime(2021, 12, 18, 0, 0): '02 06 24 51 61 01', datetime.datetime(2021, 12, 20, 0, 0): '02 13 23 34 66 02', datetime.datetime(2021, 12, 22, 0, 0): '07 16 19 48 68 15', datetime.datetime(2021, 12, 25, 0, 0): '27 29 45 55 58 02', datetime.datetime(2021, 12, 27, 0, 0): '36 38 45 62 64 19', datetime.datetime(2021, 12, 29, 0, 0): '02 06 09 33 39 11', datetime.datetime(2022, 1, 1, 0, 0): '06 12 39 48 50 07', datetime.datetime(2022, 1, 3, 0, 0): '02 13 32 33 48 22', datetime.datetime(2022, 1, 5, 0, 0): '06 14 25 33 46 17', datetime.datetime(2022, 1, 8, 0, 0): '20 21 36 60 65 13', datetime.datetime(2022, 1, 10, 0, 0): '14 17 18 21 27 09', datetime.datetime(2022, 1, 12, 0, 0): '12 21 22 30 33 24', datetime.datetime(2022, 1, 15, 0, 0): '03 18 37 51 59 13', datetime.datetime(2022, 1, 17, 0, 0): '09 24 35 46 65 22', datetime.datetime(2022, 1, 19, 0, 0): '11 15 43 55 61 10', datetime.datetime(2022, 1, 22, 0, 0): '08 14 33 36 67 17', datetime.datetime(2022, 1, 24, 0, 0): '11 29 30 47 53 16', datetime.datetime(2022, 1, 26, 0, 0): '04 11 38 49 69 16', datetime.datetime(2022, 1, 29, 0, 0): '02 15 38 54 65 11', datetime.datetime(2022, 1, 31, 0, 0): '10 15 51 61 69 14', datetime.datetime(2022, 2, 2, 0, 0): '18 29 33 62 63 15', datetime.datetime(2022, 2, 5, 0, 0): '05 16 27 39 61 24', datetime.datetime(2022, 2, 7, 0, 0): '05 15 38 47 65 10', datetime.datetime(2022, 2, 9, 0, 0): '02 17 33 51 63 26', datetime.datetime(2022, 2, 12, 0, 0): '08 10 21 41 62 07', datetime.datetime(2022, 2, 14, 0, 0): '16 25 27 49 55 17', datetime.datetime(2022, 2, 16, 0, 0): '22 30 40 42 48 16', datetime.datetime(2022, 2, 19, 0, 0): '03 10 15 33 42 11', datetime.datetime(2022, 2, 21, 0, 0): '02 36 37 45 69 03', datetime.datetime(2022, 2, 23, 0, 0): '06 17 21 35 64 18', datetime.datetime(2022, 2, 26, 0, 0): '15 32 36 48 64 19', datetime.datetime(2022, 2, 28, 0, 0): '07 21 39 47 55 19', datetime.datetime(2022, 3, 2, 0, 0): '19 37 48 61 63 12', datetime.datetime(2022, 3, 5, 0, 0): '08 23 37 52 63 13', datetime.datetime(2022, 3, 7, 0, 0): '10 43 55 59 67 02', datetime.datetime(2022, 3, 9, 0, 0): '13 22 34 51 67 10', datetime.datetime(2022, 3, 14, 0, 0): '21 28 32 44 49 06', datetime.datetime(2022, 3, 16, 0, 0): '03 28 34 35 58 17', datetime.datetime(2022, 3, 19, 0, 0): '08 09 18 48 52 06', datetime.datetime(2022, 3, 21, 0, 0): '01 15 26 63 65 16', datetime.datetime(2022, 3, 23, 0, 0): '31 32 37 38 48 24', datetime.datetime(2022, 3, 26, 0, 0): '02 10 50 59 61 06', datetime.datetime(2022, 3, 28, 0, 0): '11 18 39 58 62 03', datetime.datetime(2022, 3, 30, 0, 0): '03 07 21 31 37 11', datetime.datetime(2022, 4, 2, 0, 0): '06 28 47 58 59 18', datetime.datetime(2022, 4, 4, 0, 0): '02 32 39 46 69 06', datetime.datetime(2022, 4, 6, 0, 0): '06 42 45 47 64 18', datetime.datetime(2022, 4, 11, 0, 0): '05 07 24 31 34 04', datetime.datetime(2022, 4, 13, 0, 0): '14 16 41 63 68 26', datetime.datetime(2022, 4, 16, 0, 0): '15 21 32 62 65 26', datetime.datetime(2022, 4, 18, 0, 0): '08 33 55 59 62 18', datetime.datetime(2022, 4, 20, 0, 0): '20 30 45 55 56 14', datetime.datetime(2022, 4, 23, 0, 0): '10 39 47 49 56 08', datetime.datetime(2022, 4, 25, 0, 0): '12 18 20 39 61 10', datetime.datetime(2022, 4, 27, 0, 0): '11 36 61 62 68 04', datetime.datetime(2022, 4, 30, 0, 0): '14 21 37 44 63 01', datetime.datetime(2022, 5, 2, 0, 0): '18 27 33 39 44 08', datetime.datetime(2022, 5, 4, 0, 0): '37 39 55 63 69 23', datetime.datetime(2022, 5, 7, 0, 0): '04 05 06 28 67 10', datetime.datetime(2022, 5, 9, 0, 0): '18 30 35 52 56 05', datetime.datetime(2022, 5, 11, 0, 0): '05 07 61 63 69 18', datetime.datetime(2022, 5, 14, 0, 0): '06 40 41 45 52 09', datetime.datetime(2022, 5, 16, 0, 0): '07 15 22 36 64 13', datetime.datetime(2022, 5, 18, 0, 0): '40 41 58 64 65 17', datetime.datetime(2022, 5, 21, 0, 0): '14 15 25 52 58 11', datetime.datetime(2022, 5, 23, 0, 0): '01 33 37 39 42 26', datetime.datetime(2022, 5, 25, 0, 0): '19 28 39 42 57 17', datetime.datetime(2022, 5, 28, 0, 0): '02 39 50 61 66 15', datetime.datetime(2022, 5, 30, 0, 0): '27 28 51 68 69 22', datetime.datetime(2022, 6, 1, 0, 0): '11 41 56 57 63 02', datetime.datetime(2022, 6, 4, 0, 0): '14 16 36 52 60 16', datetime.datetime(2022, 6, 6, 0, 0): '02 10 35 44 46 04', datetime.datetime(2022, 6, 8, 0, 0): '22 39 43 62 64 07', datetime.datetime(2022, 6, 11, 0, 0): '18 20 26 53 69 05', datetime.datetime(2022, 6, 13, 0, 0): '02 27 42 44 51 25', datetime.datetime(2022, 6, 15, 0, 0): '19 28 41 42 51 07', datetime.datetime(2022, 6, 18, 0, 0): '10 19 40 45 58 25', datetime.datetime(2022, 6, 20, 0, 0): '03 44 61 63 69 13', datetime.datetime(2022, 6, 22, 0, 0): '06 10 31 48 56 12', datetime.datetime(2022, 6, 25, 0, 0): '06 12 20 27 32 04', datetime.datetime(2022, 6, 27, 0, 0): '11 13 18 30 37 16', datetime.datetime(2022, 6, 29, 0, 0): '08 40 49 58 63 14', datetime.datetime(2022, 7, 2, 0, 0): '09 10 37 59 62 26', datetime.datetime(2022, 7, 4, 0, 0): '15 16 24 31 56 04', datetime.datetime(2022, 7, 6, 0, 0): '32 36 49 62 69 13', datetime.datetime(2022, 7, 9, 0, 0): '14 22 42 46 52 24', datetime.datetime(2022, 7, 11, 0, 0): '04 26 34 37 52 09', datetime.datetime(2022, 7, 13, 0, 0): '22 23 36 47 63 02', datetime.datetime(2022, 7, 16, 0, 0): '03 18 23 32 57 21', datetime.datetime(2022, 7, 18, 0, 0): '14 34 36 50 58 05', datetime.datetime(2022, 7, 20, 0, 0): '10 20 23 49 65 22', datetime.datetime(2022, 7, 23, 0, 0): '39 41 54 59 62 12', datetime.datetime(2022, 7, 25, 0, 0): '25 37 38 39 65 05', datetime.datetime(2022, 7, 27, 0, 0): '01 25 44 55 57 26', datetime.datetime(2022, 7, 30, 0, 0): '04 17 57 58 68 12', datetime.datetime(2022, 8, 1, 0, 0): '15 21 31 36 65 16', datetime.datetime(2022, 8, 3, 0, 0): '09 21 56 57 66 11', datetime.datetime(2022, 8, 6, 0, 0): '08 15 46 56 68 03', datetime.datetime(2022, 8, 8, 0, 0): '32 45 51 57 58 12', datetime.datetime(2022, 8, 10, 0, 0): '29 44 59 61 68 19', datetime.datetime(2022, 8, 13, 0, 0): '19 24 35 43 62 02', datetime.datetime(2022, 8, 15, 0, 0): '20 24 47 50 63 05', datetime.datetime(2022, 8, 17, 0, 0): '23 28 41 50 55 24', datetime.datetime(2022, 8, 20, 0, 0): '05 09 11 16 66 07', datetime.datetime(2022, 8, 22, 0, 0): '12 27 34 55 67 09', datetime.datetime(2022, 8, 24, 0, 0): '06 24 35 37 44 22', datetime.datetime(2022, 8, 27, 0, 0): '02 18 56 60 65 14', datetime.datetime(2022, 8, 29, 0, 0): '13 36 43 61 69 18', datetime.datetime(2022, 8, 31, 0, 0): '07 08 19 24 28 01', datetime.datetime(2022, 9, 3, 0, 0): '18 27 49 65 69 09', datetime.datetime(2022, 9, 5, 0, 0): '04 07 32 55 64 25', datetime.datetime(2022, 9, 7, 0, 0): '03 16 30 33 36 20', datetime.datetime(2022, 9, 10, 0, 0): '38 42 56 68 69 04', datetime.datetime(2022, 9, 12, 0, 0): '06 14 16 34 66 25', datetime.datetime(2022, 9, 14, 0, 0): '09 10 20 22 52 25', datetime.datetime(2022, 9, 17, 0, 0): '05 25 36 51 61 01', datetime.datetime(2022, 9, 19, 0, 0): '07 15 36 46 67 07', datetime.datetime(2022, 9, 21, 0, 0): '06 33 34 45 54 07', datetime.datetime(2022, 9, 24, 0, 0): '03 09 21 24 29 14', datetime.datetime(2022, 9, 26, 0, 0): '13 20 31 33 59 20', datetime.datetime(2022, 9, 28, 0, 0): '06 10 24 33 67 11', datetime.datetime(2022, 10, 1, 0, 0): '08 21 22 65 69 26', datetime.datetime(2022, 10, 3, 0, 0): '02 16 22 55 63 22', datetime.datetime(2022, 10, 5, 0, 0): '26 30 33 37 62 06', datetime.datetime(2022, 10, 8, 0, 0): '13 43 53 60 68 05', datetime.datetime(2022, 10, 10, 0, 0): '03 06 11 17 22 11', datetime.datetime(2022, 10, 12, 0, 0): '14 30 41 42 59 06', datetime.datetime(2022, 10, 15, 0, 0): '32 37 40 58 62 15', datetime.datetime(2022, 10, 17, 0, 0): '19 30 36 46 60 25', datetime.datetime(2022, 10, 19, 0, 0): '06 08 15 27 42 10', datetime.datetime(2022, 10, 22, 0, 0): '19 25 48 55 60 18', datetime.datetime(2022, 10, 24, 0, 0): '18 23 35 45 54 16', datetime.datetime(2022, 10, 26, 0, 0): '19 36 37 46 56 24', datetime.datetime(2022, 10, 29, 0, 0): '19 31 40 46 57 23', datetime.datetime(2022, 10, 31, 0, 0): '13 19 36 39 59 13', datetime.datetime(2022, 11, 2, 0, 0): '02 11 22 35 60 23', datetime.datetime(2022, 11, 5, 0, 0): '28 45 53 56 69 20', datetime.datetime(2022, 11, 9, 0, 0): '07 14 24 30 56 07', datetime.datetime(2022, 11, 12, 0, 0): '16 20 44 57 58 06', datetime.datetime(2022, 11, 14, 0, 0): '19 35 53 54 67 21', datetime.datetime(2022, 11, 16, 0, 0): '28 34 51 53 56 11', datetime.datetime(2022, 11, 19, 0, 0): '07 28 62 63 64 10', datetime.datetime(2022, 11, 21, 0, 0): '01 06 40 51 67 02', datetime.datetime(2022, 11, 23, 0, 0): '01 02 31 39 66 25', datetime.datetime(2022, 11, 26, 0, 0): '15 30 47 50 51 03', datetime.datetime(2022, 11, 28, 0, 0): '29 30 32 48 50 20', datetime.datetime(2022, 11, 30, 0, 0): '04 19 24 47 66 10', datetime.datetime(2022, 12, 3, 0, 0): '06 13 33 36 37 07', datetime.datetime(2022, 12, 5, 0, 0): '35 45 47 54 55 14', datetime.datetime(2022, 12, 7, 0, 0): '06 28 44 59 61 21', datetime.datetime(2022, 12, 10, 0, 0): '09 23 47 49 68 19', datetime.datetime(2022, 12, 12, 0, 0): '16 31 50 55 61 09', datetime.datetime(2022, 12, 14, 0, 0): '36 51 59 66 68 25', datetime.datetime(2022, 12, 17, 0, 0): '33 56 64 66 68 12', datetime.datetime(2022, 12, 19, 0, 0): '07 37 55 65 67 12', datetime.datetime(2022, 12, 21, 0, 0): '12 15 24 34 59 14', datetime.datetime(2022, 12, 24, 0, 0): '17 37 46 54 67 08', datetime.datetime(2022, 12, 26, 0, 0): '17 41 47 60 61 17', datetime.datetime(2022, 12, 28, 0, 0): '26 32 38 45 56 01', datetime.datetime(2022, 12, 31, 0, 0): '18 37 44 50 64 11', datetime.datetime(2023, 1, 2, 0, 0): '07 09 12 31 62 22', datetime.datetime(2023, 1, 4, 0, 0): '12 32 56 67 68 26', datetime.datetime(2023, 1, 7, 0, 0): '35 36 44 45 67 14', datetime.datetime(2023, 1, 9, 0, 0): '18 43 48 60 69 14', datetime.datetime(2023, 1, 11, 0, 0): '04 08 46 47 48 05', datetime.datetime(2023, 1, 14, 0, 0): '24 26 39 47 57 23', datetime.datetime(2023, 1, 16, 0, 0): '04 14 33 39 61 03', datetime.datetime(2023, 1, 18, 0, 0): '06 15 22 42 47 26', datetime.datetime(2023, 1, 21, 0, 0): '05 14 19 46 64 22', datetime.datetime(2023, 1, 23, 0, 0): '12 31 47 58 60 23', datetime.datetime(2023, 1, 25, 0, 0): '09 17 20 38 40 18', datetime.datetime(2023, 1, 28, 0, 0): '02 18 23 27 47 15', datetime.datetime(2023, 1, 30, 0, 0): '01 04 12 36 49 05', datetime.datetime(2023, 2, 1, 0, 0): '31 43 58 59 66 09', datetime.datetime(2023, 2, 4, 0, 0): '02 08 15 19 58 10', datetime.datetime(2023, 2, 6, 0, 0): '05 11 22 23 69 07', datetime.datetime(2023, 2, 8, 0, 0): '52 58 59 64 66 09', datetime.datetime(2023, 2, 11, 0, 0): '10 23 30 54 65 11', datetime.datetime(2023, 2, 13, 0, 0): '17 26 37 61 65 02', datetime.datetime(2023, 2, 15, 0, 0): '31 32 54 60 63 12', datetime.datetime(2023, 2, 18, 0, 0): '08 21 31 32 37 23', datetime.datetime(2023, 2, 20, 0, 0): '03 17 26 38 54 15', datetime.datetime(2023, 2, 22, 0, 0): '11 19 39 44 65 07', datetime.datetime(2023, 2, 25, 0, 0): '11 24 58 66 67 26', datetime.datetime(2023, 2, 27, 0, 0): '16 28 49 51 55 23', datetime.datetime(2023, 3, 1, 0, 0): '02 09 28 36 53 04', datetime.datetime(2023, 3, 4, 0, 0): '10 16 18 40 66 16', datetime.datetime(2023, 3, 6, 0, 0): '02 13 29 58 69 04', datetime.datetime(2023, 3, 8, 0, 0): '26 27 43 61 69 04', datetime.datetime(2023, 3, 11, 0, 0): '11 20 33 43 58 24', datetime.datetime(2023, 3, 13, 0, 0): '03 10 24 46 63 04', datetime.datetime(2023, 3, 15, 0, 0): '16 18 33 37 50 08', datetime.datetime(2023, 3, 18, 0, 0): '14 20 30 54 69 11', datetime.datetime(2023, 3, 20, 0, 0): '01 27 32 47 67 14', datetime.datetime(2023, 3, 22, 0, 0): '27 28 37 50 57 05', datetime.datetime(2023, 3, 25, 0, 0): '15 17 18 47 57 19', datetime.datetime(2023, 3, 27, 0, 0): '19 26 36 43 58 14', datetime.datetime(2023, 3, 29, 0, 0): '04 09 24 46 66 07', datetime.datetime(2023, 4, 1, 0, 0): '11 19 21 29 52 17', datetime.datetime(2023, 4, 3, 0, 0): '16 30 31 54 68 01', datetime.datetime(2023, 4, 5, 0, 0): '03 05 09 42 52 11', datetime.datetime(2023, 4, 8, 0, 0): '11 22 24 51 60 18', datetime.datetime(2023, 4, 10, 0, 0): '09 10 36 46 52 14', datetime.datetime(2023, 4, 12, 0, 0): '09 36 41 44 59 04', datetime.datetime(2023, 4, 15, 0, 0): '01 33 34 56 59 18', datetime.datetime(2023, 4, 17, 0, 0): '23 25 35 63 64 25', datetime.datetime(2023, 4, 19, 0, 0): '04 11 21 38 64 11', datetime.datetime(2023, 4, 22, 0, 0): '17 36 47 63 68 25', datetime.datetime(2023, 4, 24, 0, 0): '19 21 55 66 68 03', datetime.datetime(2023, 4, 26, 0, 0): '02 15 30 35 49 06', datetime.datetime(2023, 4, 29, 0, 0): '16 53 54 57 65 08', datetime.datetime(2023, 5, 1, 0, 0): '03 38 55 61 66 01', datetime.datetime(2023, 5, 3, 0, 0): '21 26 30 45 47 23', datetime.datetime(2023, 5, 6, 0, 0): '31 39 47 51 53 06', datetime.datetime(2023, 5, 8, 0, 0): '15 20 33 36 43 12', datetime.datetime(2023, 5, 10, 0, 0): '21 24 33 55 69 03', datetime.datetime(2023, 5, 13, 0, 0): '03 15 20 23 46 11', datetime.datetime(2023, 5, 15, 0, 0): '01 26 28 55 58 25', datetime.datetime(2023, 5, 17, 0, 0): '18 34 37 45 51 14', datetime.datetime(2023, 5, 20, 0, 0): '17 23 32 38 63 23', datetime.datetime(2023, 5, 22, 0, 0): '09 38 48 52 68 25', datetime.datetime(2023, 5, 24, 0, 0): '12 21 44 50 58 26', datetime.datetime(2023, 5, 27, 0, 0): '24 38 39 48 56 04', datetime.datetime(2023, 5, 29, 0, 0): '21 33 35 62 64 24', datetime.datetime(2023, 5, 31, 0, 0): '02 04 54 61 62 14', datetime.datetime(2023, 6, 3, 0, 0): '15 45 64 67 68 18', datetime.datetime(2023, 6, 5, 0, 0): '02 31 45 46 49 20', datetime.datetime(2023, 6, 7, 0, 0): '16 21 29 53 66 02', datetime.datetime(2023, 6, 10, 0, 0): '21 32 42 46 50 04', datetime.datetime(2023, 6, 12, 0, 0): '02 03 16 23 68 07', datetime.datetime(2023, 6, 14, 0, 0): '03 20 36 42 64 04', datetime.datetime(2023, 6, 17, 0, 0): '02 12 45 61 64 26', datetime.datetime(2023, 6, 19, 0, 0): '36 39 52 57 69 01', datetime.datetime(2023, 6, 21, 0, 0): '05 11 33 35 63 14', datetime.datetime(2023, 6, 24, 0, 0): '02 38 44 50 62 19', datetime.datetime(2023, 6, 26, 0, 0): '06 28 39 43 54 12', datetime.datetime(2023, 6, 28, 0, 0): '19 25 34 57 68 04', datetime.datetime(2023, 7, 1, 0, 0): '04 17 35 49 61 08', datetime.datetime(2023, 7, 3, 0, 0): '15 26 31 38 61 03', datetime.datetime(2023, 7, 5, 0, 0): '17 24 48 62 68 23', datetime.datetime(2023, 7, 8, 0, 0): '07 23 24 32 43 18', datetime.datetime(2023, 7, 10, 0, 0): '02 24 34 53 58 13'}\n"
          ]
        }
      ],
      "source": [
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "import json\n",
        "stats_file = \"./data_sources/Lottery_Powerball_Winning_Numbers__Beginning_2010.json\"\n",
        "powerball = {}\n",
        "lines = []\n",
        "with open(stats_file, 'r') as f:\n",
        "    stats = json.load(f)\n",
        "    for data in stats:\n",
        "        dt = parse(data['Draw Date'])\n",
        "        powerball[dt] = data['Winning Numbers']\n",
        "\n",
        "res = {key: val for key, val in sorted(powerball.items(), key = lambda ele: ele[0])}\n",
        "pre_numbers = None\n",
        "for dt in powerball:\n",
        "    if pre_numbers is not None:\n",
        "        lines.append(f\"{pre_numbers} : {powerball[dt]}\\n\")\n",
        "    pre_numbers = powerball[dt]\n",
        "with open(f'./models/powerball2/prompts.txt', 'w', encoding='utf-8') as f:\n",
        "    f.writelines(lines)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stats_file = \"./data_sources/Lottery_Powerball_Winning_Numbers__Beginning_2010.json\"\n",
        "powerball = []\n",
        "test = []\n",
        "lines = []\n",
        "with open(stats_file, 'r') as f:\n",
        "    stats = json.load(f)\n",
        "    for data in stats:\n",
        "        powerball.append(\n",
        "            {\n",
        "            'instruction':f\"what are the powerball numbers from {data['Draw Date']}\",\n",
        "            \"output\":data['Winning Numbers']\n",
        "            }\n",
        "        )\n",
        "        lines.append(f\"{data['Draw Date']} : {data['Winning Numbers']}\\n\")\n",
        "    with open(f'./models/powerball/prompts.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(powerball, f, ensure_ascii=True, indent=4, allow_nan=True)\n",
        "        f.close()\n",
        "    with open(f'./models/powerball/prompts.txt', 'w', encoding='utf-8') as f:\n",
        "        f.writelines(lines)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "sample = []\n",
        "with open(\"./data_sources/lora_training_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    for d in data:\n",
        "        if random.random()*500000 < 15122:\n",
        "            sample.append(f\"###instruction:{d['instruction']} ###input {d['input']} ###response\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gI1Tp54IiVBj"
      },
      "source": [
        "## Train a custom tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from pathlib import Path\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[    \n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "    ])\n",
        "\n",
        "tokenizer.train(files=[\"./data_sources/lora_training_data.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"./models/mlb/tokenizer/powerball.json\")\n",
        "\n",
        "output = tokenizer.encode(\"what is the outcome of pitcher 461833 pitching to batter 435079\")\n",
        "print(output.tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=30000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(tokenizer_file=\"./models/mlb/tokenizer/powerball.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66586416"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./data_sources/lora_training_data.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models/mlb\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=500000,\n",
        "    per_device_train_batch_size=128,\n",
        "    save_steps=5000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d8507293a64482aad4bb0877b82bcce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0837, 'learning_rate': 2.55859375e-05, 'epoch': 500.0}\n",
            "{'loss': 0.0035, 'learning_rate': 1.1718750000000001e-06, 'epoch': 1000.0}\n",
            "{'train_runtime': 47.5922, 'train_samples_per_second': 21.516, 'train_steps_per_second': 21.516, 'train_loss': 0.5309643920336384, 'epoch': 1024.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1024, training_loss=0.5309643920336384, metrics={'train_runtime': 47.5922, 'train_samples_per_second': 21.516, 'train_steps_per_second': 21.516, 'train_loss': 0.5309643920336384, 'epoch': 1024.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21361042536d4104b4af73a0737d343a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.011, 'learning_rate': 3.7762910000000005e-05, 'epoch': 500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.775791e-05, 'epoch': 1000.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.775291e-05, 'epoch': 1500.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.7747910000000006e-05, 'epoch': 2000.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.774291e-05, 'epoch': 2500.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.7737910000000004e-05, 'epoch': 3000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.773291e-05, 'epoch': 3500.0}\n",
            "{'loss': 0.0037, 'learning_rate': 3.772791e-05, 'epoch': 4000.0}\n",
            "{'loss': 0.0002, 'learning_rate': 3.772291e-05, 'epoch': 4500.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.771791e-05, 'epoch': 5000.0}\n",
            "{'loss': 0.001, 'learning_rate': 3.7712910000000003e-05, 'epoch': 5500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.770791e-05, 'epoch': 6000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.770291e-05, 'epoch': 6500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7697910000000004e-05, 'epoch': 7000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.769291e-05, 'epoch': 7500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.768791e-05, 'epoch': 8000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.768291e-05, 'epoch': 8500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.767791e-05, 'epoch': 9000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.767291e-05, 'epoch': 9500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.766791e-05, 'epoch': 10000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.766291e-05, 'epoch': 10500.0}\n",
            "{'loss': 0.0015, 'learning_rate': 3.7657910000000005e-05, 'epoch': 11000.0}\n",
            "{'loss': 0.0068, 'learning_rate': 3.765291e-05, 'epoch': 11500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.764791e-05, 'epoch': 12000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7642910000000006e-05, 'epoch': 12500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.763791e-05, 'epoch': 13000.0}\n",
            "{'loss': 0.0079, 'learning_rate': 3.7632910000000004e-05, 'epoch': 13500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.762791e-05, 'epoch': 14000.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.762291e-05, 'epoch': 14500.0}\n",
            "{'loss': 0.0416, 'learning_rate': 3.761791e-05, 'epoch': 15000.0}\n",
            "{'loss': 0.0011, 'learning_rate': 3.761291e-05, 'epoch': 15500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7607910000000004e-05, 'epoch': 16000.0}\n",
            "{'loss': 0.0067, 'learning_rate': 3.760291e-05, 'epoch': 16500.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.759791e-05, 'epoch': 17000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7592910000000005e-05, 'epoch': 17500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.758791000000001e-05, 'epoch': 18000.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.758291e-05, 'epoch': 18500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.757791e-05, 'epoch': 19000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.757291e-05, 'epoch': 19500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.756791e-05, 'epoch': 20000.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.756291e-05, 'epoch': 20500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.755791e-05, 'epoch': 21000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7552910000000005e-05, 'epoch': 21500.0}\n",
            "{'loss': 0.0363, 'learning_rate': 3.754791e-05, 'epoch': 22000.0}\n",
            "{'loss': 0.0608, 'learning_rate': 3.7542910000000003e-05, 'epoch': 22500.0}\n",
            "{'loss': 0.0156, 'learning_rate': 3.7537910000000006e-05, 'epoch': 23000.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.753291e-05, 'epoch': 23500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.752791e-05, 'epoch': 24000.0}\n",
            "{'loss': 0.0034, 'learning_rate': 3.752291e-05, 'epoch': 24500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.751791e-05, 'epoch': 25000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.751291e-05, 'epoch': 25500.0}\n",
            "{'loss': 0.0013, 'learning_rate': 3.750791e-05, 'epoch': 26000.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.7502910000000004e-05, 'epoch': 26500.0}\n",
            "{'loss': 0.0023, 'learning_rate': 3.749791e-05, 'epoch': 27000.0}\n",
            "{'loss': 0.0239, 'learning_rate': 3.749291e-05, 'epoch': 27500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7487910000000005e-05, 'epoch': 28000.0}\n",
            "{'loss': 0.0453, 'learning_rate': 3.748291000000001e-05, 'epoch': 28500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.7477909999999996e-05, 'epoch': 29000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.747291e-05, 'epoch': 29500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.746791e-05, 'epoch': 30000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7462910000000004e-05, 'epoch': 30500.0}\n",
            "{'loss': 0.0015, 'learning_rate': 3.745791e-05, 'epoch': 31000.0}\n",
            "{'loss': 0.0051, 'learning_rate': 3.745291e-05, 'epoch': 31500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7447910000000005e-05, 'epoch': 32000.0}\n",
            "{'loss': 0.0534, 'learning_rate': 3.744291e-05, 'epoch': 32500.0}\n",
            "{'loss': 0.0004, 'learning_rate': 3.7437910000000004e-05, 'epoch': 33000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7432910000000006e-05, 'epoch': 33500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.742791e-05, 'epoch': 34000.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.742291e-05, 'epoch': 34500.0}\n",
            "{'loss': 0.0136, 'learning_rate': 3.741791e-05, 'epoch': 35000.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.741291e-05, 'epoch': 35500.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.740791e-05, 'epoch': 36000.0}\n",
            "{'loss': 0.0059, 'learning_rate': 3.740291e-05, 'epoch': 36500.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.7397910000000004e-05, 'epoch': 37000.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.7392910000000007e-05, 'epoch': 37500.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.738791e-05, 'epoch': 38000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7382910000000005e-05, 'epoch': 38500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.737791e-05, 'epoch': 39000.0}\n",
            "{'loss': 0.0338, 'learning_rate': 3.7372909999999997e-05, 'epoch': 39500.0}\n",
            "{'loss': 0.0358, 'learning_rate': 3.736791e-05, 'epoch': 40000.0}\n",
            "{'loss': 0.0332, 'learning_rate': 3.736291e-05, 'epoch': 40500.0}\n",
            "{'loss': 0.0066, 'learning_rate': 3.7357910000000004e-05, 'epoch': 41000.0}\n",
            "{'loss': 0.0013, 'learning_rate': 3.735291e-05, 'epoch': 41500.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.734791e-05, 'epoch': 42000.0}\n",
            "{'loss': 0.0215, 'learning_rate': 3.7342910000000005e-05, 'epoch': 42500.0}\n",
            "{'loss': 0.0224, 'learning_rate': 3.733791e-05, 'epoch': 43000.0}\n",
            "{'loss': 0.0049, 'learning_rate': 3.7332910000000004e-05, 'epoch': 43500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.732791e-05, 'epoch': 44000.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.732291e-05, 'epoch': 44500.0}\n",
            "{'loss': 0.0011, 'learning_rate': 3.731791e-05, 'epoch': 45000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.731291e-05, 'epoch': 45500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.730791e-05, 'epoch': 46000.0}\n",
            "{'loss': 0.0009, 'learning_rate': 3.730291e-05, 'epoch': 46500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.729791e-05, 'epoch': 47000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7292910000000004e-05, 'epoch': 47500.0}\n",
            "{'loss': 0.0052, 'learning_rate': 3.728791000000001e-05, 'epoch': 48000.0}\n",
            "{'loss': 0.0151, 'learning_rate': 3.728291e-05, 'epoch': 48500.0}\n",
            "{'loss': 0.0061, 'learning_rate': 3.727791e-05, 'epoch': 49000.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.727291e-05, 'epoch': 49500.0}\n",
            "{'loss': 0.0032, 'learning_rate': 3.7267910000000003e-05, 'epoch': 50000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.726291e-05, 'epoch': 50500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.725791e-05, 'epoch': 51000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7252910000000004e-05, 'epoch': 51500.0}\n",
            "{'loss': 0.0058, 'learning_rate': 3.724791e-05, 'epoch': 52000.0}\n",
            "{'loss': 0.0247, 'learning_rate': 3.724291e-05, 'epoch': 52500.0}\n",
            "{'loss': 0.0068, 'learning_rate': 3.7237910000000005e-05, 'epoch': 53000.0}\n",
            "{'loss': 0.0006, 'learning_rate': 3.723291e-05, 'epoch': 53500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.722791e-05, 'epoch': 54000.0}\n",
            "{'loss': 0.0013, 'learning_rate': 3.722291e-05, 'epoch': 54500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.721791e-05, 'epoch': 55000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.721291e-05, 'epoch': 55500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.720791e-05, 'epoch': 56000.0}\n",
            "{'loss': 0.0006, 'learning_rate': 3.720291e-05, 'epoch': 56500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7197910000000006e-05, 'epoch': 57000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.719291e-05, 'epoch': 57500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7187910000000004e-05, 'epoch': 58000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.718291000000001e-05, 'epoch': 58500.0}\n",
            "{'loss': 0.0058, 'learning_rate': 3.7177909999999996e-05, 'epoch': 59000.0}\n",
            "{'loss': 0.0039, 'learning_rate': 3.717291e-05, 'epoch': 59500.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.716791e-05, 'epoch': 60000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7162910000000004e-05, 'epoch': 60500.0}\n",
            "{'loss': 0.0325, 'learning_rate': 3.715791e-05, 'epoch': 61000.0}\n",
            "{'loss': 0.0195, 'learning_rate': 3.715291e-05, 'epoch': 61500.0}\n",
            "{'loss': 0.0247, 'learning_rate': 3.7147910000000005e-05, 'epoch': 62000.0}\n",
            "{'loss': 0.004, 'learning_rate': 3.714291e-05, 'epoch': 62500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.713791e-05, 'epoch': 63000.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.7132910000000006e-05, 'epoch': 63500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.712791e-05, 'epoch': 64000.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.712291e-05, 'epoch': 64500.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.711791e-05, 'epoch': 65000.0}\n",
            "{'loss': 0.0033, 'learning_rate': 3.711291e-05, 'epoch': 65500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.710791e-05, 'epoch': 66000.0}\n",
            "{'loss': 0.0025, 'learning_rate': 3.710291e-05, 'epoch': 66500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7097910000000003e-05, 'epoch': 67000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.7092910000000006e-05, 'epoch': 67500.0}\n",
            "{'loss': 0.0068, 'learning_rate': 3.708791e-05, 'epoch': 68000.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.7082910000000004e-05, 'epoch': 68500.0}\n",
            "{'loss': 0.0153, 'learning_rate': 3.707791e-05, 'epoch': 69000.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.707291e-05, 'epoch': 69500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.706791e-05, 'epoch': 70000.0}\n",
            "{'loss': 0.0051, 'learning_rate': 3.706291e-05, 'epoch': 70500.0}\n",
            "{'loss': 0.0042, 'learning_rate': 3.7057910000000004e-05, 'epoch': 71000.0}\n",
            "{'loss': 0.0037, 'learning_rate': 3.705291e-05, 'epoch': 71500.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.704791e-05, 'epoch': 72000.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.7042910000000005e-05, 'epoch': 72500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.703791e-05, 'epoch': 73000.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.703291e-05, 'epoch': 73500.0}\n",
            "{'loss': 0.0023, 'learning_rate': 3.7027910000000006e-05, 'epoch': 74000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.702291e-05, 'epoch': 74500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.701791e-05, 'epoch': 75000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.701291e-05, 'epoch': 75500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.700791e-05, 'epoch': 76000.0}\n",
            "{'loss': 0.0062, 'learning_rate': 3.700291e-05, 'epoch': 76500.0}\n",
            "{'loss': 0.0097, 'learning_rate': 3.699791e-05, 'epoch': 77000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6992910000000004e-05, 'epoch': 77500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6987910000000006e-05, 'epoch': 78000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.698291e-05, 'epoch': 78500.0}\n",
            "{'loss': 0.0032, 'learning_rate': 3.6977910000000005e-05, 'epoch': 79000.0}\n",
            "{'loss': 0.0003, 'learning_rate': 3.697291e-05, 'epoch': 79500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.696791e-05, 'epoch': 80000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.696291e-05, 'epoch': 80500.0}\n",
            "{'loss': 0.0146, 'learning_rate': 3.695791e-05, 'epoch': 81000.0}\n",
            "{'loss': 0.0044, 'learning_rate': 3.6952910000000004e-05, 'epoch': 81500.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.694791e-05, 'epoch': 82000.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.694291e-05, 'epoch': 82500.0}\n",
            "{'loss': 0.0002, 'learning_rate': 3.6937910000000005e-05, 'epoch': 83000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.693291e-05, 'epoch': 83500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.692791e-05, 'epoch': 84000.0}\n",
            "{'loss': 0.0056, 'learning_rate': 3.692291e-05, 'epoch': 84500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.691791e-05, 'epoch': 85000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.691291e-05, 'epoch': 85500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.690791e-05, 'epoch': 86000.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.690291e-05, 'epoch': 86500.0}\n",
            "{'loss': 0.017, 'learning_rate': 3.6897910000000005e-05, 'epoch': 87000.0}\n",
            "{'loss': 0.0027, 'learning_rate': 3.689291e-05, 'epoch': 87500.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.6887910000000004e-05, 'epoch': 88000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6882910000000006e-05, 'epoch': 88500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.687791e-05, 'epoch': 89000.0}\n",
            "{'loss': 0.0071, 'learning_rate': 3.687291e-05, 'epoch': 89500.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.686791e-05, 'epoch': 90000.0}\n",
            "{'loss': 0.0009, 'learning_rate': 3.686291e-05, 'epoch': 90500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.685791e-05, 'epoch': 91000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.685291e-05, 'epoch': 91500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6847910000000004e-05, 'epoch': 92000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.684291e-05, 'epoch': 92500.0}\n",
            "{'loss': 0.0053, 'learning_rate': 3.683791e-05, 'epoch': 93000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6832910000000005e-05, 'epoch': 93500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.682791000000001e-05, 'epoch': 94000.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.682291e-05, 'epoch': 94500.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.681791e-05, 'epoch': 95000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.681291e-05, 'epoch': 95500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.680791e-05, 'epoch': 96000.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.680291e-05, 'epoch': 96500.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.679791e-05, 'epoch': 97000.0}\n",
            "{'loss': 0.0033, 'learning_rate': 3.6792910000000005e-05, 'epoch': 97500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.678791e-05, 'epoch': 98000.0}\n",
            "{'loss': 0.0015, 'learning_rate': 3.6782910000000004e-05, 'epoch': 98500.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.6777910000000006e-05, 'epoch': 99000.0}\n",
            "{'loss': 0.0119, 'learning_rate': 3.677291e-05, 'epoch': 99500.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.676791e-05, 'epoch': 100000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.676291e-05, 'epoch': 100500.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.675791e-05, 'epoch': 101000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.675291e-05, 'epoch': 101500.0}\n",
            "{'loss': 0.0037, 'learning_rate': 3.674791e-05, 'epoch': 102000.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.6742910000000004e-05, 'epoch': 102500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.673791e-05, 'epoch': 103000.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.673291e-05, 'epoch': 103500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6727910000000005e-05, 'epoch': 104000.0}\n",
            "{'loss': 0.0026, 'learning_rate': 3.672291e-05, 'epoch': 104500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.671791e-05, 'epoch': 105000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.671291e-05, 'epoch': 105500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.670791e-05, 'epoch': 106000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6702910000000005e-05, 'epoch': 106500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.669791e-05, 'epoch': 107000.0}\n",
            "{'loss': 0.017, 'learning_rate': 3.669291e-05, 'epoch': 107500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6687910000000006e-05, 'epoch': 108000.0}\n",
            "{'loss': 0.0079, 'learning_rate': 3.668291e-05, 'epoch': 108500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6677910000000004e-05, 'epoch': 109000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.667291e-05, 'epoch': 109500.0}\n",
            "{'loss': 0.0027, 'learning_rate': 3.666791e-05, 'epoch': 110000.0}\n",
            "{'loss': 0.0054, 'learning_rate': 3.666291e-05, 'epoch': 110500.0}\n",
            "{'loss': 0.0031, 'learning_rate': 3.665791e-05, 'epoch': 111000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6652910000000003e-05, 'epoch': 111500.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.664791e-05, 'epoch': 112000.0}\n",
            "{'loss': 0.0037, 'learning_rate': 3.664291e-05, 'epoch': 112500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6637910000000004e-05, 'epoch': 113000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.663291e-05, 'epoch': 113500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.662791e-05, 'epoch': 114000.0}\n",
            "{'loss': 0.0044, 'learning_rate': 3.662291e-05, 'epoch': 114500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.661791e-05, 'epoch': 115000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.661291e-05, 'epoch': 115500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.660791e-05, 'epoch': 116000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.660291e-05, 'epoch': 116500.0}\n",
            "{'loss': 0.0011, 'learning_rate': 3.6597910000000005e-05, 'epoch': 117000.0}\n",
            "{'loss': 0.0061, 'learning_rate': 3.659291e-05, 'epoch': 117500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.658791e-05, 'epoch': 118000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6582910000000006e-05, 'epoch': 118500.0}\n",
            "{'loss': 0.0044, 'learning_rate': 3.657791e-05, 'epoch': 119000.0}\n",
            "{'loss': 0.0072, 'learning_rate': 3.657291e-05, 'epoch': 119500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.656791e-05, 'epoch': 120000.0}\n",
            "{'loss': 0.0137, 'learning_rate': 3.656291e-05, 'epoch': 120500.0}\n",
            "{'loss': 0.0055, 'learning_rate': 3.655791e-05, 'epoch': 121000.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.655291e-05, 'epoch': 121500.0}\n",
            "{'loss': 0.0109, 'learning_rate': 3.6547910000000004e-05, 'epoch': 122000.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.654291e-05, 'epoch': 122500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.653791e-05, 'epoch': 123000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6532910000000005e-05, 'epoch': 123500.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.652791000000001e-05, 'epoch': 124000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6522909999999996e-05, 'epoch': 124500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.651791e-05, 'epoch': 125000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.651291e-05, 'epoch': 125500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.6507910000000004e-05, 'epoch': 126000.0}\n",
            "{'loss': 0.0004, 'learning_rate': 3.650291e-05, 'epoch': 126500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.649791e-05, 'epoch': 127000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6492910000000005e-05, 'epoch': 127500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.648791e-05, 'epoch': 128000.0}\n",
            "{'loss': 0.0047, 'learning_rate': 3.648291e-05, 'epoch': 128500.0}\n",
            "{'loss': 0.0049, 'learning_rate': 3.6477910000000006e-05, 'epoch': 129000.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.647291e-05, 'epoch': 129500.0}\n",
            "{'loss': 0.0035, 'learning_rate': 3.646791e-05, 'epoch': 130000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.646291e-05, 'epoch': 130500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.645791e-05, 'epoch': 131000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.645291e-05, 'epoch': 131500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.644791e-05, 'epoch': 132000.0}\n",
            "{'loss': 0.0026, 'learning_rate': 3.6442910000000004e-05, 'epoch': 132500.0}\n",
            "{'loss': 0.0039, 'learning_rate': 3.643791e-05, 'epoch': 133000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.643291e-05, 'epoch': 133500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6427910000000005e-05, 'epoch': 134000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.642291e-05, 'epoch': 134500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6417909999999996e-05, 'epoch': 135000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.641291e-05, 'epoch': 135500.0}\n",
            "{'loss': 0.0027, 'learning_rate': 3.640791e-05, 'epoch': 136000.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.6402910000000004e-05, 'epoch': 136500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.639791e-05, 'epoch': 137000.0}\n",
            "{'loss': 0.0075, 'learning_rate': 3.639291e-05, 'epoch': 137500.0}\n",
            "{'loss': 0.0047, 'learning_rate': 3.6387910000000005e-05, 'epoch': 138000.0}\n",
            "{'loss': 0.0093, 'learning_rate': 3.638291e-05, 'epoch': 138500.0}\n",
            "{'loss': 0.0042, 'learning_rate': 3.6377910000000004e-05, 'epoch': 139000.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.6372910000000006e-05, 'epoch': 139500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.636791e-05, 'epoch': 140000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.636291e-05, 'epoch': 140500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.635791e-05, 'epoch': 141000.0}\n",
            "{'loss': 0.006, 'learning_rate': 3.635291e-05, 'epoch': 141500.0}\n",
            "{'loss': 0.0222, 'learning_rate': 3.634791e-05, 'epoch': 142000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.634291e-05, 'epoch': 142500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.6337910000000004e-05, 'epoch': 143000.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.6332910000000007e-05, 'epoch': 143500.0}\n",
            "{'loss': 0.0061, 'learning_rate': 3.632791e-05, 'epoch': 144000.0}\n",
            "{'loss': 0.0169, 'learning_rate': 3.6322910000000005e-05, 'epoch': 144500.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.631791e-05, 'epoch': 145000.0}\n",
            "{'loss': 0.0052, 'learning_rate': 3.631291e-05, 'epoch': 145500.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.630791e-05, 'epoch': 146000.0}\n",
            "{'loss': 0.0009, 'learning_rate': 3.630291e-05, 'epoch': 146500.0}\n",
            "{'loss': 0.0044, 'learning_rate': 3.6297910000000004e-05, 'epoch': 147000.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.629291e-05, 'epoch': 147500.0}\n",
            "{'loss': 0.0036, 'learning_rate': 3.628791e-05, 'epoch': 148000.0}\n",
            "{'loss': 0.0032, 'learning_rate': 3.6282910000000005e-05, 'epoch': 148500.0}\n",
            "{'loss': 0.0023, 'learning_rate': 3.627791e-05, 'epoch': 149000.0}\n",
            "{'loss': 0.0053, 'learning_rate': 3.6272910000000004e-05, 'epoch': 149500.0}\n",
            "{'loss': 0.0059, 'learning_rate': 3.626791e-05, 'epoch': 150000.0}\n",
            "{'loss': 0.0185, 'learning_rate': 3.626291e-05, 'epoch': 150500.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.625791e-05, 'epoch': 151000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.625291e-05, 'epoch': 151500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.624791e-05, 'epoch': 152000.0}\n",
            "{'loss': 0.0073, 'learning_rate': 3.624291e-05, 'epoch': 152500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.623791e-05, 'epoch': 153000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6232910000000004e-05, 'epoch': 153500.0}\n",
            "{'loss': 0.0477, 'learning_rate': 3.622791000000001e-05, 'epoch': 154000.0}\n",
            "{'loss': 0.0315, 'learning_rate': 3.622291e-05, 'epoch': 154500.0}\n",
            "{'loss': 0.0103, 'learning_rate': 3.621791e-05, 'epoch': 155000.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.621291e-05, 'epoch': 155500.0}\n",
            "{'loss': 0.0013, 'learning_rate': 3.6207910000000003e-05, 'epoch': 156000.0}\n",
            "{'loss': 0.0029, 'learning_rate': 3.620291e-05, 'epoch': 156500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.619791e-05, 'epoch': 157000.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.6192910000000004e-05, 'epoch': 157500.0}\n",
            "{'loss': 0.0077, 'learning_rate': 3.618791e-05, 'epoch': 158000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.618291e-05, 'epoch': 158500.0}\n",
            "{'loss': 0.0004, 'learning_rate': 3.6177910000000005e-05, 'epoch': 159000.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.617291e-05, 'epoch': 159500.0}\n",
            "{'loss': 0.0074, 'learning_rate': 3.616791e-05, 'epoch': 160000.0}\n",
            "{'loss': 0.0015, 'learning_rate': 3.616291e-05, 'epoch': 160500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.615791e-05, 'epoch': 161000.0}\n",
            "{'loss': 0.0084, 'learning_rate': 3.615291e-05, 'epoch': 161500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.614791e-05, 'epoch': 162000.0}\n",
            "{'loss': 0.0025, 'learning_rate': 3.614291e-05, 'epoch': 162500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.6137910000000006e-05, 'epoch': 163000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.613291e-05, 'epoch': 163500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6127910000000004e-05, 'epoch': 164000.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.612291000000001e-05, 'epoch': 164500.0}\n",
            "{'loss': 0.0091, 'learning_rate': 3.611791e-05, 'epoch': 165000.0}\n",
            "{'loss': 0.0254, 'learning_rate': 3.611291e-05, 'epoch': 165500.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.610791e-05, 'epoch': 166000.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.6102910000000004e-05, 'epoch': 166500.0}\n",
            "{'loss': 0.0112, 'learning_rate': 3.609791e-05, 'epoch': 167000.0}\n",
            "{'loss': 0.0071, 'learning_rate': 3.609291e-05, 'epoch': 167500.0}\n",
            "{'loss': 0.0057, 'learning_rate': 3.6087910000000005e-05, 'epoch': 168000.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.608291e-05, 'epoch': 168500.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.607791e-05, 'epoch': 169000.0}\n",
            "{'loss': 0.008, 'learning_rate': 3.6072910000000006e-05, 'epoch': 169500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.606791e-05, 'epoch': 170000.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.606291e-05, 'epoch': 170500.0}\n",
            "{'loss': 0.0005, 'learning_rate': 3.605791e-05, 'epoch': 171000.0}\n",
            "{'loss': 0.0047, 'learning_rate': 3.605291e-05, 'epoch': 171500.0}\n",
            "{'loss': 0.006, 'learning_rate': 3.604791e-05, 'epoch': 172000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.604291e-05, 'epoch': 172500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.6037910000000003e-05, 'epoch': 173000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6032910000000006e-05, 'epoch': 173500.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.602791e-05, 'epoch': 174000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.6022910000000004e-05, 'epoch': 174500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.601791e-05, 'epoch': 175000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.601291e-05, 'epoch': 175500.0}\n",
            "{'loss': 0.0462, 'learning_rate': 3.600791e-05, 'epoch': 176000.0}\n",
            "{'loss': 0.0013, 'learning_rate': 3.600291e-05, 'epoch': 176500.0}\n",
            "{'loss': 0.0003, 'learning_rate': 3.5997910000000004e-05, 'epoch': 177000.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.599291e-05, 'epoch': 177500.0}\n",
            "{'loss': 0.0042, 'learning_rate': 3.598791e-05, 'epoch': 178000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5982910000000005e-05, 'epoch': 178500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.597791e-05, 'epoch': 179000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.597291e-05, 'epoch': 179500.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.596791e-05, 'epoch': 180000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.596291e-05, 'epoch': 180500.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.595791e-05, 'epoch': 181000.0}\n",
            "{'loss': 0.0378, 'learning_rate': 3.595291e-05, 'epoch': 181500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.594791e-05, 'epoch': 182000.0}\n",
            "{'loss': 0.0015, 'learning_rate': 3.5942910000000005e-05, 'epoch': 182500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.593791e-05, 'epoch': 183000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5932910000000004e-05, 'epoch': 183500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5927910000000006e-05, 'epoch': 184000.0}\n",
            "{'loss': 0.0034, 'learning_rate': 3.592291e-05, 'epoch': 184500.0}\n",
            "{'loss': 0.0036, 'learning_rate': 3.591791e-05, 'epoch': 185000.0}\n",
            "{'loss': 0.0199, 'learning_rate': 3.591291e-05, 'epoch': 185500.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.590791e-05, 'epoch': 186000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.590291e-05, 'epoch': 186500.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.589791e-05, 'epoch': 187000.0}\n",
            "{'loss': 0.0037, 'learning_rate': 3.5892910000000004e-05, 'epoch': 187500.0}\n",
            "{'loss': 0.0031, 'learning_rate': 3.588791e-05, 'epoch': 188000.0}\n",
            "{'loss': 0.0089, 'learning_rate': 3.588291e-05, 'epoch': 188500.0}\n",
            "{'loss': 0.0117, 'learning_rate': 3.5877910000000005e-05, 'epoch': 189000.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.587291e-05, 'epoch': 189500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5867909999999997e-05, 'epoch': 190000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.586291e-05, 'epoch': 190500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.585791e-05, 'epoch': 191000.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.585291e-05, 'epoch': 191500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.584791e-05, 'epoch': 192000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.584291e-05, 'epoch': 192500.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.5837910000000005e-05, 'epoch': 193000.0}\n",
            "{'loss': 0.0031, 'learning_rate': 3.583291e-05, 'epoch': 193500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5827910000000004e-05, 'epoch': 194000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5822910000000006e-05, 'epoch': 194500.0}\n",
            "{'loss': 0.0024, 'learning_rate': 3.581791e-05, 'epoch': 195000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.581291e-05, 'epoch': 195500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.580791e-05, 'epoch': 196000.0}\n",
            "{'loss': 0.0118, 'learning_rate': 3.580291e-05, 'epoch': 196500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.579791e-05, 'epoch': 197000.0}\n",
            "{'loss': 0.0009, 'learning_rate': 3.579291e-05, 'epoch': 197500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.5787910000000004e-05, 'epoch': 198000.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.578291e-05, 'epoch': 198500.0}\n",
            "{'loss': 0.0098, 'learning_rate': 3.577791e-05, 'epoch': 199000.0}\n",
            "{'loss': 0.0053, 'learning_rate': 3.5772910000000005e-05, 'epoch': 199500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.576791000000001e-05, 'epoch': 200000.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.576291e-05, 'epoch': 200500.0}\n",
            "{'loss': 0.005, 'learning_rate': 3.575791e-05, 'epoch': 201000.0}\n",
            "{'loss': 0.0074, 'learning_rate': 3.575291e-05, 'epoch': 201500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5747910000000004e-05, 'epoch': 202000.0}\n",
            "{'loss': 0.0071, 'learning_rate': 3.574291e-05, 'epoch': 202500.0}\n",
            "{'loss': 0.0178, 'learning_rate': 3.573791e-05, 'epoch': 203000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.5732910000000005e-05, 'epoch': 203500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.572791e-05, 'epoch': 204000.0}\n",
            "{'loss': 0.0032, 'learning_rate': 3.5722910000000004e-05, 'epoch': 204500.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.5717910000000006e-05, 'epoch': 205000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.571291e-05, 'epoch': 205500.0}\n",
            "{'loss': 0.004, 'learning_rate': 3.570791e-05, 'epoch': 206000.0}\n",
            "{'loss': 0.0174, 'learning_rate': 3.570291e-05, 'epoch': 206500.0}\n",
            "{'loss': 0.0117, 'learning_rate': 3.569791e-05, 'epoch': 207000.0}\n",
            "{'loss': 0.0004, 'learning_rate': 3.569291e-05, 'epoch': 207500.0}\n",
            "{'loss': 0.0007, 'learning_rate': 3.568791e-05, 'epoch': 208000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5682910000000004e-05, 'epoch': 208500.0}\n",
            "{'loss': 0.0065, 'learning_rate': 3.567791e-05, 'epoch': 209000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.567291e-05, 'epoch': 209500.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.5667910000000005e-05, 'epoch': 210000.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.566291e-05, 'epoch': 210500.0}\n",
            "{'loss': 0.0006, 'learning_rate': 3.565791e-05, 'epoch': 211000.0}\n",
            "{'loss': 0.0325, 'learning_rate': 3.565291e-05, 'epoch': 211500.0}\n",
            "{'loss': 0.0054, 'learning_rate': 3.564791e-05, 'epoch': 212000.0}\n",
            "{'loss': 0.0028, 'learning_rate': 3.5642910000000005e-05, 'epoch': 212500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.563791e-05, 'epoch': 213000.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.563291e-05, 'epoch': 213500.0}\n",
            "{'loss': 0.0022, 'learning_rate': 3.5627910000000006e-05, 'epoch': 214000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.562291e-05, 'epoch': 214500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5617910000000004e-05, 'epoch': 215000.0}\n",
            "{'loss': 0.0033, 'learning_rate': 3.561291e-05, 'epoch': 215500.0}\n",
            "{'loss': 0.0316, 'learning_rate': 3.560791e-05, 'epoch': 216000.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.560291e-05, 'epoch': 216500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.559791e-05, 'epoch': 217000.0}\n",
            "{'loss': 0.0055, 'learning_rate': 3.5592910000000003e-05, 'epoch': 217500.0}\n",
            "{'loss': 0.0054, 'learning_rate': 3.558791e-05, 'epoch': 218000.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.558291e-05, 'epoch': 218500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5577910000000004e-05, 'epoch': 219000.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.557291000000001e-05, 'epoch': 219500.0}\n",
            "{'loss': 0.0064, 'learning_rate': 3.556791e-05, 'epoch': 220000.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.556291e-05, 'epoch': 220500.0}\n",
            "{'loss': 0.0069, 'learning_rate': 3.555791e-05, 'epoch': 221000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.555291e-05, 'epoch': 221500.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.554791e-05, 'epoch': 222000.0}\n",
            "{'loss': 0.0115, 'learning_rate': 3.554291e-05, 'epoch': 222500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5537910000000005e-05, 'epoch': 223000.0}\n",
            "{'loss': 0.0019, 'learning_rate': 3.553291e-05, 'epoch': 223500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.552791e-05, 'epoch': 224000.0}\n",
            "{'loss': 0.0168, 'learning_rate': 3.5522910000000006e-05, 'epoch': 224500.0}\n",
            "{'loss': 0.0011, 'learning_rate': 3.551791e-05, 'epoch': 225000.0}\n",
            "{'loss': 0.0038, 'learning_rate': 3.551291e-05, 'epoch': 225500.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.550791e-05, 'epoch': 226000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.550291e-05, 'epoch': 226500.0}\n",
            "{'loss': 0.0008, 'learning_rate': 3.549791e-05, 'epoch': 227000.0}\n",
            "{'loss': 0.0056, 'learning_rate': 3.549291e-05, 'epoch': 227500.0}\n",
            "{'loss': 0.0053, 'learning_rate': 3.5487910000000004e-05, 'epoch': 228000.0}\n",
            "{'loss': 0.0031, 'learning_rate': 3.548291e-05, 'epoch': 228500.0}\n",
            "{'loss': 0.0021, 'learning_rate': 3.547791e-05, 'epoch': 229000.0}\n",
            "{'loss': 0.0012, 'learning_rate': 3.5472910000000005e-05, 'epoch': 229500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.546791000000001e-05, 'epoch': 230000.0}\n",
            "{'loss': 0.0016, 'learning_rate': 3.5462909999999996e-05, 'epoch': 230500.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.545791e-05, 'epoch': 231000.0}\n",
            "{'loss': 0.0063, 'learning_rate': 3.545291e-05, 'epoch': 231500.0}\n",
            "{'loss': 0.0196, 'learning_rate': 3.5447910000000004e-05, 'epoch': 232000.0}\n",
            "{'loss': 0.001, 'learning_rate': 3.544291e-05, 'epoch': 232500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.543791e-05, 'epoch': 233000.0}\n",
            "{'loss': 0.0045, 'learning_rate': 3.5432910000000005e-05, 'epoch': 233500.0}\n",
            "{'loss': 0.0078, 'learning_rate': 3.542791e-05, 'epoch': 234000.0}\n",
            "{'loss': 0.0018, 'learning_rate': 3.542291e-05, 'epoch': 234500.0}\n",
            "{'loss': 0.0091, 'learning_rate': 3.5417910000000006e-05, 'epoch': 235000.0}\n",
            "{'loss': 0.0089, 'learning_rate': 3.541291e-05, 'epoch': 235500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.540791e-05, 'epoch': 236000.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.540291e-05, 'epoch': 236500.0}\n",
            "{'loss': 0.0007, 'learning_rate': 3.539791e-05, 'epoch': 237000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.539291e-05, 'epoch': 237500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.538791e-05, 'epoch': 238000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5382910000000004e-05, 'epoch': 238500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5377910000000006e-05, 'epoch': 239000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.537291e-05, 'epoch': 239500.0}\n",
            "{'loss': 0.0476, 'learning_rate': 3.5367910000000005e-05, 'epoch': 240000.0}\n",
            "{'loss': 0.0113, 'learning_rate': 3.536291e-05, 'epoch': 240500.0}\n",
            "{'loss': 0.003, 'learning_rate': 3.5357909999999996e-05, 'epoch': 241000.0}\n",
            "{'loss': 0.0008, 'learning_rate': 3.535291e-05, 'epoch': 241500.0}\n",
            "{'loss': 0.0014, 'learning_rate': 3.534791e-05, 'epoch': 242000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5342910000000004e-05, 'epoch': 242500.0}\n",
            "{'loss': 0.0071, 'learning_rate': 3.533791e-05, 'epoch': 243000.0}\n",
            "{'loss': 0.0001, 'learning_rate': 3.533291e-05, 'epoch': 243500.0}\n",
            "{'loss': 0.0007, 'learning_rate': 3.5327910000000005e-05, 'epoch': 244000.0}\n",
            "{'loss': 0.007, 'learning_rate': 3.532291e-05, 'epoch': 244500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5317910000000004e-05, 'epoch': 245000.0}\n",
            "{'loss': 0.002, 'learning_rate': 3.531291e-05, 'epoch': 245500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.530791e-05, 'epoch': 246000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.530291e-05, 'epoch': 246500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.529791e-05, 'epoch': 247000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.529291e-05, 'epoch': 247500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.528791e-05, 'epoch': 248000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.528291e-05, 'epoch': 248500.0}\n",
            "{'loss': 0.0041, 'learning_rate': 3.5277910000000004e-05, 'epoch': 249000.0}\n",
            "{'loss': 0.0175, 'learning_rate': 3.5272910000000006e-05, 'epoch': 249500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.526791e-05, 'epoch': 250000.0}\n",
            "{'loss': 0.0017, 'learning_rate': 3.526291e-05, 'epoch': 250500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.525791e-05, 'epoch': 251000.0}\n",
            "{'loss': 0.0076, 'learning_rate': 3.525291e-05, 'epoch': 251500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.524791e-05, 'epoch': 252000.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.524291e-05, 'epoch': 252500.0}\n",
            "{'loss': 0.0, 'learning_rate': 3.5237910000000004e-05, 'epoch': 253000.0}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./models/mlb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2006\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2004\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 2006\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2008\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2291\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m   2292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2348\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2346\u001b[0m run_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_dir(trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m   2347\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> 2348\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_model(output_dir, _internal_call\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   2349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m   2350\u001b[0m     \u001b[39m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[0;32m   2351\u001b[0m     \u001b[39m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2830\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   2827\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   2829\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2830\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[0;32m   2832\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   2833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   2884\u001b[0m             torch\u001b[39m.\u001b[39msave(state_dict, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[0;32m   2885\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2886\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_pretrained(\n\u001b[0;32m   2887\u001b[0m         output_dir, state_dict\u001b[39m=\u001b[39;49mstate_dict, safe_serialization\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msave_safetensors\n\u001b[0;32m   2888\u001b[0m     )\n\u001b[0;32m   2890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2891\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py:1843\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m   1842\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file))\n\u001b[0;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     path_to_weights \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    667\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[1;32m--> 668\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(name, storage\u001b[39m.\u001b[39mdata_ptr(), num_bytes)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint=\"./models/mlb/checkpoint-252000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./models/mlb/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./models/mlb/checkpoint-252000\",\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=20,\n",
        ")\n",
        "\n",
        "fill_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"./models/mlb/checkpoint-252000\",\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###instruction:what is the outcome of pitcher 282332 pitching to batter 340192 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 502085 pitching to batter 136770 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 501955 pitching to batter 134181 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 450306 pitching to batter 400083 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 400141 pitching to batter 449168 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 445156 pitching to batter 434661 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 279824 pitching to batter 425783 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 451596 pitching to batter 456655 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 434622 pitching to batter 460075 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 434622 pitching to batter 113744 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 434378 pitching to batter 460051 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 425844 pitching to batter 276346 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 425844 pitching to batter 408042 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 434565 pitching to batter 113028 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 407297 pitching to batter 276346 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 407878 pitching to batter 460051 ###response a game 263816 output output verses NYY NYY the a a the 1 inning with 0 0 and 0 balls ### 0 outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 116539\n",
            "###instruction:what is the outcome of pitcher 435043 pitching to batter 425766 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 435043 pitching to batter 455088 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 218894 pitching to batter 434540 ###response a game 263816, 116539 116539 NYY NYY the instruction of outs 1 inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n",
            "###instruction:what is the outcome of pitcher 457425 pitching to batter 461314 ###response a game 263816, 116539 116539 NYY NYY the instruction instruction a a inning with with strike and 0 0 and a outs outs 277417,, a - handed pitcher, pitch throws 1 to handed 116539 and\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# prompt = \"instruction: what is the outcome of pitcher 460024 pitching to batter 116338 \"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sample:\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mprint\u001b[39m(fill_text(s, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m1.5\u001b[39;49m, no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:209\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    169\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         )\n\u001b[0;32m   1107\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:251\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m prompt_text \u001b[39m=\u001b[39m model_inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mprompt_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    252\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py:1437\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1432\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1433\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1434\u001b[0m         )\n\u001b[0;32m   1436\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1438\u001b[0m         input_ids,\n\u001b[0;32m   1439\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1440\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1441\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1442\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1443\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1444\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1445\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1446\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1447\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1448\u001b[0m     )\n\u001b[0;32m   1450\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1451\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py:2248\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2245\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2247\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2248\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2249\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2250\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2251\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2252\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2253\u001b[0m )\n\u001b[0;32m   2255\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2256\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:975\u001b[0m, in \u001b[0;36mRobertaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    973\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m    976\u001b[0m     input_ids,\n\u001b[0;32m    977\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    978\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    979\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    980\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    981\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    982\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    983\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    984\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    985\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    986\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    987\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    988\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    989\u001b[0m )\n\u001b[0;32m    991\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    992\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    450\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    451\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 453\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    455\u001b[0m )\n\u001b[0;32m    456\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    458\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 465\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    466\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:364\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    363\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 364\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[0;32m    365\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# prompt = \"instruction: what is the outcome of pitcher 460024 pitching to batter 116338 \"\n",
        "for s in sample:\n",
        "    print(fill_text(s, max_new_tokens=50, top_k=22, temperature=1.5)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from operator import itemgetter\n",
        "from collections import OrderedDict\n",
        "import collections\n",
        "reg_picks = {}\n",
        "powerball = {}\n",
        "sample_cout = 15000\n",
        "count = 0\n",
        "predictions = []\n",
        "for i in range(0, sample_cout):\n",
        "    prompt = \"20 22 26 28 63 05 ::\"\n",
        "    prediction = fill_text(prompt, max_new_tokens=6, top_k=50, do_sample=True, temperature=1.0, no_repeat_ngram_size=1)[0][\"generated_text\"]\n",
        "    prediction = prediction.replace(prompt, \"\")\n",
        "    prediction = prediction.strip()\n",
        "    prediction = prediction.replace(\":\", \"\")\n",
        "    _prediction = prediction.strip()\n",
        "    prediction = _prediction.split(\" \")\n",
        "    if int(prediction[-1]) <= 29:\n",
        "        predictions.append(_prediction)\n",
        "        count += 1\n",
        "        if prediction[-1] not in powerball:\n",
        "            powerball[prediction[-1]] = 0\n",
        "        powerball[prediction[-1]] += 1\n",
        "        for p in prediction[:-1]:\n",
        "            if p not in reg_picks:\n",
        "                reg_picks[p] = 1\n",
        "            else:\n",
        "                reg_picks[p] += 1\n",
        "elements_count = collections.Counter(predictions)\n",
        "sortedDict = sorted(elements_count.items(), key=lambda x:x[1])\n",
        "for key in sortedDict:\n",
        "   print(f\"{key}\")\n",
        "sorted_x = OrderedDict(sorted(powerball.items(), key=itemgetter(1)))\n",
        "print(len(sorted_x))\n",
        "for pick in sorted_x:\n",
        "    print(f\"pick: {pick} % {sorted_x[pick]/count*100}\")\n",
        "sorted_x = OrderedDict(sorted(reg_picks.items(), key=itemgetter(1)))\n",
        "print(len(sorted_x))\n",
        "for pick in sorted_x:\n",
        "    print(f\"pick: {pick} {sorted_x[pick]/(count*5)*100} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': '###instruction: what is the outcome of pitcher 460024 pitching to batter 116338 ### input: bottom of the 9 inning with 2 on and no outs ### output : strike and 0 balls and 0 outs, 277417, a right - handed pitcher, pitches pitch number 1 to batter 116539, a right - handed batter 116539 hits with with with with with with with with with with with with with with with with with with with with with with with with with with with with with 277417 throws a 94 miles per hour Sinker and 116539, a right - handed batter, makes contact with a pitch from 277417 resulting in a field out ### instruction :'}]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fill_text(\"###instruction: what is the outcome of pitcher 460024 pitching to batter 116338 ### input: bottom of the 9 inning with 2 on and no outs ### output :\", max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sortedDict = sorted(elements_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fill_mask(\"03 09 21 24 29 14 : 13 20 <mask> 33 59 20\") #03 09 21 24 29 14 : 13 20 31 33 59 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fill_mask(\"03 09 21 24 29 14 : 13 20 31 33 59 <mask>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07/15/2023\n",
            "won so far 4 cost 6 profit/loss -2\n",
            "won so far 8 cost 18 profit/loss -10\n",
            "won so far 12 cost 34 profit/loss -22\n",
            "won so far 16 cost 188 profit/loss -172\n",
            "won so far 20 cost 206 profit/loss -186\n",
            "won so far 24 cost 234 profit/loss -210\n",
            "won so far 28 cost 294 profit/loss -266\n",
            "won so far 32 cost 412 profit/loss -380\n",
            "won so far 36 cost 484 profit/loss -448\n",
            "won so far 40 cost 518 profit/loss -478\n",
            "won so far 44 cost 756 profit/loss -712\n",
            "won so far 48 cost 878 profit/loss -830\n",
            "won so far 52 cost 932 profit/loss -880\n",
            "won so far 56 cost 1014 profit/loss -958\n",
            "won so far 60 cost 1022 profit/loss -962\n",
            "won so far 64 cost 1054 profit/loss -990\n",
            "won so far 68 cost 1076 profit/loss -1008\n",
            "won so far 72 cost 1078 profit/loss -1006\n",
            "won so far 76 cost 1084 profit/loss -1008\n",
            "won so far 80 cost 1168 profit/loss -1088\n",
            "won so far 87 cost 1176 profit/loss -1089\n",
            "won so far 91 cost 1196 profit/loss -1105\n",
            "won so far 95 cost 1246 profit/loss -1151\n",
            "won so far 99 cost 1272 profit/loss -1173\n",
            "won so far 103 cost 1376 profit/loss -1273\n",
            "won so far 107 cost 1404 profit/loss -1297\n",
            "won so far 111 cost 1468 profit/loss -1357\n",
            "won so far 115 cost 1522 profit/loss -1407\n",
            "won so far 119 cost 1538 profit/loss -1419\n",
            "won so far 123 cost 1560 profit/loss -1437\n",
            "won so far 130 cost 1680 profit/loss -1550\n",
            "won so far 134 cost 1736 profit/loss -1602\n",
            "won so far 138 cost 1802 profit/loss -1664\n",
            "won so far 142 cost 1832 profit/loss -1690\n",
            "won so far 146 cost 1868 profit/loss -1722\n",
            "won so far 150 cost 1890 profit/loss -1740\n",
            "won so far 154 cost 2170 profit/loss -2016\n",
            "won so far 158 cost 2298 profit/loss -2140\n",
            "won so far 162 cost 2346 profit/loss -2184\n",
            "won so far 166 cost 2422 profit/loss -2256\n",
            "won so far 170 cost 2436 profit/loss -2266\n",
            "won so far 174 cost 2508 profit/loss -2334\n",
            "won so far 178 cost 2542 profit/loss -2364\n",
            "won so far 182 cost 2564 profit/loss -2382\n",
            "won so far 186 cost 2664 profit/loss -2478\n",
            "won so far 190 cost 2722 profit/loss -2532\n",
            "won so far 194 cost 2846 profit/loss -2652\n",
            "won so far 198 cost 2972 profit/loss -2774\n",
            "won so far 202 cost 2978 profit/loss -2776\n",
            "won so far 206 cost 2994 profit/loss -2788\n",
            "won so far 210 cost 3008 profit/loss -2798\n",
            "won so far 214 cost 3044 profit/loss -2830\n",
            "won so far 218 cost 3162 profit/loss -2944\n",
            "won so far 222 cost 3282 profit/loss -3060\n",
            "won so far 226 cost 3310 profit/loss -3084\n",
            "won so far 230 cost 3410 profit/loss -3180\n",
            "won so far 234 cost 3426 profit/loss -3192\n",
            "won so far 238 cost 3474 profit/loss -3236\n",
            "won so far 242 cost 3524 profit/loss -3282\n",
            "won so far 246 cost 3526 profit/loss -3280\n",
            "won so far 250 cost 3634 profit/loss -3384\n",
            "won so far 254 cost 3678 profit/loss -3424\n",
            "won so far 258 cost 3714 profit/loss -3456\n",
            "won so far 262 cost 3752 profit/loss -3490\n",
            "won so far 266 cost 3948 profit/loss -3682\n",
            "won so far 270 cost 4012 profit/loss -3742\n",
            "won so far 274 cost 4032 profit/loss -3758\n",
            "won so far 278 cost 4066 profit/loss -3788\n",
            "won so far 282 cost 4076 profit/loss -3794\n",
            "won so far 286 cost 4118 profit/loss -3832\n",
            "won so far 290 cost 4156 profit/loss -3866\n",
            "won so far 297 cost 4296 profit/loss -3999\n",
            "won so far 301 cost 4328 profit/loss -4027\n",
            "won so far 305 cost 4512 profit/loss -4207\n",
            "won so far 312 cost 4534 profit/loss -4222\n",
            "won so far 316 cost 4568 profit/loss -4252\n",
            "won so far 320 cost 4622 profit/loss -4302\n",
            "won so far 324 cost 4644 profit/loss -4320\n",
            "won so far 328 cost 4672 profit/loss -4344\n",
            "won so far 332 cost 4696 profit/loss -4364\n",
            "won so far 336 cost 4780 profit/loss -4444\n",
            "won so far 340 cost 4828 profit/loss -4488\n",
            "won so far 344 cost 4966 profit/loss -4622\n",
            "won so far 348 cost 5016 profit/loss -4668\n",
            "won so far 352 cost 5080 profit/loss -4728\n",
            "won so far 356 cost 5142 profit/loss -4786\n",
            "won so far 360 cost 5262 profit/loss -4902\n",
            "won so far 364 cost 5378 profit/loss -5014\n",
            "won so far 368 cost 5396 profit/loss -5028\n",
            "won so far 372 cost 5554 profit/loss -5182\n",
            "won so far 376 cost 5562 profit/loss -5186\n",
            "won so far 380 cost 5586 profit/loss -5206\n",
            "won so far 384 cost 5624 profit/loss -5240\n",
            "won so far 388 cost 5688 profit/loss -5300\n",
            "won so far 392 cost 5696 profit/loss -5304\n",
            "won so far 396 cost 5744 profit/loss -5348\n",
            "won so far 400 cost 5884 profit/loss -5484\n",
            "won so far 404 cost 5920 profit/loss -5516\n",
            "won so far 408 cost 5954 profit/loss -5546\n",
            "won so far 412 cost 6048 profit/loss -5636\n",
            "won so far 416 cost 6060 profit/loss -5644\n",
            "won so far 420 cost 6136 profit/loss -5716\n",
            "won so far 424 cost 6310 profit/loss -5886\n",
            "won so far 428 cost 6314 profit/loss -5886\n",
            "won so far 432 cost 6392 profit/loss -5960\n",
            "won so far 436 cost 6430 profit/loss -5994\n",
            "won so far 440 cost 6496 profit/loss -6056\n",
            "won so far 444 cost 6626 profit/loss -6182\n",
            "won so far 448 cost 6698 profit/loss -6250\n",
            "won so far 452 cost 6704 profit/loss -6252\n",
            "won so far 456 cost 6716 profit/loss -6260\n",
            "won so far 460 cost 6752 profit/loss -6292\n",
            "won so far 464 cost 6776 profit/loss -6312\n",
            "won so far 471 cost 6782 profit/loss -6311\n",
            "won so far 475 cost 6846 profit/loss -6371\n",
            "won so far 479 cost 7006 profit/loss -6527\n",
            "won so far 483 cost 7208 profit/loss -6725\n",
            "won so far 487 cost 7392 profit/loss -6905\n",
            "won so far 491 cost 7444 profit/loss -6953\n",
            "won so far 495 cost 7474 profit/loss -6979\n",
            "won so far 499 cost 7492 profit/loss -6993\n",
            "won so far 503 cost 7512 profit/loss -7009\n",
            "won so far 507 cost 7552 profit/loss -7045\n",
            "won so far 511 cost 7590 profit/loss -7079\n",
            "won so far 515 cost 7602 profit/loss -7087\n",
            "won so far 519 cost 7806 profit/loss -7287\n",
            "won so far 523 cost 7840 profit/loss -7317\n",
            "won so far 527 cost 8022 profit/loss -7495\n",
            "won so far 531 cost 8080 profit/loss -7549\n",
            "won so far 535 cost 8152 profit/loss -7617\n",
            "won so far 539 cost 8156 profit/loss -7617\n",
            "won so far 543 cost 8262 profit/loss -7719\n",
            "won so far 547 cost 8334 profit/loss -7787\n",
            "won so far 551 cost 8338 profit/loss -7787\n",
            "won so far 555 cost 8388 profit/loss -7833\n",
            "won so far 559 cost 8408 profit/loss -7849\n",
            "won so far 563 cost 8446 profit/loss -7883\n",
            "won so far 567 cost 8460 profit/loss -7893\n",
            "won so far 571 cost 8504 profit/loss -7933\n",
            "won so far 575 cost 8552 profit/loss -7977\n",
            "won so far 579 cost 8582 profit/loss -8003\n",
            "won so far 583 cost 8782 profit/loss -8199\n",
            "won so far 587 cost 8826 profit/loss -8239\n",
            "won so far 591 cost 8882 profit/loss -8291\n",
            "won so far 595 cost 8890 profit/loss -8295\n",
            "won so far 599 cost 8946 profit/loss -8347\n",
            "won so far 603 cost 9212 profit/loss -8609\n",
            "won so far 607 cost 9222 profit/loss -8615\n",
            "won so far 611 cost 9350 profit/loss -8739\n",
            "won so far 615 cost 9388 profit/loss -8773\n",
            "won so far 619 cost 9398 profit/loss -8779\n",
            "won so far 623 cost 9402 profit/loss -8779\n",
            "won so far 627 cost 9432 profit/loss -8805\n",
            "won so far 631 cost 9472 profit/loss -8841\n",
            "won so far 635 cost 9692 profit/loss -9057\n",
            "won so far 639 cost 9778 profit/loss -9139\n",
            "won so far 643 cost 9906 profit/loss -9263\n",
            "won so far 647 cost 9916 profit/loss -9269\n",
            "won so far 651 cost 9968 profit/loss -9317\n",
            "won so far 655 cost 10172 profit/loss -9517\n",
            "won so far 659 cost 10176 profit/loss -9517\n",
            "won so far 666 cost 10202 profit/loss -9536\n",
            "won so far 673 cost 10204 profit/loss -9531\n",
            "won so far 677 cost 10268 profit/loss -9591\n",
            "won so far 681 cost 10352 profit/loss -9671\n",
            "won so far 685 cost 10394 profit/loss -9709\n",
            "won so far 689 cost 10480 profit/loss -9791\n",
            "won so far 693 cost 10620 profit/loss -9927\n",
            "won so far 697 cost 10648 profit/loss -9951\n",
            "won so far 701 cost 10678 profit/loss -9977\n",
            "won so far 705 cost 10706 profit/loss -10001\n",
            "won so far 709 cost 10788 profit/loss -10079\n",
            "won so far 713 cost 10798 profit/loss -10085\n",
            "won so far 717 cost 10836 profit/loss -10119\n",
            "won so far 721 cost 10860 profit/loss -10139\n",
            "won so far 725 cost 10884 profit/loss -10159\n",
            "won so far 729 cost 10898 profit/loss -10169\n",
            "won so far 733 cost 11018 profit/loss -10285\n",
            "won so far 737 cost 11148 profit/loss -10411\n",
            "won so far 741 cost 11154 profit/loss -10413\n",
            "won so far 745 cost 11284 profit/loss -10539\n",
            "won so far 749 cost 11322 profit/loss -10573\n",
            "won so far 753 cost 11506 profit/loss -10753\n",
            "won so far 757 cost 11594 profit/loss -10837\n",
            "won so far 764 cost 11604 profit/loss -10840\n",
            "won so far 768 cost 11728 profit/loss -10960\n",
            "won so far 772 cost 11816 profit/loss -11044\n",
            "won so far 776 cost 11832 profit/loss -11056\n",
            "won so far 780 cost 11864 profit/loss -11084\n",
            "won so far 784 cost 11942 profit/loss -11158\n",
            "won so far 788 cost 11962 profit/loss -11174\n",
            "won so far 792 cost 11984 profit/loss -11192\n",
            "won so far 796 cost 12124 profit/loss -11328\n",
            "won so far 800 cost 12196 profit/loss -11396\n",
            "won so far 804 cost 12226 profit/loss -11422\n",
            "won so far 808 cost 12270 profit/loss -11462\n",
            "won so far 812 cost 12304 profit/loss -11492\n",
            "won so far 816 cost 12432 profit/loss -11616\n",
            "won so far 823 cost 12454 profit/loss -11631\n",
            "won so far 827 cost 12522 profit/loss -11695\n",
            "won so far 831 cost 12530 profit/loss -11699\n",
            "won so far 835 cost 12578 profit/loss -11743\n",
            "won so far 839 cost 12590 profit/loss -11751\n",
            "won so far 843 cost 12644 profit/loss -11801\n",
            "won so far 847 cost 12656 profit/loss -11809\n",
            "won so far 851 cost 12662 profit/loss -11811\n",
            "won so far 855 cost 12678 profit/loss -11823\n",
            "won so far 859 cost 13062 profit/loss -12203\n",
            "won so far 863 cost 13190 profit/loss -12327\n",
            "won so far 870 cost 13236 profit/loss -12366\n",
            "won so far 874 cost 13256 profit/loss -12382\n",
            "won so far 878 cost 13288 profit/loss -12410\n",
            "won so far 882 cost 13310 profit/loss -12428\n",
            "won so far 886 cost 13328 profit/loss -12442\n",
            "won so far 890 cost 13338 profit/loss -12448\n",
            "won so far 894 cost 13360 profit/loss -12466\n",
            "won so far 898 cost 13376 profit/loss -12478\n",
            "won so far 902 cost 13460 profit/loss -12558\n",
            "won so far 906 cost 13476 profit/loss -12570\n",
            "won so far 913 cost 13484 profit/loss -12571\n",
            "won so far 917 cost 13606 profit/loss -12689\n",
            "won so far 921 cost 13660 profit/loss -12739\n",
            "won so far 928 cost 13762 profit/loss -12834\n",
            "won so far 932 cost 13766 profit/loss -12834\n",
            "won so far 936 cost 13784 profit/loss -12848\n",
            "won so far 940 cost 13880 profit/loss -12940\n",
            "won so far 944 cost 13996 profit/loss -13052\n",
            "won so far 948 cost 14036 profit/loss -13088\n",
            "won so far 952 cost 14038 profit/loss -13086\n",
            "won so far 956 cost 14138 profit/loss -13182\n",
            "won so far 960 cost 14152 profit/loss -13192\n",
            "won so far 964 cost 14188 profit/loss -13224\n",
            "won so far 968 cost 14212 profit/loss -13244\n",
            "won so far 972 cost 14250 profit/loss -13278\n",
            "won so far 976 cost 14254 profit/loss -13278\n",
            "won so far 980 cost 14416 profit/loss -13436\n",
            "won so far 984 cost 14440 profit/loss -13456\n",
            "won so far 988 cost 14494 profit/loss -13506\n",
            "won so far 992 cost 14538 profit/loss -13546\n",
            "won so far 996 cost 14590 profit/loss -13594\n",
            "won so far 1000 cost 14628 profit/loss -13628\n",
            "won so far 1004 cost 14638 profit/loss -13634\n",
            "won so far 1008 cost 14750 profit/loss -13742\n",
            "won so far 1012 cost 14868 profit/loss -13856\n",
            "won so far 1019 cost 14918 profit/loss -13899\n",
            "won so far 1023 cost 15140 profit/loss -14117\n",
            "won so far 1027 cost 15194 profit/loss -14167\n",
            "won so far 1031 cost 15254 profit/loss -14223\n",
            "won so far 1035 cost 15282 profit/loss -14247\n",
            "won so far 1039 cost 15370 profit/loss -14331\n",
            "won so far 1043 cost 15474 profit/loss -14431\n",
            "won so far 1050 cost 15522 profit/loss -14472\n",
            "won so far 1054 cost 15542 profit/loss -14488\n",
            "won so far 1058 cost 15554 profit/loss -14496\n",
            "won so far 1062 cost 15620 profit/loss -14558\n",
            "won so far 1066 cost 15658 profit/loss -14592\n",
            "won so far 1070 cost 15726 profit/loss -14656\n",
            "won so far 1074 cost 15892 profit/loss -14818\n",
            "won so far 1078 cost 15986 profit/loss -14908\n",
            "won so far 1082 cost 16018 profit/loss -14936\n",
            "won so far 1086 cost 16032 profit/loss -14946\n",
            "won so far 1090 cost 16092 profit/loss -15002\n",
            "won so far 1094 cost 16110 profit/loss -15016\n",
            "won so far 1098 cost 16200 profit/loss -15102\n",
            "won so far 1102 cost 16262 profit/loss -15160\n",
            "won so far 1106 cost 16342 profit/loss -15236\n",
            "won so far 1110 cost 16446 profit/loss -15336\n",
            "won so far 1114 cost 16580 profit/loss -15466\n",
            "won so far 1118 cost 16610 profit/loss -15492\n",
            "won so far 1122 cost 16744 profit/loss -15622\n",
            "won so far 1126 cost 16754 profit/loss -15628\n",
            "won so far 1133 cost 16838 profit/loss -15705\n",
            "won so far 1137 cost 16960 profit/loss -15823\n",
            "won so far 1141 cost 17196 profit/loss -16055\n",
            "won so far 1145 cost 17324 profit/loss -16179\n",
            "won so far 1149 cost 17476 profit/loss -16327\n",
            "won so far 1153 cost 17478 profit/loss -16325\n",
            "won so far 1157 cost 17682 profit/loss -16525\n",
            "won so far 1161 cost 17686 profit/loss -16525\n",
            "won so far 1165 cost 17778 profit/loss -16613\n",
            "won so far 1169 cost 17812 profit/loss -16643\n",
            "won so far 1173 cost 17836 profit/loss -16663\n",
            "won so far 1177 cost 17866 profit/loss -16689\n",
            "won so far 1181 cost 17992 profit/loss -16811\n",
            "won so far 1185 cost 18010 profit/loss -16825\n",
            "won so far 1189 cost 18014 profit/loss -16825\n",
            "won so far 1193 cost 18082 profit/loss -16889\n",
            "won so far 1197 cost 18110 profit/loss -16913\n",
            "won so far 1201 cost 18112 profit/loss -16911\n",
            "won so far 1205 cost 18248 profit/loss -17043\n",
            "won so far 1209 cost 18448 profit/loss -17239\n",
            "won so far 1213 cost 18492 profit/loss -17279\n",
            "won so far 1217 cost 18514 profit/loss -17297\n",
            "won so far 1221 cost 18598 profit/loss -17377\n",
            "won so far 1225 cost 18636 profit/loss -17411\n",
            "won so far 1229 cost 18778 profit/loss -17549\n",
            "won so far 1233 cost 18826 profit/loss -17593\n",
            "won so far 1237 cost 18856 profit/loss -17619\n",
            "won so far 1241 cost 18874 profit/loss -17633\n",
            "won so far 1245 cost 18938 profit/loss -17693\n",
            "won so far 1252 cost 19024 profit/loss -17772\n",
            "won so far 1256 cost 19026 profit/loss -17770\n",
            "won so far 1260 cost 19140 profit/loss -17880\n",
            "won so far 1264 cost 19272 profit/loss -18008\n",
            "won so far 1268 cost 19274 profit/loss -18006\n",
            "won so far 1272 cost 19334 profit/loss -18062\n",
            "won so far 1276 cost 19382 profit/loss -18106\n",
            "won so far 1280 cost 19394 profit/loss -18114\n",
            "won so far 1284 cost 19454 profit/loss -18170\n",
            "won so far 1288 cost 19484 profit/loss -18196\n",
            "won so far 1292 cost 19538 profit/loss -18246\n",
            "won so far 1299 cost 19630 profit/loss -18331\n",
            "won so far 1303 cost 19680 profit/loss -18377\n",
            "won so far 1307 cost 19758 profit/loss -18451\n",
            "won so far 1311 cost 19786 profit/loss -18475\n",
            "won so far 1315 cost 19946 profit/loss -18631\n",
            "won so far 1319 cost 19966 profit/loss -18647\n",
            "won so far 1323 cost 19980 profit/loss -18657\n",
            "==========================\n",
            "percetange 3.18\n",
            "percetange power ball 3.1\n",
            "==========================\n",
            "16 24 32 50 34 08: 742\n",
            "16 40 29 50 58 05: 648\n",
            "16 24 38 50 58 12: 326\n",
            "16 13 34 58 69 12: 171\n",
            "16 40 29 53 58 18: 146\n",
            "16 58 22 53 32 25: 123\n",
            "16 33 38 58 54 12: 98\n",
            "16 13 34 58 43 12: 46\n",
            "16 24 34 50 59 11: 46\n",
            "16 40 29 50 58 12: 45\n",
            "16 24 38 50 58 25: 37\n",
            "16 33 30 58 53 24: 32\n",
            "16 33 34 58 69 12: 31\n",
            "16 09 32 30 58 11: 31\n",
            "27 34 58 53 65 16: 24\n",
            "27 34 58 54 59 12: 24\n",
            "16 29 48 50 58 24: 24\n",
            "16 40 29 50 58 25: 21\n",
            "16 40 29 50 58 08: 21\n",
            "16 40 29 53 58 25: 21\n",
            "16 33 30 58 69 24: 20\n",
            "16 34 24 58 30 12: 20\n",
            "16 24 34 50 58 04: 19\n",
            "34 58 53 54 69 12: 18\n",
            "02 40 58 53 69 16: 16\n",
            "16 24 38 50 58 05: 15\n",
            "16 24 32 30 56 11: 15\n",
            "16 33 24 58 69 12: 15\n",
            "16 24 34 50 59 04: 14\n",
            "16 33 30 58 57 24: 14\n",
            "16 24 34 50 61 04: 13\n",
            "11 34 58 53 16 12: 13\n",
            "16 40 29 51 58 08: 13\n",
            "16 24 38 30 58 11: 13\n",
            "27 58 47 64 16 12: 12\n",
            "47 34 53 16 58 12: 11\n",
            "16 58 32 30 69 11: 11\n",
            "16 40 29 50 58 11: 11\n",
            "16 40 29 53 58 09: 11\n",
            "16 34 24 53 58 18: 10\n",
            "16 09 24 50 34 25: 10\n",
            "27 34 58 54 02 12: 10\n",
            "16 58 22 50 32 25: 10\n",
            "16 24 34 50 59 05: 10\n",
            "16 34 24 53 58 25: 10\n",
            "27 34 58 47 65 12: 9\n",
            "16 30 24 50 58 11: 9\n",
            "28 47 48 53 58 16: 8\n",
            "47 34 60 53 16 12: 8\n",
            "16 34 38 58 54 12: 8\n",
            "16 40 24 53 58 18: 8\n",
            "16 34 38 58 51 12: 7\n",
            "16 40 47 53 58 12: 7\n",
            "16 13 34 41 58 25: 7\n",
            "16 53 32 58 69 12: 7\n",
            "34 40 58 47 59 12: 7\n",
            "16 29 48 50 58 17: 7\n",
            "56 34 53 54 58 12: 7\n",
            "16 40 29 41 58 25: 7\n",
            "16 24 34 50 58 05: 7\n",
            "16 58 22 54 32 25: 7\n",
            "16 09 32 54 58 25: 7\n",
            "30 34 47 53 16 12: 7\n",
            "47 34 51 53 58 16: 7\n",
            "16 09 24 53 32 25: 6\n",
            "34 58 53 14 54 12: 6\n",
            "27 34 58 59 69 12: 6\n",
            "16 40 29 51 58 05: 6\n",
            "16 33 30 58 54 24: 6\n",
            "16 56 32 58 69 11: 6\n",
            "16 58 22 53 32 05: 6\n",
            "16 34 38 58 57 12: 6\n",
            "05 34 47 58 52 12: 6\n",
            "09 34 21 16 54 12: 6\n",
            "27 40 58 53 69 16: 6\n",
            "16 33 30 58 34 24: 6\n",
            "27 58 53 16 65 12: 6\n",
            "16 58 32 54 34 08: 6\n",
            "65 34 58 53 16 12: 6\n",
            "11 34 09 53 58 16: 6\n",
            "16 15 13 30 58 11: 6\n",
            "16 13 34 47 58 05: 6\n",
            "16 34 38 53 58 12: 6\n",
            "16 47 24 50 58 25: 5\n",
            "16 24 38 50 58 11: 5\n",
            "16 09 34 41 58 25: 5\n",
            "02 16 24 32 34 26: 5\n",
            "16 09 13 30 58 11: 5\n",
            "16 36 13 50 58 12: 5\n",
            "30 34 47 59 58 12: 5\n",
            "16 13 29 50 43 25: 5\n",
            "27 47 58 55 65 09: 5\n",
            "16 28 22 43 69 11: 5\n",
            "16 24 32 34 58 11: 5\n",
            "16 30 11 53 69 03: 5\n",
            "16 58 32 53 69 25: 5\n",
            "16 30 34 50 58 11: 5\n",
            "30 34 47 58 16 12: 5\n",
            "16 40 29 53 58 12: 5\n",
            "27 58 53 16 54 12: 5\n",
            "16 13 34 50 58 04: 5\n",
            "56 34 16 58 54 12: 5\n",
            "16 24 30 50 34 08: 5\n",
            "16 13 34 59 58 12: 5\n",
            "16 13 34 58 57 12: 5\n",
            "27 58 53 47 65 12: 4\n",
            "16 33 09 58 69 12: 4\n",
            "16 34 38 37 58 12: 4\n",
            "16 29 48 50 58 12: 4\n",
            "16 34 38 58 30 12: 4\n",
            "11 34 09 16 58 12: 4\n",
            "12 34 24 16 58 14: 4\n",
            "16 17 24 40 32 05: 4\n",
            "16 60 32 69 58 11: 4\n",
            "16 13 34 24 58 12: 4\n",
            "32 34 52 53 16 12: 4\n",
            "16 24 38 50 58 08: 4\n",
            "16 58 22 50 53 25: 4\n",
            "27 58 53 16 69 12: 4\n",
            "16 24 32 50 34 01: 4\n",
            "02 40 58 47 69 12: 4\n",
            "27 34 36 52 58 12: 4\n",
            "27 34 58 53 16 12: 4\n",
            "27 34 58 64 16 12: 4\n",
            "16 54 32 56 69 25: 4\n",
            "16 24 32 58 49 11: 4\n",
            "40 58 41 53 43 16: 4\n",
            "16 40 29 51 58 25: 4\n",
            "02 34 58 53 69 16: 4\n",
            "16 13 34 47 58 04: 4\n",
            "11 34 09 53 54 16: 4\n",
            "27 58 53 59 16 12: 4\n",
            "16 30 09 53 69 03: 4\n",
            "14 34 58 53 54 16: 4\n",
            "28 47 58 53 65 03: 4\n",
            "16 24 34 50 58 11: 4\n",
            "16 58 22 54 53 25: 4\n",
            "16 58 22 53 32 03: 4\n",
            "34 40 58 55 54 12: 4\n",
            "11 34 09 53 16 15: 4\n",
            "16 29 22 58 53 17: 4\n",
            "06 16 24 32 34 26: 4\n",
            "27 34 47 58 16 12: 4\n",
            "16 40 47 58 57 12: 4\n",
            "16 13 34 54 58 25: 4\n",
            "16 65 34 50 58 05: 4\n",
            "16 30 37 33 58 26: 4\n",
            "27 58 29 47 52 12: 3\n",
            "15 34 47 59 58 12: 3\n",
            "16 58 24 53 32 25: 3\n",
            "27 13 47 16 58 12: 3\n",
            "16 28 22 43 58 11: 3\n",
            "16 24 33 30 58 11: 3\n",
            "16 40 29 51 58 17: 3\n",
            "16 40 47 53 58 18: 3\n",
            "16 13 34 58 63 12: 3\n",
            "34 36 52 47 58 12: 3\n",
            "11 34 58 16 65 12: 3\n",
            "27 09 58 16 69 12: 3\n",
            "16 40 38 50 58 12: 3\n",
            "27 58 53 64 16 12: 3\n",
            "27 34 58 53 69 12: 3\n",
            "16 37 34 58 69 12: 3\n",
            "16 12 30 32 58 08: 3\n",
            "16 13 34 58 65 12: 3\n",
            "16 54 32 69 58 19: 3\n",
            "16 30 09 40 58 11: 3\n",
            "16 33 38 58 41 12: 3\n",
            "27 34 58 55 16 12: 3\n",
            "16 30 09 50 58 11: 3\n",
            "27 34 58 43 65 09: 3\n",
            "16 40 38 58 60 12: 3\n",
            "16 34 38 51 58 12: 3\n",
            "11 34 58 53 54 16: 3\n",
            "27 16 58 47 69 12: 3\n",
            "56 34 53 16 58 12: 3\n",
            "27 47 58 64 65 09: 3\n",
            "15 34 47 58 16 12: 3\n",
            "16 24 38 50 34 05: 3\n",
            "16 40 34 58 60 12: 3\n",
            "27 34 58 55 59 12: 3\n",
            "36 34 51 53 58 16: 3\n",
            "27 58 47 16 69 12: 3\n",
            "28 47 58 53 60 03: 3\n",
            "16 13 29 30 58 11: 3\n",
            "16 13 29 50 58 05: 3\n",
            "16 29 34 50 58 04: 3\n",
            "11 34 09 54 58 12: 3\n",
            "16 47 22 58 53 12: 3\n",
            "16 29 48 58 53 17: 3\n",
            "10 34 60 58 16 12: 3\n",
            "34 40 58 47 54 12: 3\n",
            "27 58 53 64 69 12: 3\n",
            "16 40 11 53 58 18: 3\n",
            "16 30 34 50 58 05: 3\n",
            "16 47 34 50 59 24: 3\n",
            "27 34 58 54 14 12: 3\n",
            "16 40 29 53 58 05: 3\n",
            "16 40 24 50 58 25: 3\n",
            "27 47 29 64 65 09: 3\n",
            "16 58 32 30 51 11: 3\n",
            "16 58 32 63 69 11: 3\n",
            "16 40 29 48 58 12: 3\n",
            "11 34 09 58 54 16: 3\n",
            "16 58 13 53 69 25: 3\n",
            "16 58 32 30 34 11: 3\n",
            "16 33 34 58 68 12: 3\n",
            "30 34 47 16 58 12: 3\n",
            "16 40 29 51 58 12: 3\n",
            "56 34 16 53 58 12: 3\n",
            "16 40 34 51 58 24: 3\n",
            "28 47 58 53 69 03: 3\n",
            "16 17 34 47 58 05: 3\n",
            "27 34 58 55 65 12: 3\n",
            "13 34 58 16 69 12: 3\n",
            "47 34 53 54 58 12: 3\n",
            "16 29 22 50 58 24: 3\n",
            "16 24 32 50 58 11: 3\n",
            "30 34 47 64 58 12: 3\n",
            "16 24 34 50 02 05: 3\n",
            "07 11 09 15 32 25: 2\n",
            "16 13 29 58 69 12: 2\n",
            "11 34 54 16 58 12: 2\n",
            "16 28 13 58 69 11: 2\n",
            "16 28 13 51 58 08: 2\n",
            "56 34 16 53 58 25: 2\n",
            "54 34 58 53 69 16: 2\n",
            "16 24 38 58 60 12: 2\n",
            "47 34 64 53 58 16: 2\n",
            "11 34 58 47 54 16: 2\n",
            "34 16 24 54 58 25: 2\n",
            "06 16 47 34 58 14: 2\n",
            "16 34 38 53 58 25: 2\n",
            "16 58 22 41 63 25: 2\n",
            "16 30 09 33 58 25: 2\n",
            "34 39 57 53 58 16: 2\n",
            "34 40 58 55 59 12: 2\n",
            "16 33 34 58 02 24: 2\n",
            "16 09 24 58 34 26: 2\n",
            "16 58 32 53 43 25: 2\n",
            "16 18 34 55 58 19: 2\n",
            "47 29 30 64 58 16: 2\n",
            "47 34 53 14 58 12: 2\n",
            "16 40 38 53 58 12: 2\n",
            "16 13 34 24 58 02: 2\n",
            "06 58 09 16 32 25: 2\n",
            "11 16 22 54 60 24: 2\n",
            "56 34 29 53 58 16: 2\n",
            "16 24 33 50 34 08: 2\n",
            "16 30 34 58 63 11: 2\n",
            "16 09 24 53 58 25: 2\n",
            "28 34 13 43 58 16: 2\n",
            "52 34 54 16 58 12: 2\n",
            "52 58 47 64 16 12: 2\n",
            "16 09 24 58 69 12: 2\n",
            "56 34 53 51 58 12: 2\n",
            "16 34 38 50 58 12: 2\n",
            "33 36 54 47 58 12: 2\n",
            "16 24 32 30 34 11: 2\n",
            "16 33 34 58 59 12: 2\n",
            "28 47 48 58 53 16: 2\n",
            "27 34 58 55 14 12: 2\n",
            "11 16 22 32 54 18: 2\n",
            "16 09 32 63 58 11: 2\n",
            "51 34 58 53 16 12: 2\n",
            "27 13 52 16 58 12: 2\n",
            "16 30 11 43 69 25: 2\n",
            "34 44 16 53 58 12: 2\n",
            "27 58 53 64 54 12: 2\n",
            "47 58 53 54 69 12: 2\n",
            "16 40 24 53 58 05: 2\n",
            "16 40 13 51 58 24: 2\n",
            "16 09 34 50 58 11: 2\n",
            "16 60 32 30 58 11: 2\n",
            "16 58 22 64 53 25: 2\n",
            "24 34 58 53 69 16: 2\n",
            "16 40 34 50 58 11: 2\n",
            "16 29 33 50 58 11: 2\n",
            "16 47 48 53 58 09: 2\n",
            "09 34 21 16 58 12: 2\n",
            "16 30 24 33 58 25: 2\n",
            "07 16 09 34 32 25: 2\n",
            "16 28 22 58 53 11: 2\n",
            "09 34 58 16 54 12: 2\n",
            "54 58 51 64 69 16: 2\n",
            "16 53 24 50 58 25: 2\n",
            "16 15 13 54 58 05: 2\n",
            "16 58 32 60 65 12: 2\n",
            "30 34 47 53 58 16: 2\n",
            "16 09 24 53 34 26: 2\n",
            "13 34 47 43 58 16: 2\n",
            "16 10 24 39 58 25: 2\n",
            "11 34 13 47 58 16: 2\n",
            "16 37 09 53 58 12: 2\n",
            "56 40 64 53 58 16: 2\n",
            "33 47 34 58 65 12: 2\n",
            "16 33 34 58 30 12: 2\n",
            "10 58 34 47 43 16: 2\n",
            "16 13 34 47 58 12: 2\n",
            "16 56 32 69 58 11: 2\n",
            "16 24 28 50 32 11: 2\n",
            "16 53 32 58 69 11: 2\n",
            "27 58 59 64 65 12: 2\n",
            "52 34 47 58 16 12: 2\n",
            "27 58 29 64 65 16: 2\n",
            "16 40 29 51 58 09: 2\n",
            "27 16 47 34 58 12: 2\n",
            "27 34 58 47 65 09: 2\n",
            "34 58 53 55 54 12: 2\n",
            "27 34 58 65 59 12: 2\n",
            "33 40 47 53 58 12: 2\n",
            "36 58 47 53 69 16: 2\n",
            "11 34 41 53 58 16: 2\n",
            "12 34 09 16 58 14: 2\n",
            "40 36 41 64 58 12: 2\n",
            "16 13 41 50 58 25: 2\n",
            "27 58 53 65 13 09: 2\n",
            "56 34 64 58 16 12: 2\n",
            "13 34 58 16 59 12: 2\n",
            "16 13 34 58 53 12: 2\n",
            "16 21 34 55 58 12: 2\n",
            "28 47 58 53 59 18: 2\n",
            "52 16 54 34 58 14: 2\n",
            "16 30 09 40 69 11: 2\n",
            "16 12 32 54 34 25: 2\n",
            "28 47 58 53 59 05: 2\n",
            "57 34 58 53 69 16: 2\n",
            "16 40 11 56 58 12: 2\n",
            "16 33 38 40 41 34: 2\n",
            "16 24 32 63 34 11: 2\n",
            "34 40 16 50 58 12: 2\n",
            "14 34 58 53 16 12: 2\n",
            "56 34 58 53 69 16: 2\n",
            "16 24 38 50 57 12: 2\n",
            "28 34 47 58 54 16: 2\n",
            "16 13 47 58 69 12: 2\n",
            "24 34 58 16 65 12: 2\n",
            "02 29 58 53 69 16: 2\n",
            "27 34 58 54 24 12: 2\n",
            "09 40 29 47 54 05: 2\n",
            "16 09 24 37 58 25: 2\n",
            "27 15 53 16 58 12: 2\n",
            "34 58 53 54 62 12: 2\n",
            "16 40 48 53 58 18: 2\n",
            "30 34 47 58 54 12: 2\n",
            "16 58 24 50 34 04: 2\n",
            "16 42 34 50 58 05: 2\n",
            "16 47 24 53 58 25: 2\n",
            "27 34 47 59 58 12: 2\n",
            "27 34 47 58 52 12: 2\n",
            "27 34 58 51 59 12: 2\n",
            "16 40 13 51 58 17: 2\n",
            "16 24 34 41 58 25: 2\n",
            "27 16 58 53 69 14: 2\n",
            "34 58 53 69 54 12: 2\n",
            "40 58 41 53 02 16: 2\n",
            "34 16 52 58 54 12: 2\n",
            "16 34 41 58 44 33: 2\n",
            "16 13 29 50 58 25: 2\n",
            "07 34 09 58 54 12: 2\n",
            "16 24 34 50 59 25: 2\n",
            "16 28 24 53 58 25: 2\n",
            "24 34 58 53 16 12: 2\n",
            "16 34 38 58 60 12: 2\n",
            "15 34 47 53 58 16: 2\n",
            "16 09 32 61 58 11: 2\n",
            "33 36 47 64 58 12: 2\n",
            "34 58 53 64 54 12: 2\n",
            "16 40 47 50 58 12: 2\n",
            "02 34 58 53 16 12: 2\n",
            "27 58 64 14 65 09: 2\n",
            "16 34 24 58 60 12: 2\n",
            "34 58 47 64 52 12: 2\n",
            "16 24 34 14 61 12: 2\n",
            "16 47 24 54 58 25: 2\n",
            "02 16 29 58 54 12: 2\n",
            "34 58 53 16 54 12: 2\n",
            "68 34 58 53 65 16: 2\n",
            "10 58 22 47 52 14: 2\n",
            "16 24 34 41 61 25: 2\n",
            "54 58 55 69 65 11: 2\n",
            "27 34 58 52 65 12: 2\n",
            "16 13 34 41 58 12: 2\n",
            "40 60 41 53 58 16: 2\n",
            "01 34 16 30 58 12: 2\n",
            "16 09 32 43 34 11: 2\n",
            "10 58 47 64 16 12: 2\n",
            "56 58 53 16 69 12: 2\n",
            "16 17 24 40 58 05: 2\n",
            "10 58 34 16 43 12: 2\n",
            "16 09 24 50 54 25: 2\n",
            "16 30 24 50 58 25: 2\n",
            "16 58 32 60 34 12: 2\n",
            "16 24 34 58 61 12: 2\n",
            "30 34 47 64 16 12: 2\n",
            "16 42 13 50 58 12: 2\n",
            "16 29 22 31 58 24: 2\n",
            "11 34 38 16 58 12: 2\n",
            "47 34 57 53 58 16: 2\n",
            "56 34 64 16 58 12: 2\n",
            "16 58 22 50 53 05: 2\n",
            "27 30 29 41 58 05: 2\n",
            "16 42 24 50 58 25: 2\n",
            "32 34 52 53 58 16: 2\n",
            "56 34 64 16 65 12: 2\n",
            "27 34 58 54 16 12: 2\n",
            "10 58 34 16 59 12: 2\n",
            "16 09 24 50 34 04: 2\n",
            "02 58 16 47 54 25: 2\n",
            "36 34 51 53 58 12: 2\n",
            "16 40 38 53 58 18: 2\n",
            "33 40 34 58 60 12: 2\n",
            "16 53 58 50 69 12: 2\n",
            "47 34 59 58 69 12: 2\n",
            "02 40 34 41 58 25: 2\n",
            "28 34 47 58 64 16: 2\n",
            "56 34 58 53 16 12: 2\n",
            "02 16 09 15 32 25: 2\n",
            "34 58 16 64 54 12: 2\n",
            "07 09 53 30 54 11: 2\n",
            "16 29 48 50 53 24: 2\n",
            "16 12 24 58 34 05: 2\n",
            "09 40 11 16 54 12: 2\n",
            "34 58 16 53 54 25: 2\n",
            "34 58 53 57 54 12: 2\n",
            "16 24 34 54 61 04: 2\n",
            "34 58 16 64 54 11: 2\n",
            "33 34 54 58 60 12: 2\n",
            "52 34 54 58 16 12: 2\n",
            "16 33 30 58 63 24: 2\n",
            "33 36 58 47 52 12: 2\n",
            "16 34 38 30 58 12: 2\n",
            "09 16 28 32 68 11: 2\n",
            "27 36 58 47 16 12: 2\n",
            "16 24 53 51 58 11: 2\n",
            "03 34 13 16 58 12: 2\n",
            "56 34 47 16 58 12: 2\n",
            "16 29 22 53 58 18: 2\n",
            "34 16 54 24 58 12: 2\n",
            "47 34 60 53 58 16: 2\n",
            "40 58 41 53 54 16: 2\n",
            "33 40 34 58 54 12: 2\n",
            "16 24 38 50 58 01: 2\n",
            "16 24 34 28 58 11: 2\n",
            "16 40 47 58 69 12: 2\n",
            "27 34 36 43 58 12: 2\n",
            "16 60 32 58 69 12: 2\n",
            "13 34 09 16 58 12: 2\n",
            "34 58 53 54 57 12: 2\n",
            "16 24 53 30 58 11: 2\n",
            "16 58 32 65 34 09: 2\n",
            "32 34 54 53 58 12: 2\n",
            "27 58 29 65 54 09: 2\n",
            "won=1323 cost 12\n",
            "pick: 12 % 33.21 diff 6\n",
            "pick: 05 % 11.17 diff 13\n",
            "pick: 16 % 10.65 diff 2\n",
            "pick: 25 % 9.700000000000001 diff -7\n",
            "pick: 08 % 9.51 diff 10\n",
            "pick: 11 % 6.58 diff 7\n",
            "pick: 18 % 3.1!!!!!\n",
            "pick: 09 % 2.36 diff 9\n",
            "pick: 24 % 2.2800000000000002 diff -6\n",
            "pick: 14 % 2.09 diff 4\n",
            "pick: 26 % 1.53 diff -8\n",
            "pick: 04 % 1.29 diff 14\n",
            "pick: 02 % 0.65 diff 16\n",
            "pick: 03 % 0.63 diff 15\n",
            "pick: 01 % 0.62 diff 17\n",
            "pick: 17 % 0.5499999999999999 diff 1\n",
            "pick: 07 % 0.53 diff 11\n",
            "pick: 19 % 0.5 diff -1\n",
            "pick: 06 % 0.3 diff 12\n",
            "pick: 10 % 0.29 diff 8\n",
            "pick: 15 % 0.15 diff 3\n",
            "pick: 21 % 0.1 diff -3\n",
            "pick: 13 % 0.08 diff 5\n",
            "pick: 22 % 0.08 diff -4\n",
            "pick: 29 % 0.06999999999999999 diff -11\n",
            "pick: 27 % 0.04 diff -9\n",
            "pick: 28 % 0.03 diff -10\n",
            "==========================\n",
            "63\n",
            "pick: 58 13.344000000000001 %\n",
            "pick: 16 13.282 %\n",
            "pick: 34 7.4959999999999996 %\n",
            "pick: 50 5.384 %\n",
            "pick: 24 4.398 %\n",
            "pick: 53 4.046 %\n",
            "pick: 40 3.392 %\n",
            "pick: 32 3.386 %\n",
            "pick: 29 3.032 %\n",
            "pick: 47 2.964 %\n",
            "pick: 54 2.854 %\n",
            "pick: 69 2.6 %\n",
            "pick: 30 2.028 %\n",
            "pick: 13 1.7579999999999998 %\n",
            "pick: 27 1.5879999999999999 %\n",
            "pick: 09 1.528 % !!!!\n",
            "pick: 38 1.462 %\n",
            "pick: 33 1.3599999999999999 %\n",
            "pick: 52 1.1320000000000001 %\n",
            "pick: 64 1.126 %\n",
            "pick: 11 1.076 %\n",
            "pick: 56 1.044 %\n",
            "pick: 02 1.028 % !!!!\n",
            "pick: 22 1.028 %\n",
            "pick: 51 0.9520000000000001 %\n",
            "pick: 65 0.942 %\n",
            "pick: 59 0.8999999999999999 %\n",
            "pick: 55 0.8380000000000001 % !!!!\n",
            "pick: 57 0.818 % !!!!\n",
            "pick: 60 0.804 %\n",
            "pick: 43 0.804 % !!!!\n",
            "pick: 28 0.788 %\n",
            "pick: 12 0.692 %\n",
            "pick: 14 0.67 %\n",
            "pick: 48 0.638 %\n",
            "pick: 15 0.598 %\n",
            "pick: 41 0.596 %\n",
            "pick: 36 0.586 %\n",
            "pick: 37 0.582 %\n",
            "pick: 10 0.544 %\n",
            "pick: 18 0.428 %\n",
            "pick: 39 0.422 %\n",
            "pick: 63 0.404 %\n",
            "pick: 44 0.394 %\n",
            "pick: 07 0.38999999999999996 %\n",
            "pick: 21 0.386 %\n",
            "pick: 17 0.38 %\n",
            "pick: 42 0.35400000000000004 %\n",
            "pick: 49 0.344 %\n",
            "pick: 04 0.302 %\n",
            "pick: 06 0.27 %\n",
            "pick: 68 0.232 %\n",
            "pick: 05 0.22399999999999998 %\n",
            "pick: 25 0.21 %\n",
            "pick: 61 0.21 %\n",
            "pick: 01 0.192 %\n",
            "pick: 26 0.17600000000000002 %\n",
            "pick: 46 0.156 %\n",
            "pick: 62 0.15 %\n",
            "pick: 19 0.1 %\n",
            "pick: 03 0.086 %\n",
            "pick: 31 0.066 %\n",
            "pick: 08 0.036000000000000004 %\n",
            "07/17/2023\n",
            "won so far 4 cost 54 profit/loss -50\n",
            "won so far 8 cost 172 profit/loss -164\n",
            "won so far 12 cost 180 profit/loss -168\n",
            "won so far 16 cost 204 profit/loss -188\n",
            "won so far 20 cost 244 profit/loss -224\n",
            "won so far 24 cost 318 profit/loss -294\n",
            "won so far 28 cost 342 profit/loss -314\n",
            "won so far 32 cost 368 profit/loss -336\n",
            "won so far 36 cost 430 profit/loss -394\n",
            "won so far 40 cost 502 profit/loss -462\n",
            "won so far 44 cost 520 profit/loss -476\n",
            "won so far 48 cost 562 profit/loss -514\n",
            "won so far 52 cost 572 profit/loss -520\n",
            "won so far 56 cost 644 profit/loss -588\n",
            "won so far 60 cost 748 profit/loss -688\n",
            "won so far 64 cost 820 profit/loss -756\n",
            "won so far 68 cost 844 profit/loss -776\n",
            "won so far 72 cost 944 profit/loss -872\n",
            "won so far 76 cost 954 profit/loss -878\n",
            "won so far 80 cost 988 profit/loss -908\n",
            "won so far 84 cost 1002 profit/loss -918\n",
            "won so far 88 cost 1298 profit/loss -1210\n",
            "won so far 92 cost 1630 profit/loss -1538\n",
            "won so far 96 cost 1680 profit/loss -1584\n",
            "won so far 100 cost 1814 profit/loss -1714\n",
            "won so far 104 cost 2060 profit/loss -1956\n",
            "won so far 108 cost 2142 profit/loss -2034\n",
            "won so far 112 cost 2158 profit/loss -2046\n",
            "won so far 116 cost 2160 profit/loss -2044\n",
            "won so far 123 cost 2192 profit/loss -2069\n",
            "won so far 127 cost 2208 profit/loss -2081\n",
            "won so far 131 cost 2226 profit/loss -2095\n",
            "won so far 135 cost 2564 profit/loss -2429\n",
            "won so far 139 cost 2626 profit/loss -2487\n",
            "won so far 143 cost 2672 profit/loss -2529\n",
            "won so far 147 cost 2674 profit/loss -2527\n",
            "won so far 151 cost 2774 profit/loss -2623\n",
            "won so far 155 cost 2938 profit/loss -2783\n",
            "won so far 162 cost 3256 profit/loss -3094\n",
            "won so far 166 cost 3262 profit/loss -3096\n",
            "won so far 170 cost 3284 profit/loss -3114\n",
            "won so far 174 cost 3442 profit/loss -3268\n",
            "won so far 178 cost 3516 profit/loss -3338\n",
            "won so far 182 cost 3608 profit/loss -3426\n",
            "won so far 186 cost 3690 profit/loss -3504\n",
            "won so far 190 cost 3808 profit/loss -3618\n",
            "won so far 194 cost 4016 profit/loss -3822\n",
            "won so far 198 cost 4040 profit/loss -3842\n",
            "won so far 202 cost 4042 profit/loss -3840\n",
            "won so far 206 cost 4374 profit/loss -4168\n",
            "won so far 210 cost 4380 profit/loss -4170\n",
            "won so far 214 cost 4438 profit/loss -4224\n",
            "won so far 218 cost 4606 profit/loss -4388\n",
            "won so far 222 cost 4620 profit/loss -4398\n",
            "won so far 226 cost 4696 profit/loss -4470\n",
            "won so far 230 cost 4708 profit/loss -4478\n",
            "won so far 234 cost 4750 profit/loss -4516\n",
            "won so far 238 cost 5414 profit/loss -5176\n",
            "won so far 242 cost 5454 profit/loss -5212\n",
            "won so far 246 cost 5540 profit/loss -5294\n",
            "won so far 250 cost 5564 profit/loss -5314\n",
            "won so far 254 cost 5606 profit/loss -5352\n",
            "won so far 258 cost 5634 profit/loss -5376\n",
            "won so far 262 cost 5682 profit/loss -5420\n",
            "won so far 266 cost 5714 profit/loss -5448\n",
            "won so far 270 cost 5716 profit/loss -5446\n",
            "won so far 274 cost 5724 profit/loss -5450\n",
            "won so far 278 cost 5736 profit/loss -5458\n",
            "won so far 282 cost 5782 profit/loss -5500\n",
            "won so far 286 cost 5890 profit/loss -5604\n",
            "won so far 290 cost 6096 profit/loss -5806\n",
            "won so far 294 cost 6278 profit/loss -5984\n",
            "won so far 298 cost 6390 profit/loss -6092\n",
            "won so far 302 cost 6584 profit/loss -6282\n",
            "won so far 306 cost 6634 profit/loss -6328\n",
            "won so far 310 cost 6712 profit/loss -6402\n",
            "won so far 314 cost 6836 profit/loss -6522\n",
            "won so far 318 cost 6862 profit/loss -6544\n",
            "won so far 322 cost 7004 profit/loss -6682\n",
            "won so far 326 cost 7048 profit/loss -6722\n",
            "won so far 330 cost 7146 profit/loss -6816\n",
            "won so far 334 cost 7332 profit/loss -6998\n",
            "won so far 338 cost 7406 profit/loss -7068\n",
            "won so far 342 cost 7584 profit/loss -7242\n",
            "won so far 346 cost 7626 profit/loss -7280\n",
            "won so far 350 cost 7636 profit/loss -7286\n",
            "won so far 354 cost 7730 profit/loss -7376\n",
            "won so far 358 cost 7818 profit/loss -7460\n",
            "won so far 362 cost 8010 profit/loss -7648\n",
            "won so far 366 cost 8100 profit/loss -7734\n",
            "won so far 370 cost 8142 profit/loss -7772\n",
            "won so far 377 cost 8148 profit/loss -7771\n",
            "won so far 381 cost 8176 profit/loss -7795\n",
            "won so far 385 cost 8210 profit/loss -7825\n",
            "won so far 389 cost 8236 profit/loss -7847\n",
            "won so far 393 cost 8308 profit/loss -7915\n",
            "won so far 397 cost 8438 profit/loss -8041\n",
            "won so far 401 cost 8674 profit/loss -8273\n",
            "won so far 405 cost 8696 profit/loss -8291\n",
            "won so far 409 cost 8890 profit/loss -8481\n",
            "won so far 413 cost 8998 profit/loss -8585\n",
            "won so far 417 cost 9000 profit/loss -8583\n",
            "won so far 421 cost 9022 profit/loss -8601\n",
            "won so far 425 cost 9040 profit/loss -8615\n",
            "won so far 429 cost 9060 profit/loss -8631\n",
            "won so far 433 cost 9146 profit/loss -8713\n",
            "won so far 437 cost 9170 profit/loss -8733\n",
            "won so far 441 cost 9236 profit/loss -8795\n",
            "won so far 445 cost 9244 profit/loss -8799\n",
            "won so far 449 cost 9308 profit/loss -8859\n",
            "won so far 453 cost 9332 profit/loss -8879\n",
            "won so far 457 cost 9592 profit/loss -9135\n",
            "won so far 461 cost 9622 profit/loss -9161\n",
            "won so far 465 cost 9728 profit/loss -9263\n",
            "won so far 469 cost 9950 profit/loss -9481\n",
            "won so far 473 cost 10054 profit/loss -9581\n",
            "won so far 477 cost 10100 profit/loss -9623\n",
            "won so far 481 cost 10178 profit/loss -9697\n",
            "won so far 485 cost 10230 profit/loss -9745\n",
            "won so far 489 cost 10282 profit/loss -9793\n",
            "won so far 493 cost 10292 profit/loss -9799\n",
            "won so far 497 cost 10384 profit/loss -9887\n",
            "won so far 501 cost 10404 profit/loss -9903\n",
            "won so far 505 cost 10496 profit/loss -9991\n",
            "won so far 509 cost 10586 profit/loss -10077\n",
            "won so far 513 cost 10730 profit/loss -10217\n",
            "won so far 517 cost 10736 profit/loss -10219\n",
            "won so far 521 cost 10758 profit/loss -10237\n",
            "won so far 525 cost 10966 profit/loss -10441\n",
            "won so far 529 cost 10972 profit/loss -10443\n",
            "won so far 533 cost 11096 profit/loss -10563\n",
            "won so far 537 cost 11118 profit/loss -10581\n",
            "won so far 541 cost 11202 profit/loss -10661\n",
            "won so far 545 cost 11492 profit/loss -10947\n",
            "won so far 549 cost 11574 profit/loss -11025\n",
            "won so far 553 cost 11592 profit/loss -11039\n",
            "won so far 557 cost 11594 profit/loss -11037\n",
            "won so far 561 cost 11666 profit/loss -11105\n",
            "won so far 565 cost 11740 profit/loss -11175\n",
            "won so far 569 cost 11856 profit/loss -11287\n",
            "won so far 573 cost 11886 profit/loss -11313\n",
            "won so far 577 cost 11964 profit/loss -11387\n",
            "won so far 581 cost 12010 profit/loss -11429\n",
            "won so far 585 cost 12020 profit/loss -11435\n",
            "won so far 589 cost 12322 profit/loss -11733\n",
            "won so far 593 cost 12394 profit/loss -11801\n",
            "won so far 597 cost 12498 profit/loss -11901\n",
            "won so far 601 cost 12508 profit/loss -11907\n",
            "won so far 605 cost 12540 profit/loss -11935\n",
            "won so far 609 cost 12554 profit/loss -11945\n",
            "won so far 613 cost 12624 profit/loss -12011\n",
            "won so far 617 cost 12670 profit/loss -12053\n",
            "won so far 621 cost 12898 profit/loss -12277\n",
            "won so far 625 cost 12904 profit/loss -12279\n",
            "won so far 629 cost 13042 profit/loss -12413\n",
            "won so far 633 cost 13152 profit/loss -12519\n",
            "won so far 637 cost 13368 profit/loss -12731\n",
            "won so far 641 cost 13380 profit/loss -12739\n",
            "won so far 645 cost 13492 profit/loss -12847\n",
            "won so far 649 cost 13512 profit/loss -12863\n",
            "won so far 653 cost 13716 profit/loss -13063\n",
            "won so far 657 cost 13746 profit/loss -13089\n",
            "won so far 661 cost 13754 profit/loss -13093\n",
            "won so far 665 cost 13768 profit/loss -13103\n",
            "won so far 669 cost 13848 profit/loss -13179\n",
            "won so far 673 cost 13978 profit/loss -13305\n",
            "won so far 677 cost 14338 profit/loss -13661\n",
            "won so far 681 cost 14456 profit/loss -13775\n",
            "won so far 685 cost 14544 profit/loss -13859\n",
            "won so far 689 cost 14634 profit/loss -13945\n",
            "won so far 693 cost 14678 profit/loss -13985\n",
            "won so far 697 cost 14684 profit/loss -13987\n",
            "won so far 701 cost 14732 profit/loss -14031\n",
            "won so far 705 cost 14802 profit/loss -14097\n",
            "won so far 709 cost 14940 profit/loss -14231\n",
            "won so far 713 cost 15188 profit/loss -14475\n",
            "won so far 717 cost 15274 profit/loss -14557\n",
            "won so far 721 cost 15334 profit/loss -14613\n",
            "won so far 725 cost 15410 profit/loss -14685\n",
            "won so far 729 cost 15422 profit/loss -14693\n",
            "won so far 733 cost 15430 profit/loss -14697\n",
            "won so far 737 cost 15494 profit/loss -14757\n",
            "won so far 741 cost 15516 profit/loss -14775\n",
            "won so far 745 cost 15546 profit/loss -14801\n",
            "won so far 749 cost 15880 profit/loss -15131\n",
            "won so far 756 cost 15926 profit/loss -15170\n",
            "won so far 760 cost 15986 profit/loss -15226\n",
            "won so far 764 cost 16014 profit/loss -15250\n",
            "won so far 768 cost 16302 profit/loss -15534\n",
            "won so far 772 cost 16318 profit/loss -15546\n",
            "won so far 776 cost 16458 profit/loss -15682\n",
            "won so far 780 cost 16472 profit/loss -15692\n",
            "won so far 784 cost 16486 profit/loss -15702\n",
            "won so far 788 cost 16626 profit/loss -15838\n",
            "won so far 792 cost 16666 profit/loss -15874\n",
            "won so far 796 cost 16820 profit/loss -16024\n",
            "won so far 800 cost 16958 profit/loss -16158\n",
            "won so far 804 cost 17132 profit/loss -16328\n",
            "won so far 808 cost 17190 profit/loss -16382\n",
            "won so far 812 cost 17498 profit/loss -16686\n",
            "won so far 816 cost 17958 profit/loss -17142\n",
            "won so far 820 cost 17986 profit/loss -17166\n",
            "won so far 824 cost 18072 profit/loss -17248\n",
            "won so far 828 cost 18100 profit/loss -17272\n",
            "won so far 832 cost 18118 profit/loss -17286\n",
            "won so far 836 cost 18174 profit/loss -17338\n",
            "won so far 840 cost 18204 profit/loss -17364\n",
            "won so far 844 cost 18306 profit/loss -17462\n",
            "won so far 848 cost 18370 profit/loss -17522\n",
            "won so far 852 cost 18420 profit/loss -17568\n",
            "won so far 856 cost 18482 profit/loss -17626\n",
            "won so far 860 cost 18526 profit/loss -17666\n",
            "won so far 864 cost 18600 profit/loss -17736\n",
            "won so far 868 cost 18744 profit/loss -17876\n",
            "won so far 872 cost 18806 profit/loss -17934\n",
            "won so far 876 cost 18880 profit/loss -18004\n",
            "won so far 880 cost 19114 profit/loss -18234\n",
            "won so far 884 cost 19164 profit/loss -18280\n",
            "won so far 888 cost 19174 profit/loss -18286\n",
            "won so far 892 cost 19316 profit/loss -18424\n",
            "won so far 896 cost 19352 profit/loss -18456\n",
            "won so far 900 cost 19428 profit/loss -18528\n",
            "won so far 904 cost 19466 profit/loss -18562\n",
            "won so far 908 cost 19498 profit/loss -18590\n",
            "won so far 912 cost 19568 profit/loss -18656\n",
            "won so far 916 cost 19586 profit/loss -18670\n",
            "won so far 920 cost 19738 profit/loss -18818\n",
            "won so far 927 cost 19966 profit/loss -19039\n",
            "==========================\n",
            "percetange 2.2800000000000002\n",
            "percetange power ball 2.26\n",
            "==========================\n",
            "12 14 10 21 24 07: 677\n",
            "12 14 23 21 56 07: 181\n",
            "26 14 45 21 52 29: 176\n",
            "26 14 45 21 58 16: 143\n",
            "14 26 45 39 53 11: 143\n",
            "08 26 15 52 27 16: 101\n",
            "08 26 15 45 53 20: 99\n",
            "12 14 10 21 56 07: 97\n",
            "14 10 23 45 52 07: 84\n",
            "26 33 45 36 52 06: 79\n",
            "01 26 33 45 19 08: 68\n",
            "14 26 45 39 52 11: 61\n",
            "01 26 24 67 29 05: 58\n",
            "12 14 34 21 65 10: 42\n",
            "14 39 23 40 52 13: 29\n",
            "14 26 20 24 45 17: 28\n",
            "12 14 34 52 40 10: 26\n",
            "12 14 07 21 24 10: 25\n",
            "06 26 37 36 53 20: 22\n",
            "08 26 15 52 53 16: 21\n",
            "12 14 34 21 56 10: 17\n",
            "26 14 45 21 24 29: 16\n",
            "01 26 33 45 53 20: 16\n",
            "08 26 24 59 29 20: 15\n",
            "14 19 39 62 65 15: 15\n",
            "14 26 45 46 65 13: 14\n",
            "26 14 31 21 56 20: 13\n",
            "08 26 15 61 58 20: 13\n",
            "26 17 45 35 51 13: 13\n",
            "26 17 40 45 52 20: 13\n",
            "12 14 34 21 67 10: 12\n",
            "14 10 23 15 52 13: 11\n",
            "08 26 24 62 29 05: 11\n",
            "14 19 39 24 56 15: 11\n",
            "12 14 23 21 52 07: 11\n",
            "01 26 24 67 29 17: 10\n",
            "01 26 45 35 56 16: 10\n",
            "12 14 23 45 52 07: 10\n",
            "12 14 26 21 24 07: 10\n",
            "08 26 15 45 59 20: 9\n",
            "14 19 39 62 53 15: 9\n",
            "26 12 37 52 40 08: 9\n",
            "14 19 39 24 53 15: 9\n",
            "08 26 27 24 65 20: 9\n",
            "14 26 20 24 45 13: 9\n",
            "14 12 23 34 56 03: 8\n",
            "14 12 23 49 52 05: 8\n",
            "26 14 45 21 52 06: 8\n",
            "26 17 45 35 54 13: 8\n",
            "12 14 26 21 27 10: 8\n",
            "12 14 07 21 27 10: 8\n",
            "10 33 23 36 52 12: 8\n",
            "12 14 07 21 45 10: 8\n",
            "12 14 10 21 65 07: 8\n",
            "26 14 45 21 58 13: 8\n",
            "12 14 45 21 56 10: 7\n",
            "14 24 23 32 56 17: 7\n",
            "08 26 15 59 53 20: 7\n",
            "14 19 39 24 56 17: 7\n",
            "12 14 10 21 67 07: 7\n",
            "26 14 45 21 58 29: 7\n",
            "08 26 15 35 53 20: 7\n",
            "14 26 15 24 29 20: 7\n",
            "24 14 34 33 58 20: 7\n",
            "12 14 07 21 65 10: 7\n",
            "14 33 23 45 52 08: 6\n",
            "14 33 23 39 49 17: 6\n",
            "26 14 45 21 53 13: 6\n",
            "14 26 45 39 58 11: 6\n",
            "26 14 45 21 51 13: 6\n",
            "26 15 33 49 59 23: 6\n",
            "12 14 34 52 65 13: 6\n",
            "08 26 27 24 15 20: 6\n",
            "26 15 33 45 59 20: 6\n",
            "08 10 23 15 52 20: 6\n",
            "26 14 45 21 52 13: 6\n",
            "14 19 39 24 29 36: 6\n",
            "26 14 45 21 46 13: 6\n",
            "14 34 39 51 46 36: 5\n",
            "12 14 23 21 15 07: 5\n",
            "12 14 34 52 65 10: 5\n",
            "12 14 07 21 15 10: 5\n",
            "14 24 23 49 56 07: 5\n",
            "26 24 28 34 49 11: 5\n",
            "08 26 52 67 53 05: 5\n",
            "14 10 23 45 56 07: 5\n",
            "26 14 45 21 51 28: 5\n",
            "26 14 45 21 51 29: 5\n",
            "14 15 23 34 56 10: 5\n",
            "26 24 23 28 21 17: 5\n",
            "08 26 52 53 67 21: 5\n",
            "12 14 23 21 45 07: 5\n",
            "08 26 15 67 29 05: 5\n",
            "26 14 34 21 24 29: 5\n",
            "01 26 33 35 53 20: 5\n",
            "01 26 24 59 29 20: 5\n",
            "14 26 45 39 52 13: 5\n",
            "14 34 23 39 51 13: 5\n",
            "08 26 28 36 53 20: 5\n",
            "12 14 34 21 50 10: 5\n",
            "01 26 45 35 53 11: 5\n",
            "26 34 44 45 52 08: 5\n",
            "08 26 15 36 53 20: 5\n",
            "26 37 36 52 40 35: 5\n",
            "12 14 23 45 56 07: 5\n",
            "08 26 28 36 29 20: 5\n",
            "12 14 40 21 50 10: 5\n",
            "12 14 10 21 45 07: 5\n",
            "26 14 45 21 58 10: 5\n",
            "12 14 65 21 58 10: 5\n",
            "12 14 23 21 27 10: 4\n",
            "08 26 15 67 65 05: 4\n",
            "12 14 23 49 13 21: 4\n",
            "08 26 28 35 36 10: 4\n",
            "01 26 04 45 27 16: 4\n",
            "06 26 15 35 53 16: 4\n",
            "26 19 28 36 29 20: 4\n",
            "12 14 10 45 56 07: 4\n",
            "24 14 34 33 46 27: 4\n",
            "12 14 34 52 40 13: 4\n",
            "24 14 34 33 46 35: 4\n",
            "26 12 45 52 53 13: 4\n",
            "08 26 33 45 53 20: 4\n",
            "14 26 45 39 51 11: 4\n",
            "14 26 15 34 58 17: 4\n",
            "08 23 15 28 29 20: 4\n",
            "12 14 23 52 45 05: 4\n",
            "26 17 45 35 52 13: 4\n",
            "26 17 45 52 63 13: 3\n",
            "01 26 24 67 45 17: 3\n",
            "08 23 15 28 59 20: 3\n",
            "14 26 45 36 53 11: 3\n",
            "01 26 53 59 58 15: 3\n",
            "14 23 39 34 67 13: 3\n",
            "08 26 15 52 65 16: 3\n",
            "26 24 34 59 46 35: 3\n",
            "08 26 15 44 53 20: 3\n",
            "14 26 49 53 58 15: 3\n",
            "14 26 16 24 27 17: 3\n",
            "12 14 26 52 45 10: 3\n",
            "01 26 33 45 21 20: 3\n",
            "26 14 31 45 52 20: 3\n",
            "26 14 45 21 33 29: 3\n",
            "08 26 15 46 58 20: 3\n",
            "08 26 15 45 53 10: 3\n",
            "12 14 26 52 21 05: 3\n",
            "26 15 33 49 59 16: 3\n",
            "08 26 27 24 59 20: 3\n",
            "01 26 33 45 19 16: 3\n",
            "14 39 45 40 52 13: 3\n",
            "26 33 45 52 59 06: 3\n",
            "14 26 20 45 46 19: 3\n",
            "01 26 51 67 58 17: 3\n",
            "14 24 23 21 56 07: 3\n",
            "26 14 45 21 54 29: 3\n",
            "14 26 20 39 45 17: 3\n",
            "14 33 45 39 49 06: 3\n",
            "24 14 34 21 58 13: 3\n",
            "14 32 39 65 40 06: 3\n",
            "26 24 23 51 11 14: 3\n",
            "08 26 15 52 53 20: 3\n",
            "12 14 65 52 58 05: 3\n",
            "01 26 53 35 58 05: 3\n",
            "12 14 10 21 27 07: 3\n",
            "26 24 34 33 46 35: 3\n",
            "14 19 39 51 53 15: 3\n",
            "26 24 33 49 11 20: 3\n",
            "14 26 45 40 53 17: 3\n",
            "08 14 15 24 23 20: 3\n",
            "26 14 45 21 24 10: 3\n",
            "12 14 26 21 50 10: 3\n",
            "26 14 45 21 49 13: 3\n",
            "01 26 45 40 53 17: 3\n",
            "08 26 15 62 27 05: 3\n",
            "08 26 24 52 29 10: 3\n",
            "12 14 26 21 24 10: 3\n",
            "14 39 23 40 13 05: 3\n",
            "08 26 27 24 65 15: 3\n",
            "12 14 21 52 58 10: 3\n",
            "24 14 40 45 59 26: 3\n",
            "06 26 37 40 53 17: 3\n",
            "14 10 23 30 52 15: 3\n",
            "06 26 53 35 58 05: 3\n",
            "26 14 45 21 58 06: 3\n",
            "01 26 45 35 46 13: 3\n",
            "01 26 24 67 23 17: 3\n",
            "08 10 23 52 49 05: 3\n",
            "08 26 15 52 29 20: 3\n",
            "06 26 37 36 53 16: 3\n",
            "20 26 53 35 58 05: 3\n",
            "11 26 51 45 58 08: 3\n",
            "12 14 15 21 56 07: 3\n",
            "12 14 23 21 65 07: 3\n",
            "12 14 34 52 45 10: 3\n",
            "26 14 45 21 58 28: 3\n",
            "12 14 10 21 24 11: 3\n",
            "26 34 33 45 52 08: 3\n",
            "12 26 24 45 40 10: 3\n",
            "14 12 23 49 52 26: 2\n",
            "08 26 15 34 27 20: 2\n",
            "14 26 15 39 53 17: 2\n",
            "14 12 34 33 48 20: 2\n",
            "12 14 23 45 13 07: 2\n",
            "14 26 38 45 40 20: 2\n",
            "24 14 26 45 49 23: 2\n",
            "10 15 23 49 52 14: 2\n",
            "01 26 33 53 59 20: 2\n",
            "26 24 28 36 52 20: 2\n",
            "12 14 10 21 33 07: 2\n",
            "08 26 15 61 27 20: 2\n",
            "24 45 23 67 52 08: 2\n",
            "26 12 51 36 58 37: 2\n",
            "36 31 45 48 67 05: 2\n",
            "26 14 45 21 67 06: 2\n",
            "14 12 39 49 59 05: 2\n",
            "12 14 15 45 52 10: 2\n",
            "08 26 28 52 29 10: 2\n",
            "14 23 15 38 46 13: 2\n",
            "01 26 45 36 53 05: 2\n",
            "01 49 37 52 62 08: 2\n",
            "26 37 45 52 51 16: 2\n",
            "12 14 44 21 51 10: 2\n",
            "26 15 33 49 59 21: 2\n",
            "08 26 15 39 53 20: 2\n",
            "12 14 23 21 15 10: 2\n",
            "10 39 23 40 52 05: 2\n",
            "12 14 10 21 27 05: 2\n",
            "12 14 21 20 24 07: 2\n",
            "08 26 45 67 52 05: 2\n",
            "14 26 33 40 53 17: 2\n",
            "26 14 45 21 53 16: 2\n",
            "26 14 36 45 52 08: 2\n",
            "14 12 39 49 67 05: 2\n",
            "26 14 35 21 59 28: 2\n",
            "26 17 40 45 36 20: 2\n",
            "01 26 45 35 46 05: 2\n",
            "24 23 26 28 32 14: 2\n",
            "08 26 20 24 23 15: 2\n",
            "26 15 45 52 53 13: 2\n",
            "14 26 17 39 53 20: 2\n",
            "01 17 40 45 52 20: 2\n",
            "01 12 23 49 52 05: 2\n",
            "26 14 45 21 46 10: 2\n",
            "08 26 33 52 53 20: 2\n",
            "01 12 45 21 56 05: 2\n",
            "06 40 37 52 53 08: 2\n",
            "12 14 10 52 59 05: 2\n",
            "26 17 45 40 63 05: 2\n",
            "12 14 10 21 40 07: 2\n",
            "08 26 52 44 53 05: 2\n",
            "01 26 33 45 46 20: 2\n",
            "12 14 10 21 05 17: 2\n",
            "06 26 08 35 53 16: 2\n",
            "12 14 21 37 58 07: 2\n",
            "26 37 36 45 40 35: 2\n",
            "12 14 17 39 24 13: 2\n",
            "08 26 24 62 29 20: 2\n",
            "26 24 29 34 33 10: 2\n",
            "12 26 14 36 65 17: 2\n",
            "14 33 23 45 59 08: 2\n",
            "12 14 23 21 27 07: 2\n",
            "14 26 15 24 29 10: 2\n",
            "36 26 51 45 58 08: 2\n",
            "26 24 28 34 41 11: 2\n",
            "12 14 23 10 27 05: 2\n",
            "10 26 24 59 33 27: 2\n",
            "26 33 45 59 49 14: 2\n",
            "06 26 49 33 53 16: 2\n",
            "08 26 14 24 15 20: 2\n",
            "12 14 34 21 52 10: 2\n",
            "08 23 34 28 38 14: 2\n",
            "14 24 23 49 56 25: 2\n",
            "08 26 15 45 49 20: 2\n",
            "14 26 45 21 53 13: 2\n",
            "26 14 45 21 52 28: 2\n",
            "14 12 39 34 65 05: 2\n",
            "14 26 20 24 41 17: 2\n",
            "14 19 39 52 65 05: 2\n",
            "14 26 33 45 46 20: 2\n",
            "14 19 39 62 56 15: 2\n",
            "08 26 52 59 53 21: 2\n",
            "12 14 39 52 65 05: 2\n",
            "14 26 45 46 52 13: 2\n",
            "14 33 23 36 52 06: 2\n",
            "14 23 28 38 40 13: 2\n",
            "08 26 45 52 53 05: 2\n",
            "26 12 44 52 62 08: 2\n",
            "24 14 34 33 40 20: 2\n",
            "12 14 37 52 40 10: 2\n",
            "26 14 45 21 51 16: 2\n",
            "14 15 45 39 52 13: 2\n",
            "24 14 34 33 51 35: 2\n",
            "12 14 34 52 59 13: 2\n",
            "24 14 34 33 48 27: 2\n",
            "26 37 36 61 40 21: 2\n",
            "14 26 33 40 41 17: 2\n",
            "26 24 33 23 21 11: 2\n",
            "12 14 45 21 56 11: 2\n",
            "08 23 34 28 59 14: 2\n",
            "10 33 23 52 59 12: 2\n",
            "08 26 23 15 53 16: 2\n",
            "24 14 34 33 58 27: 2\n",
            "36 40 39 52 53 08: 2\n",
            "12 14 23 45 37 07: 2\n",
            "12 14 10 21 01 07: 2\n",
            "12 14 26 21 23 15: 2\n",
            "01 37 26 62 53 15: 2\n",
            "12 14 34 33 48 35: 2\n",
            "26 17 34 39 52 35: 2\n",
            "12 14 15 21 24 10: 2\n",
            "08 26 28 34 33 20: 2\n",
            "12 14 34 21 15 10: 2\n",
            "36 19 45 62 67 06: 2\n",
            "08 23 15 28 34 20: 2\n",
            "26 34 28 35 51 29: 2\n",
            "08 26 24 52 29 20: 2\n",
            "12 14 07 21 56 10: 2\n",
            "01 26 24 67 40 17: 2\n",
            "26 17 45 35 63 13: 2\n",
            "14 26 24 45 29 27: 2\n",
            "26 15 33 59 67 20: 2\n",
            "08 26 15 49 27 20: 2\n",
            "08 26 33 35 53 20: 2\n",
            "14 26 07 24 45 17: 2\n",
            "26 37 36 52 62 35: 2\n",
            "08 26 15 34 58 20: 2\n",
            "27 10 23 51 52 35: 2\n",
            "26 24 19 49 52 10: 2\n",
            "14 26 45 21 67 13: 2\n",
            "12 14 10 21 33 05: 2\n",
            "14 21 23 34 29 05: 2\n",
            "08 26 33 67 34 20: 2\n",
            "26 15 34 28 35 14: 2\n",
            "08 17 23 15 52 05: 2\n",
            "12 14 10 21 34 23: 2\n",
            "08 26 52 45 53 05: 2\n",
            "12 14 23 21 07 10: 2\n",
            "14 23 15 34 59 13: 2\n",
            "12 22 15 33 51 20: 2\n",
            "14 26 17 51 29 15: 2\n",
            "26 14 36 52 59 08: 2\n",
            "12 14 10 21 24 05: 2\n",
            "08 26 28 45 66 10: 2\n",
            "26 14 45 21 35 23: 2\n",
            "08 31 34 59 58 10: 2\n",
            "08 20 23 50 52 05: 2\n",
            "12 14 26 45 56 07: 2\n",
            "26 12 45 52 58 13: 2\n",
            "06 26 07 35 53 16: 2\n",
            "08 26 15 46 53 20: 2\n",
            "14 26 49 53 58 16: 2\n",
            "14 26 49 46 65 13: 2\n",
            "10 33 12 36 28 20: 2\n",
            "14 28 23 45 52 07: 2\n",
            "36 33 39 62 67 06: 2\n",
            "12 14 41 21 28 10: 2\n",
            "14 19 23 24 52 17: 2\n",
            "12 14 15 21 56 10: 2\n",
            "26 24 28 34 52 20: 2\n",
            "24 14 34 21 58 28: 2\n",
            "12 14 07 52 24 10: 2\n",
            "26 14 45 21 52 10: 2\n",
            "12 14 10 21 52 07: 2\n",
            "26 17 34 39 52 28: 2\n",
            "12 14 34 52 65 11: 2\n",
            "26 14 34 21 28 35: 2\n",
            "14 12 23 34 49 05: 2\n",
            "12 14 10 45 58 07: 2\n",
            "06 26 04 35 53 16: 2\n",
            "14 26 15 45 59 08: 2\n",
            "12 14 21 45 58 07: 2\n",
            "10 33 24 36 52 20: 2\n",
            "08 26 28 67 29 10: 2\n",
            "12 14 26 21 45 23: 2\n",
            "08 26 45 35 56 16: 2\n",
            "won=927 cost 12\n",
            "pick: 20 % 14.12 diff 1\n",
            "pick: 07 % 13.089999999999998 diff 14\n",
            "pick: 10 % 10.08 diff 11\n",
            "pick: 05 % 7.9799999999999995 diff 16\n",
            "pick: 16 % 7.8100000000000005 diff 5\n",
            "pick: 13 % 6.59 diff 8\n",
            "pick: 08 % 4.87 diff 13\n",
            "pick: 17 % 4.44 diff 4\n",
            "pick: 11 % 3.6799999999999997 diff 10\n",
            "pick: 15 % 3.0700000000000003 diff 6\n",
            "pick: 06 % 2.71 diff 15\n",
            "pick: 29 % 2.69 diff -8\n",
            "pick: 21 % 2.26!!!!!\n",
            "pick: 14 % 2.06 diff 7\n",
            "pick: 27 % 1.97 diff -6\n",
            "pick: 26 % 1.2 diff -5\n",
            "pick: 23 % 1.2 diff -2\n",
            "pick: 12 % 1.06 diff 9\n",
            "pick: 28 % 0.9900000000000001 diff -7\n",
            "pick: 19 % 0.8099999999999999 diff 2\n",
            "pick: 01 % 0.75 diff 20\n",
            "pick: 04 % 0.7100000000000001 diff 17\n",
            "pick: 24 % 0.44 diff -3\n",
            "pick: 25 % 0.29 diff -4\n",
            "pick: 03 % 0.16999999999999998 diff 18\n",
            "pick: 22 % 0.09 diff -1\n",
            "==========================\n",
            "63\n",
            "pick: 14 8.642 %\n",
            "pick: 26 7.632 %\n",
            "pick: 45 5.376 %\n",
            "pick: 12 5.318 %\n",
            "pick: 21 4.744000000000001 %\n",
            "pick: 52 4.526 %\n",
            "pick: 24 4.394 %\n",
            "pick: 23 3.4259999999999997 %\n",
            "pick: 10 3.386 %\n",
            "pick: 08 2.704 % !!!!\n",
            "pick: 33 2.638 %\n",
            "pick: 15 2.618 %\n",
            "pick: 39 2.334 %\n",
            "pick: 34 2.204 %\n",
            "pick: 53 2.0500000000000003 %\n",
            "pick: 40 1.836 %\n",
            "pick: 01 1.814 %\n",
            "pick: 58 1.736 %\n",
            "pick: 36 1.72 %\n",
            "pick: 59 1.698 %\n",
            "pick: 56 1.6420000000000001 %\n",
            "pick: 67 1.5599999999999998 %\n",
            "pick: 51 1.436 %\n",
            "pick: 27 1.4200000000000002 %\n",
            "pick: 49 1.28 %\n",
            "pick: 28 1.27 %\n",
            "pick: 29 1.232 %\n",
            "pick: 19 1.228 %\n",
            "pick: 20 1.1780000000000002 %\n",
            "pick: 37 1.142 %\n",
            "pick: 65 1.052 %\n",
            "pick: 35 1.036 %\n",
            "pick: 11 0.984 %\n",
            "pick: 32 0.882 %\n",
            "pick: 07 0.8580000000000001 %\n",
            "pick: 06 0.836 %\n",
            "pick: 17 0.812 % !!!!\n",
            "pick: 62 0.784 %\n",
            "pick: 41 0.742 % !!!!\n",
            "pick: 46 0.656 %\n",
            "pick: 16 0.596 %\n",
            "pick: 05 0.582 % !!!!\n",
            "pick: 44 0.5519999999999999 %\n",
            "pick: 31 0.536 %\n",
            "pick: 61 0.532 %\n",
            "pick: 38 0.484 %\n",
            "pick: 48 0.44 %\n",
            "pick: 13 0.416 %\n",
            "pick: 04 0.326 %\n",
            "pick: 50 0.31 %\n",
            "pick: 54 0.294 %\n",
            "pick: 63 0.292 %\n",
            "pick: 47 0.264 %\n",
            "pick: 30 0.23600000000000002 %\n",
            "pick: 66 0.22200000000000003 %\n",
            "pick: 03 0.214 %\n",
            "pick: 22 0.212 %\n",
            "pick: 25 0.202 %\n",
            "pick: 68 0.14200000000000002 %\n",
            "pick: 42 0.124 %\n",
            "pick: 69 0.104 %\n",
            "pick: 64 0.062 %\n",
            "pick: 60 0.032 %\n",
            "07/19/2023\n",
            "won so far 4 cost 18 profit/loss -14\n",
            "won so far 8 cost 68 profit/loss -60\n",
            "won so far 15 cost 74 profit/loss -59\n",
            "won so far 19 cost 162 profit/loss -143\n",
            "won so far 23 cost 172 profit/loss -149\n",
            "won so far 27 cost 178 profit/loss -151\n",
            "won so far 31 cost 326 profit/loss -295\n",
            "won so far 35 cost 342 profit/loss -307\n",
            "won so far 39 cost 484 profit/loss -445\n",
            "won so far 43 cost 570 profit/loss -527\n",
            "won so far 47 cost 596 profit/loss -549\n",
            "won so far 51 cost 638 profit/loss -587\n",
            "won so far 55 cost 656 profit/loss -601\n",
            "won so far 59 cost 672 profit/loss -613\n",
            "won so far 63 cost 742 profit/loss -679\n",
            "won so far 67 cost 744 profit/loss -677\n",
            "won so far 71 cost 772 profit/loss -701\n",
            "won so far 75 cost 776 profit/loss -701\n",
            "won so far 79 cost 848 profit/loss -769\n",
            "won so far 83 cost 952 profit/loss -869\n",
            "won so far 87 cost 958 profit/loss -871\n",
            "won so far 91 cost 972 profit/loss -881\n",
            "won so far 95 cost 1112 profit/loss -1017\n",
            "won so far 99 cost 1134 profit/loss -1035\n",
            "won so far 103 cost 1146 profit/loss -1043\n",
            "won so far 107 cost 1158 profit/loss -1051\n",
            "won so far 111 cost 1208 profit/loss -1097\n",
            "won so far 115 cost 1326 profit/loss -1211\n",
            "won so far 119 cost 1332 profit/loss -1213\n",
            "won so far 123 cost 1410 profit/loss -1287\n",
            "won so far 127 cost 1452 profit/loss -1325\n",
            "won so far 131 cost 1514 profit/loss -1383\n",
            "won so far 135 cost 1546 profit/loss -1411\n",
            "won so far 142 cost 1588 profit/loss -1446\n",
            "won so far 146 cost 1614 profit/loss -1468\n",
            "won so far 150 cost 1774 profit/loss -1624\n",
            "won so far 154 cost 1856 profit/loss -1702\n",
            "won so far 158 cost 2036 profit/loss -1878\n",
            "won so far 162 cost 2098 profit/loss -1936\n",
            "won so far 166 cost 2120 profit/loss -1954\n",
            "won so far 170 cost 2280 profit/loss -2110\n",
            "won so far 174 cost 2290 profit/loss -2116\n",
            "won so far 178 cost 2346 profit/loss -2168\n",
            "won so far 182 cost 2398 profit/loss -2216\n",
            "won so far 186 cost 2440 profit/loss -2254\n",
            "won so far 190 cost 2442 profit/loss -2252\n",
            "won so far 194 cost 2470 profit/loss -2276\n",
            "won so far 198 cost 2482 profit/loss -2284\n",
            "won so far 202 cost 2614 profit/loss -2412\n",
            "won so far 206 cost 2650 profit/loss -2444\n",
            "won so far 210 cost 2894 profit/loss -2684\n",
            "won so far 217 cost 3006 profit/loss -2789\n",
            "won so far 221 cost 3118 profit/loss -2897\n",
            "won so far 225 cost 3148 profit/loss -2923\n",
            "won so far 229 cost 3406 profit/loss -3177\n",
            "won so far 233 cost 3568 profit/loss -3335\n",
            "won so far 237 cost 3584 profit/loss -3347\n",
            "won so far 241 cost 3664 profit/loss -3423\n",
            "won so far 245 cost 3668 profit/loss -3423\n",
            "won so far 249 cost 3798 profit/loss -3549\n",
            "won so far 253 cost 3804 profit/loss -3551\n",
            "won so far 257 cost 3816 profit/loss -3559\n",
            "won so far 261 cost 3860 profit/loss -3599\n",
            "won so far 265 cost 3878 profit/loss -3613\n",
            "won so far 269 cost 3892 profit/loss -3623\n",
            "won so far 273 cost 3900 profit/loss -3627\n",
            "won so far 277 cost 3982 profit/loss -3705\n",
            "won so far 281 cost 4104 profit/loss -3823\n",
            "won so far 285 cost 4190 profit/loss -3905\n",
            "won so far 289 cost 4328 profit/loss -4039\n",
            "won so far 293 cost 4464 profit/loss -4171\n",
            "won so far 300 cost 4588 profit/loss -4288\n",
            "won so far 304 cost 4662 profit/loss -4358\n",
            "won so far 308 cost 4710 profit/loss -4402\n",
            "won so far 312 cost 4750 profit/loss -4438\n",
            "won so far 316 cost 4782 profit/loss -4466\n",
            "won so far 320 cost 4792 profit/loss -4472\n",
            "won so far 324 cost 4816 profit/loss -4492\n",
            "won so far 328 cost 4948 profit/loss -4620\n",
            "won so far 332 cost 5008 profit/loss -4676\n",
            "won so far 336 cost 5034 profit/loss -4698\n",
            "won so far 343 cost 5056 profit/loss -4713\n",
            "won so far 347 cost 5070 profit/loss -4723\n",
            "won so far 351 cost 5126 profit/loss -4775\n",
            "won so far 355 cost 5226 profit/loss -4871\n",
            "won so far 359 cost 5258 profit/loss -4899\n",
            "won so far 363 cost 5366 profit/loss -5003\n",
            "won so far 367 cost 5440 profit/loss -5073\n",
            "won so far 371 cost 5500 profit/loss -5129\n",
            "won so far 375 cost 5552 profit/loss -5177\n",
            "won so far 379 cost 5572 profit/loss -5193\n",
            "won so far 383 cost 5658 profit/loss -5275\n",
            "won so far 387 cost 5698 profit/loss -5311\n",
            "won so far 391 cost 5744 profit/loss -5353\n",
            "won so far 395 cost 5746 profit/loss -5351\n",
            "won so far 399 cost 5790 profit/loss -5391\n",
            "won so far 403 cost 5818 profit/loss -5415\n",
            "won so far 407 cost 5842 profit/loss -5435\n",
            "won so far 411 cost 5890 profit/loss -5479\n",
            "won so far 418 cost 5910 profit/loss -5492\n",
            "won so far 422 cost 6332 profit/loss -5910\n",
            "won so far 426 cost 6400 profit/loss -5974\n",
            "won so far 430 cost 6574 profit/loss -6144\n",
            "won so far 434 cost 6784 profit/loss -6350\n",
            "won so far 438 cost 6808 profit/loss -6370\n",
            "won so far 442 cost 6814 profit/loss -6372\n",
            "won so far 446 cost 6932 profit/loss -6486\n",
            "won so far 450 cost 7010 profit/loss -6560\n",
            "won so far 454 cost 7052 profit/loss -6598\n",
            "won so far 458 cost 7064 profit/loss -6606\n",
            "won so far 462 cost 7190 profit/loss -6728\n",
            "won so far 466 cost 7220 profit/loss -6754\n",
            "won so far 470 cost 7314 profit/loss -6844\n",
            "won so far 474 cost 7500 profit/loss -7026\n",
            "won so far 478 cost 7514 profit/loss -7036\n",
            "won so far 482 cost 7864 profit/loss -7382\n",
            "won so far 486 cost 7874 profit/loss -7388\n",
            "won so far 490 cost 8054 profit/loss -7564\n",
            "won so far 494 cost 8244 profit/loss -7750\n",
            "won so far 498 cost 8552 profit/loss -8054\n",
            "won so far 502 cost 8716 profit/loss -8214\n",
            "won so far 506 cost 8786 profit/loss -8280\n",
            "won so far 510 cost 8896 profit/loss -8386\n",
            "won so far 514 cost 8926 profit/loss -8412\n",
            "won so far 518 cost 9052 profit/loss -8534\n",
            "won so far 522 cost 9072 profit/loss -8550\n",
            "won so far 526 cost 9078 profit/loss -8552\n",
            "won so far 530 cost 9170 profit/loss -8640\n",
            "won so far 534 cost 9216 profit/loss -8682\n",
            "won so far 538 cost 9334 profit/loss -8796\n",
            "won so far 542 cost 9370 profit/loss -8828\n",
            "won so far 546 cost 9390 profit/loss -8844\n",
            "won so far 550 cost 9560 profit/loss -9010\n",
            "won so far 554 cost 9596 profit/loss -9042\n",
            "won so far 558 cost 9598 profit/loss -9040\n",
            "won so far 562 cost 9612 profit/loss -9050\n",
            "won so far 566 cost 9698 profit/loss -9132\n",
            "won so far 570 cost 9764 profit/loss -9194\n",
            "won so far 574 cost 10008 profit/loss -9434\n",
            "won so far 581 cost 10118 profit/loss -9537\n",
            "won so far 585 cost 10130 profit/loss -9545\n",
            "won so far 589 cost 10252 profit/loss -9663\n",
            "won so far 593 cost 10286 profit/loss -9693\n",
            "won so far 597 cost 10338 profit/loss -9741\n",
            "won so far 601 cost 10342 profit/loss -9741\n",
            "won so far 605 cost 10508 profit/loss -9903\n",
            "won so far 609 cost 10528 profit/loss -9919\n",
            "won so far 613 cost 10554 profit/loss -9941\n",
            "won so far 617 cost 10566 profit/loss -9949\n",
            "won so far 621 cost 10576 profit/loss -9955\n",
            "won so far 625 cost 10654 profit/loss -10029\n",
            "won so far 629 cost 10790 profit/loss -10161\n",
            "won so far 633 cost 10810 profit/loss -10177\n",
            "won so far 637 cost 10846 profit/loss -10209\n",
            "won so far 641 cost 10860 profit/loss -10219\n",
            "won so far 645 cost 11064 profit/loss -10419\n",
            "won so far 649 cost 11126 profit/loss -10477\n",
            "won so far 653 cost 11194 profit/loss -10541\n",
            "won so far 657 cost 11222 profit/loss -10565\n",
            "won so far 661 cost 11250 profit/loss -10589\n",
            "won so far 665 cost 11270 profit/loss -10605\n",
            "won so far 672 cost 11556 profit/loss -10884\n",
            "won so far 676 cost 11702 profit/loss -11026\n",
            "won so far 680 cost 11706 profit/loss -11026\n",
            "won so far 684 cost 11728 profit/loss -11044\n",
            "won so far 688 cost 11764 profit/loss -11076\n",
            "won so far 692 cost 11838 profit/loss -11146\n",
            "won so far 696 cost 11872 profit/loss -11176\n",
            "won so far 700 cost 11980 profit/loss -11280\n",
            "won so far 704 cost 12100 profit/loss -11396\n",
            "won so far 708 cost 12218 profit/loss -11510\n",
            "won so far 712 cost 12264 profit/loss -11552\n",
            "won so far 716 cost 12312 profit/loss -11596\n",
            "won so far 723 cost 12314 profit/loss -11591\n",
            "won so far 727 cost 12382 profit/loss -11655\n",
            "won so far 731 cost 12542 profit/loss -11811\n",
            "won so far 735 cost 12712 profit/loss -11977\n",
            "won so far 739 cost 12732 profit/loss -11993\n",
            "won so far 743 cost 12846 profit/loss -12103\n",
            "won so far 747 cost 13204 profit/loss -12457\n",
            "won so far 751 cost 13254 profit/loss -12503\n",
            "won so far 755 cost 13402 profit/loss -12647\n",
            "won so far 759 cost 13560 profit/loss -12801\n",
            "won so far 763 cost 13586 profit/loss -12823\n",
            "won so far 767 cost 13596 profit/loss -12829\n",
            "won so far 771 cost 13800 profit/loss -13029\n",
            "won so far 775 cost 13868 profit/loss -13093\n",
            "won so far 779 cost 13870 profit/loss -13091\n",
            "won so far 783 cost 13926 profit/loss -13143\n",
            "won so far 787 cost 14088 profit/loss -13301\n",
            "won so far 791 cost 14112 profit/loss -13321\n",
            "won so far 795 cost 14316 profit/loss -13521\n",
            "won so far 799 cost 14328 profit/loss -13529\n",
            "won so far 803 cost 14368 profit/loss -13565\n",
            "won so far 807 cost 14420 profit/loss -13613\n",
            "won so far 811 cost 14660 profit/loss -13849\n",
            "won so far 815 cost 14732 profit/loss -13917\n",
            "won so far 819 cost 14754 profit/loss -13935\n",
            "won so far 826 cost 14758 profit/loss -13932\n",
            "won so far 830 cost 14838 profit/loss -14008\n",
            "won so far 834 cost 14848 profit/loss -14014\n",
            "won so far 838 cost 14898 profit/loss -14060\n",
            "won so far 845 cost 14920 profit/loss -14075\n",
            "won so far 849 cost 14956 profit/loss -14107\n",
            "won so far 853 cost 15418 profit/loss -14565\n",
            "won so far 857 cost 15436 profit/loss -14579\n",
            "won so far 861 cost 15478 profit/loss -14617\n",
            "won so far 865 cost 15488 profit/loss -14623\n",
            "won so far 869 cost 15536 profit/loss -14667\n",
            "won so far 873 cost 15580 profit/loss -14707\n",
            "won so far 877 cost 15614 profit/loss -14737\n",
            "won so far 881 cost 15616 profit/loss -14735\n",
            "won so far 885 cost 15744 profit/loss -14859\n",
            "won so far 889 cost 15782 profit/loss -14893\n",
            "won so far 893 cost 15802 profit/loss -14909\n",
            "won so far 897 cost 15880 profit/loss -14983\n",
            "won so far 901 cost 16026 profit/loss -15125\n",
            "won so far 905 cost 16154 profit/loss -15249\n",
            "won so far 909 cost 16194 profit/loss -15285\n",
            "won so far 913 cost 16272 profit/loss -15359\n",
            "won so far 917 cost 16280 profit/loss -15363\n",
            "won so far 921 cost 16324 profit/loss -15403\n",
            "won so far 925 cost 16408 profit/loss -15483\n",
            "won so far 929 cost 16416 profit/loss -15487\n",
            "won so far 933 cost 16422 profit/loss -15489\n",
            "won so far 937 cost 16618 profit/loss -15681\n",
            "won so far 941 cost 16642 profit/loss -15701\n",
            "won so far 945 cost 16728 profit/loss -15783\n",
            "won so far 949 cost 16774 profit/loss -15825\n",
            "won so far 953 cost 16866 profit/loss -15913\n",
            "won so far 960 cost 16952 profit/loss -15992\n",
            "won so far 964 cost 17074 profit/loss -16110\n",
            "won so far 968 cost 17148 profit/loss -16180\n",
            "won so far 972 cost 17162 profit/loss -16190\n",
            "won so far 976 cost 17178 profit/loss -16202\n",
            "won so far 980 cost 17308 profit/loss -16328\n",
            "won so far 984 cost 17328 profit/loss -16344\n",
            "won so far 988 cost 17330 profit/loss -16342\n",
            "won so far 992 cost 17432 profit/loss -16440\n",
            "won so far 996 cost 17458 profit/loss -16462\n",
            "won so far 1000 cost 17580 profit/loss -16580\n",
            "won so far 1004 cost 17736 profit/loss -16732\n",
            "won so far 1008 cost 17744 profit/loss -16736\n",
            "won so far 1012 cost 17812 profit/loss -16800\n",
            "won so far 1016 cost 17944 profit/loss -16928\n",
            "won so far 1020 cost 18012 profit/loss -16992\n",
            "won so far 1024 cost 18208 profit/loss -17184\n",
            "won so far 1028 cost 18512 profit/loss -17484\n",
            "won so far 1032 cost 18546 profit/loss -17514\n",
            "won so far 1036 cost 18652 profit/loss -17616\n",
            "won so far 1040 cost 18702 profit/loss -17662\n",
            "won so far 1044 cost 18708 profit/loss -17664\n",
            "won so far 1048 cost 18760 profit/loss -17712\n",
            "won so far 1052 cost 18990 profit/loss -17938\n",
            "won so far 1056 cost 18992 profit/loss -17936\n",
            "won so far 1060 cost 19082 profit/loss -18022\n",
            "won so far 1064 cost 19240 profit/loss -18176\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, sample_cout):\n\u001b[0;32m     29\u001b[0m     did_win_power_ball \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     prediction \u001b[39m=\u001b[39m fill_text(prompt, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, temperature\u001b[39m=\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandom()\u001b[39m*\u001b[39;49m\u001b[39m2.0\u001b[39;49m, no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m     prediction \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mreplace(prompt, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     prediction \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mstrip()\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:209\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    169\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         )\n\u001b[0;32m   1107\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:251\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m prompt_text \u001b[39m=\u001b[39m model_inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mprompt_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    252\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1478\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1479\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1480\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1481\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1482\u001b[0m     )\n\u001b[0;32m   1484\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1485\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1486\u001b[0m         input_ids,\n\u001b[0;32m   1487\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1488\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1489\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1490\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1491\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1492\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1493\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1494\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1495\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1496\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1497\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py:2524\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2523\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2524\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2526\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2527\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2528\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2532\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:975\u001b[0m, in \u001b[0;36mRobertaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    973\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m    976\u001b[0m     input_ids,\n\u001b[0;32m    977\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    978\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    979\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    980\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    981\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    982\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    983\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    984\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    985\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    986\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    987\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    988\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    989\u001b[0m )\n\u001b[0;32m    991\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    992\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    450\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    451\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 453\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    455\u001b[0m )\n\u001b[0;32m    456\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    458\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 465\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    466\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:364\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    363\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 364\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[0;32m    365\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import OrderedDict\n",
        "f = open(\"./07-12-08-10.json\")\n",
        "numbers = json.load(f)\n",
        "numbers.sort(key = itemgetter('Draw Date'), reverse=False)\n",
        "pre_winners = \"\"\n",
        "i = 0\n",
        "for drawing in numbers:\n",
        "    if i < 1:\n",
        "        pre_winners = drawing[\"Winning Numbers\"]\n",
        "        i += 1\n",
        "    else:\n",
        "        print(drawing[\"Draw Date\"])\n",
        "        winning_numbers = drawing[\"Winning Numbers\"]\n",
        "        winning_numbers = winning_numbers.split(\" \")\n",
        "        prompt = f\"{pre_winners} ::\"\n",
        "        winning_picks = []\n",
        "        power_ball_picks = []\n",
        "        power_ball = {}\n",
        "        reg_picks = {}\n",
        "        win_picks = {}\n",
        "        sample_cout = 10000\n",
        "        p_count = 0\n",
        "        predictions = []\n",
        "        won = 0\n",
        "        for i in range(0, sample_cout):\n",
        "            did_win_power_ball = False\n",
        "            prediction = fill_text(prompt, max_new_tokens=6, top_k=50, do_sample=True, temperature=random.random()*2.0, no_repeat_ngram_size=1)[0][\"generated_text\"]\n",
        "            prediction = prediction.replace(prompt, \"\")\n",
        "            prediction = prediction.strip()\n",
        "            prediction = prediction.replace(\":\", \"\")\n",
        "            prediction_ = prediction.strip()\n",
        "            prediction = prediction_.split(\" \")\n",
        "            if int(prediction[-1]) <= 29:\n",
        "                if prediction[-1] not in power_ball:\n",
        "                    power_ball[prediction[-1]] = 1\n",
        "                else:\n",
        "                    power_ball[prediction[-1]] += 1\n",
        "            predictions.append(prediction_)\n",
        "            count = 0\n",
        "            if winning_numbers[-1] in prediction[-1]:\n",
        "                did_win_power_ball = True\n",
        "                power_ball_picks.append(winning_numbers)\n",
        "                count+=1\n",
        "                # print(f\"{winning_numbers} power_ball {prediction} correct count: {count}\")\n",
        "            for p in prediction[:-1]:\n",
        "                if p not in reg_picks:\n",
        "                    reg_picks[p] = 1\n",
        "                else:\n",
        "                    reg_picks[p] += 1\n",
        "            for w in winning_numbers[:-1]:\n",
        "                if w in prediction[:-1]:\n",
        "                    if w not in win_picks:\n",
        "                        win_picks[w]=0\n",
        "                    win_picks[w]+=1\n",
        "                    count+=1\n",
        "\n",
        "            if did_win_power_ball:\n",
        "                if count == 6:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 100000000\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 5:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 50000\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 4:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 100\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 3:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 7\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 2:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 4\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 1:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 4\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "            else:\n",
        "                if count == 5:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 1000000\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 4:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 100\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "                if count == 3:\n",
        "                    winning_picks.append(prediction)\n",
        "                    won += 7\n",
        "                    print(f\"won so far {won} cost {len(predictions)*2} profit/loss {won - len(predictions)*2}\")\n",
        "\n",
        "            \n",
        "        pre_winners = winning_numbers\n",
        "        print(\"==========================\")\n",
        "        print(f\"percetange {len(winning_picks)/len(predictions)*100}\")\n",
        "        print(f\"percetange power ball {len(power_ball_picks)/(len(predictions))*100}\")\n",
        "        print(\"==========================\")\n",
        "        elements_count = collections.Counter(predictions)\n",
        "        sortedDict = sorted(elements_count.items(), key=lambda x:x[1], reverse=True) \n",
        "        for key, value in sortedDict:\n",
        "            if value > 1:\n",
        "                print(f\"{key}: {value}\")\n",
        "        print(f\"won={won} cost {len(prediction)*2}\")             \n",
        "        sorted_x = OrderedDict(sorted(power_ball.items(), key=itemgetter(1), reverse=True))\n",
        "        for pick in sorted_x:\n",
        "            if pick in winning_numbers[-1]:\n",
        "                print(f\"pick: {pick} % {sorted_x[pick]/len(predictions)*100}!!!!!\")\n",
        "            else:\n",
        "                print(f\"pick: {pick} % {sorted_x[pick]/len(predictions)*100} diff {int(winning_numbers[-1]) - int(pick)}\")\n",
        "        \n",
        "        print(\"==========================\")\n",
        "        sorted_x = OrderedDict(sorted(reg_picks.items(), key=itemgetter(1), reverse=True))\n",
        "        print(len(sorted_x))\n",
        "        for pick in sorted_x:\n",
        "            if pick in winning_numbers[:-1]:\n",
        "                print(f\"pick: {pick} {sorted_x[pick]/(len(predictions)*5)*100} % !!!!\")\n",
        "            else:\n",
        "                print(f\"pick: {pick} {sorted_x[pick]/(len(predictions)*5)*100} %\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPqbqeA0VB70ho26cHWVp02",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "smallBERTa_Pretraining.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
