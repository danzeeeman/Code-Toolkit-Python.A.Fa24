{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Everything You Need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IQ7hj9kuhBIj"
      },
      "source": [
        "## Load and Preprocess data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper Functions you might need\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HOG-fl1cGhJ4"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "def basicPreprocess(text):\n",
        "  try:\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'\\W +', ' ', processed_text)\n",
        "  except Exception as e:\n",
        "    print(\"Exception:\",e,\",on text:\", text)\n",
        "    return None\n",
        "  return processed_text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert CSV file to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "def convert_csv_to_json(csv_file_path):\n",
        "    # Read CSV file\n",
        "    with open(csv_file_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "    # Convert CSV data to JSON\n",
        "    json_data = json.dumps(rows, indent=4)\n",
        "\n",
        "    # Save JSON data to a file (optional)\n",
        "    with open('Lottery_Powerball_Winning_Numbers__Beginning_2010.json', 'w') as json_file:\n",
        "        json_file.write(json_data)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = 'Lottery_Powerball_Winning_Numbers__Beginning_2010.csv'\n",
        "\n",
        "# Convert CSV to JSON\n",
        "json_data = convert_csv_to_json(csv_file_path)\n",
        "\n",
        "print(\"Conversion completed. JSON data:\")\n",
        "print(json_data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert JSON to Prompt List and llama input JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stats_file = \"Lottery_Powerball_Winning_Numbers__Beginning_2010.json\"\n",
        "powerball = []\n",
        "test = []\n",
        "lines = []\n",
        "with open(stats_file, 'r') as f:\n",
        "    stats = json.load(f)\n",
        "    for data in stats:\n",
        "        powerball.append(\n",
        "            {\n",
        "            'instruction':f\"what is the powerball drawing on {data['Draw Date']}\",\n",
        "            \"output\":data['Winning Numbers']\n",
        "            }\n",
        "        )\n",
        "        lines.append(f\"###instruction: what is the powerball drawing on {data['Draw Date']}, ###output: {data['Winning Numbers']}\\n\")\n",
        "    with open(f'prompts.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(powerball, f, ensure_ascii=True, indent=4, allow_nan=True)\n",
        "        f.close()\n",
        "    with open(f'prompts.txt', 'w', encoding='utf-8') as f:\n",
        "        f.writelines(lines)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gI1Tp54IiVBj"
      },
      "source": [
        "## Train a custom tokenizer\n",
        "I have used a ByteLevelBPETokenizer just to prevent \\<unk> tokens entirely.\n",
        "Furthermore, the function used to train the tokenizer assumes that each sample is stored in a different text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['###', 'INSTRUCTION', ':', 'what', 'is', 'the', 'outcome', 'of', 'pitcher', '477132', 'pitching', 'to', 'batter', '593428', '###', 'INPUT', ':', 'T', 'o', 'p', 'of', 'the', '4', 'in', 'n', 'ing', 'w', 'i', 'th', '1', 'stri', 'k', 'e', 'and', '1', 'ball', 's', 'and', '1', 'out', 's', '###', 'RESPONSE', ':']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from pathlib import Path\n",
        "\n",
        "# txt_files_dir = \"./models/baseball/tokenizer/raw_3\"\n",
        "\n",
        "# paths = [str(x) for x in Path(txt_files_dir).glob(\"**/*.txt\")]\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[    \n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "    ])\n",
        "tokenizer.train(files=[\"./models/powerball/prompts.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"./models/powerball/tokenizer/powerball.json\")\n",
        "\n",
        "output = tokenizer.encode(\"\")\n",
        "print(output.tokens)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(tokenizer_file=\"./models/powerball/tokenizer/powerball.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./models/powerball/prompts.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models/powerball\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=300,\n",
        "    per_device_train_batch_size=256,\n",
        "    save_steps=5_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"./models/powerball\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=\"./models/powerball\")\n",
        "trainer.save_model(\"./models/powerball\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./models/powerball\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./models/powerball\",\n",
        "    tokenizer=\"./models/powerball/tokenizer\",\n",
        "    top_k=10,\n",
        ")\n",
        "\n",
        "fill_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"./models/powerball\",\n",
        "    tokenizer=\"./models/powerball/tokenizer\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPqbqeA0VB70ho26cHWVp02",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "smallBERTa_Pretraining.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
